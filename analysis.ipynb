{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visulaization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, null values imputed, outliers removed etc, it is far easier to analyse the data correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#importing my classes\n",
    "from dataFrameInfo import Data_FrameInfo\n",
    "from data_Transform import DataTransform\n",
    "from dataFrameTransform import Data_FrameTransform\n",
    "from plotter import Plotter\n",
    "\n",
    "#importing dataframes\n",
    "df = pd.read_csv('dfnonulls.csv')\n",
    "normalised = pd.read_csv('dfnoskew.csv')\n",
    "\n",
    "#instantiating classes\n",
    "transform = DataTransform()\n",
    "query = Data_FrameInfo()\n",
    "visual = Plotter(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current state of the loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working out percentage of loans fully recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will suummarise what percentage of the loans have been currently recovered compared to the total amount to be paid over the loans term including interest.\n",
    "\n",
    "Additionally I will calculate how much will be paid back in 6 months time with interest and visualise my results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess this, I need to calculate the following:\n",
    "\n",
    "1) The amount of money actually recovered. This will be simply the total payments\n",
    "2) The amount of money that would be recovered if everyone paid in full. This will be the amount of installments * the amount of each installment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The % of expected funded amount recovered is: 70.61%\n",
      "The % of expected invested amount recovered is: 68.92%\n",
      "The % of funded amount recovered is: 91.78%\n",
      "The % of invested amount recovered is: 91.02%\n"
     ]
    }
   ],
   "source": [
    "#calculate totals recovered by funded amount and by investor\n",
    "total_recovered_fun = df['total_payment'].sum()\n",
    "total_recovered_inv = df['total_payment_inv'].sum()\n",
    "\n",
    "#calculate total amounts loaned out and invested\n",
    "total_funded_inv = df['funded_amount_inv'].sum()\n",
    "total_funded = df['funded_amount'].sum()\n",
    "\n",
    "#calculate totals expected by funded amount and by investor\n",
    "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
    "\n",
    "total_expected = df['total_expected'].sum()\n",
    "\n",
    "#print percentages by expected funded amount and by investor\n",
    "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
    "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
    "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
    "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
    "\n",
    "#print percentages of loaned amounts and invested amounts\n",
    "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
    "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
    "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
    "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems although the bank olny recoved about 70% of what it was expecting to recover, it has recovered about 90% of what was loaned out and invested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Funding</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Percent Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investor</td>\n",
       "      <td>91.78</td>\n",
       "      <td>70.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total</td>\n",
       "      <td>91.02</td>\n",
       "      <td>68.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Funding  Percent  Percent Expected\n",
       "0  Investor    91.78             70.61\n",
       "1     Total    91.02             68.92"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Funding', ylabel='% Recovered'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHbCAYAAADRb3COAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1oklEQVR4nO3deVhV5d7/8c8GmVFIk0kxTHEWJS1DPWqKOaRlNnosZ+sxypxSOeWQpahPlpmmHge0jubxHIcsO5pResoUEccSZ0orQUuBBBmE/fujp/1r58RGYO2F79d17eti3Wv6LoT4dN/3WstitVqtAgAAMCEXowsAAAAoKYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwrUpGF1DWioqK9NNPP6ly5cqyWCxGlwMAAIrBarXq119/VUhIiFxcrt3vUuGDzE8//aTQ0FCjywAAACVw+vRp1axZ85rrK3yQqVy5sqTfvhFVqlQxuBoAAFAcWVlZCg0Ntf0dv5YKH2R+H06qUqUKQQYAAJO50bQQJvsCAADTIsgAAADTIsgAAADTqvBzZADgVmS1WnX58mUVFhYaXQpwVa6urqpUqdJNPxqFIAMAFUx+fr7OnDmjnJwco0sBrsvb21vBwcFyd3cv8TEIMgBQgRQVFSk1NVWurq4KCQmRu7s7DwOF07FarcrPz9e5c+eUmpqq8PDw6z707noIMgBQgeTn56uoqEihoaHy9vY2uhzgmry8vOTm5qbvv/9e+fn58vT0LNFxmOwLABVQSf/vFihPpfFzyk86AAAwLYIMAAAwLYIMAAAwLSb7AsAtImz8xnI933fTH3Bo+wEDBmj58uWSJDc3N9WqVUv9+vXT3/72N1Wq5Lx/riwWi9atW6devXoZXcotyXl/MgAAt5yuXbsqPj5eeXl5+uSTTxQTEyM3NzfFxsY6dJzCwkJZLBYmPd8C+BcGADgNDw8PBQUF6Y477tCwYcMUHR2tDRs2KC8vT2PGjFGNGjXk4+OjVq1aaevWrbb9li1bJn9/f23YsEGNGjWSh4eHTp06pby8PI0bN06hoaHy8PBQ3bp1tWTJEtt+33zzjbp16yZfX18FBgbq6aef1s8//2xb36FDBw0fPlxjx45V1apVFRQUpMmTJ9vWh4WFSZIefvhhWSwW2zLKDz0yFVh5dyM7C0e7swE4Ly8vL/3yyy96/vnndejQIa1atUohISFat26dunbtqoMHDyo8PFySlJOToxkzZmjx4sWqVq2aAgIC1K9fP+3YsUNz5sxRs2bNlJqaagsqGRkZ6tixo4YMGaK33npLly5d0rhx4/T444/r888/t9WwfPlyjRo1SomJidqxY4cGDBigNm3aqHPnzkpKSlJAQIDi4+PVtWtXubq6GvJ9upURZAAATsdqtSohIUGbN29Wnz59FB8fr1OnTikkJESSNGbMGG3atEnx8fGaNm2aJKmgoEDvvvuumjVrJkk6evSoVq9erS1btig6OlqSdOedd9rOMXfuXEVGRtr2l6SlS5cqNDRUR48eVb169SRJERERmjRpkiQpPDxcc+fOVUJCgjp37qzq1atLkvz9/RUUFFTG3xVcDUEGAOA0Pv74Y/n6+qqgoEBFRUX661//qkcffVTLli2zBYvf5eXlqVq1arZld3d3RURE2Jb37dsnV1dXtW/f/qrn2r9/v7744gv5+vpese7EiRN2QeaPgoODdfbs2RJfI0oXQQYATOhaQ8c1Krtq8n0ByvfKkqVSbjlXZe/ADxkObX8hO193t/6LXp46S27ubqoeGKxKlSpp04a1cnV11cqNX6hRjdvs9vljCPHy8rJ7r5SXl9d1z3fx4kX17NlTM2bMuGJdcHCw7Ws3Nze7dRaLRUVFRQ5dG8oOQQYA4DS8vLxVq/addm0NmkSosLBQ538+p7rt7y72sZo2baqioiJt27bNNrT0R3fddZfWrFmjsLCwm7q9283NTYWFhSXeHzeHu5YAAE4t7M666v7wY3p55DCtXbtWqamp2rVrl+Li4rRx47VvaggLC1P//v01aNAgrV+/Xqmpqdq6datWr14tSYqJidH58+fVp08fJSUl6cSJE9q8ebMGDhzoUDAJCwtTQkKC0tLSdOHChZu+XjiGHhkAuEVseL6N0SWU2JRZ87RozhsaPXq0fvzxR91+++2699571aNHj+vuN3/+fP3tb3/Tc889p19++UW1atXS3/72N0lSSEiItm/frnHjxun+++9XXl6e7rjjDnXt2tWh58/MmjVLo0aN0qJFi1SjRg199913N3OpcJDFarVajS6iLGVlZcnPz0+ZmZmqUqWK0eWUK26/BiquG82RCQipKUsl93KuquxF1PQ3ugSUotzcXKWmpqp27dry9PS0W1fcv98MLQEAANMiyAAAANMiyAAAANMiyAAAANMiyAAAANMiyAAAANMiyAAAANMiyAAAANMiyAAAgDLx3XffyWKxaN++fWV2Dl5RAAC3iIjFd5Tr+Q4M+d6h7SeMfE4b/v2BJKmSm5uCQ2qqx6NPasjzo27qpY7lwWKxaN26derVq9cNt7uaDz74QE8++WQZVOa47777TrVr19bevXvVvHlzo8u5Ief+yQAA3FLadOikKbPmKT8/T199vkXTXnlJbpUqafDzoxw+VmFhoSwWi0PvTSoP8fHx6tq1q12bv7+/McVUAM71rwsAuKW5u3vo9oBAhdSspcf7DVarth20dcsmSVJ+Xp7GjBmjGjVqyMfHR61atdLWrVtt+y5btkz+/v7asGGDGjVqJA8PD506dUp5eXkaN26cQkND5eHhobp162rJkiW2/b755ht169ZNvr6+CgwM1NNPP62ff/7Ztr5Dhw4aPny4xo4dq6pVqyooKEiTJ0+2rQ8LC5MkPfzww7JYLLbla/H391dQUJDd5/f3DA0aNEgRERHKy8v77Zrz8xUZGal+/fpJ+v9DNatWrVLr1q3l6empJk2aaNu2bXbnuNE1FRUVaebMmapbt648PDxUq1YtTZ06VZJUu3ZtSVJkZKQsFos6dOhg22/x4sVq2LChPD091aBBA7377rt25921a5ciIyPl6empli1bau/evdf9XpQGggwAwGl5enqqoCBfkhQ3Yax27NihVatW6cCBA3rsscfUtWtXHTt2zLZ9Tk6OZsyYocWLF+vbb79VQECA+vXrpw8++EBz5sxRSkqKFi5cKF9fX0lSRkaGOnbsqMjISO3evVubNm1Senq6Hn/8cbs6li9fLh8fHyUmJmrmzJmaMmWKtmzZIklKSkqS9FtPy5kzZ2zLJTFnzhxlZ2dr/PjxkqSXX35ZGRkZmjt3rt12L730kkaPHq29e/cqKipKPXv21C+//FLsa4qNjdX06dM1YcIEHTp0SCtXrlRgYKCk38KIJH322Wc6c+aM1q5dK0lasWKFJk6cqKlTpyolJUXTpk3ThAkTtHz5cknSxYsX1aNHDzVq1EjJycmaPHmyxowZU+LvRXExtAQAcDpWq1WJX23T1//9XH0GDNWZH0/rw9UrdOrUKYWEhEiSxowZo02bNik+Pl7Tpk2TJBUUFOjdd99Vs2bNJElHjx7V6tWrtWXLFkVHR0uS7rzzTtt55s6dq8jISNv+krR06VKFhobq6NGjqlevniQpIiJCkyZNkiSFh4dr7ty5SkhIUOfOnVW9enVJ/7+n5Ub69OkjV1dXu7ZDhw6pVq1a8vX11T/+8Q+1b99elStX1uzZs/XFF19c8fbn559/Xo888ogkaf78+dq0aZOWLFmisWPH3vCagoOD9fbbb2vu3Lnq37+/JKlOnTpq27atJNmup1q1anbXM2nSJM2aNUu9e/eW9FvPzaFDh7Rw4UL1799fK1euVFFRkZYsWSJPT081btxYP/zwg4YNG3bD78nNIMgAAJzGfxM26976NXX5coGsRUXq1utR/c+o8dq94ysVFhbagsXv8vLyVK1aNduyu7u7IiIibMv79u2Tq6ur2rdvf9Xz7d+/X1988YWth+aPTpw4YRdk/ig4OFhnz54t0TW+9dZbtlD1u9/DmSRFRUVpzJgxeu211zRu3DhbwPijqKgo29eVKlVSy5YtlZKSUqxrysjIUF5enjp16lTsmrOzs3XixAkNHjxYQ4cOtbVfvnxZfn5+kqSUlBRFRETYhsn+XGdZIcgAAJzG3a3/openzpKbu5uqBwbb7lbKyc6Wq6urkpOTr+jN+OMfbC8vL7s7g7y8vK57vosXL6pnz56aMWPGFeuCg4NtX7u5udmts1gsKioqKv6F/UFQUJDq1q17zfVFRUXavn27XF1ddfz4cYePf6NrOnnyZImOKUmLFi1Sq1at7Nb9+d+jvDFHBgDgNLy8vFWr9p0KrhFqd8t1gyYRKiws1NmzZ1W3bl27z/WGc5o2baqioqIrJsP+7q677tK3336rsLCwK47r4+NT7Lrd3NxUWFhY/Au9jv/93//V4cOHtW3bNtvQ2Z/t3LnT9vXly5eVnJyshg0bSrrxNYWHh8vLy0sJCQlXPb+7u7sk2V1PYGCgQkJCdPLkySuO+fvk4IYNG+rAgQPKzc29ap1lhSADAHB6YXfWVfeHH1O/fv20du1apaamateuXYqLi9PGjRuvvV9YmPr3769BgwZp/fr1Sk1N1datW7V69WpJUkxMjM6fP68+ffooKSlJJ06c0ObNmzVw4ECHgklYWJgSEhKUlpamCxcuXHfbjIwMpaWl2X2ys7MlSXv37tXEiRO1ePFitWnTRm+++aZefPHFK3pR5s2bp3Xr1unw4cOKiYnRhQsXNGjQoGJdk6enp8aNG6exY8fqvffe04kTJ7Rz507bnVwBAQHy8vKyTRLOzMyUJL366quKi4vTnDlzdPToUR08eFDx8fF68803JUl//etfZbFYNHToUB06dEiffPKJ3njjjWJ/D0uKIAMAMIUps+apX79+Gj16tOrXr69evXopKSlJtWrVuu5+8+fP16OPPqrnnntODRo00NChQ23BISQkRNu3b1dhYaHuv/9+NW3aVCNGjJC/v79Dz5+ZNWuWtmzZotDQUEVGRl5324EDByo4ONju88477yg3N1dPPfWUBgwYoJ49e0qSnnnmGd133316+umn7YLV9OnTNX36dDVr1kxfffWVNmzYoNtvv73Y1zRhwgSNHj1aEydOVMOGDfXEE0/Y5vxUqlRJc+bM0cKFCxUSEqKHHnpIkjRkyBAtXrxY8fHxatq0qdq3b69ly5bZemR8fX310Ucf6eDBg4qMjNTLL7981eGt0maxWq3WMj+LgbKysuTn56fMzMwrZn1XdGHjr/1/KRXZd9MfMLoEoMxd6/e7RmVXTb4vQAEhNWWp5F7OVZW9iJr+RpdgKLM9dfdGcnNzlZqaqtq1a9tNEpaK//ebHhkAAGBaBBkAAGBa3H4NAIBJhIWFqYLPCHEYPTIAAMC0CDIAAMC0CDIAUIEUWSXJKjH8ABMojWEyggwAVCAZuUUqKLTKejnf6FKAG8rJyZF05SsgHMFkXwCoQC5dtirh5EX1cHfVbVX127Nk/vDuIbP74+PvYV5Wq1U5OTk6e/as/P39b+p9TQQZAKhg1qb89tTaTncWys3VIqniBBn3S9d/CSTMxd/f/7rvyioOggwAVDBWSWtSsrXxWI5u83SRS8XJMUoY3cHoElBK3NzcSuXN2QQZAKigci9bdeZi6byR2Vn8+TH2AJN9AQCAaRFkAACAaRkaZAoLCzVhwgTVrl1bXl5eqlOnjl577TW7+8qtVqsmTpyo4OBgeXl5KTo6WseOHTOwagAA4CwMDTIzZszQ/PnzNXfuXKWkpGjGjBmaOXOm3nnnHds2M2fO1Jw5c7RgwQIlJibKx8dHXbp04RY8AABg7GTfr7/+Wg899JAeeOABSb+9DOuDDz7Qrl27JP3WGzN79my98soreuihhyRJ7733ngIDA7V+/Xo9+eSThtUOAACMZ2iPTOvWrZWQkKCjR49Kkvbv36+vvvpK3bp1kySlpqYqLS1N0dHRtn38/PzUqlUr7dixw5CaAQCA8zC0R2b8+PHKyspSgwYN5OrqqsLCQk2dOlV9+/aVJKWlpUmSAgMD7fYLDAy0rfuzvLw85eXl2ZazsrLKqHoAAGA0Q3tkVq9erRUrVmjlypXas2ePli9frjfeeEPLly8v8THj4uLk5+dn+4SGhpZixQAAwJkYGmReeukljR8/Xk8++aSaNm2qp59+WiNHjlRcXJwk2R5bnJ6ebrdfenr6NR9pHBsbq8zMTNvn9OnTZXsRAADAMIYGmZycHLm42Jfg6uqqoqIiSVLt2rUVFBSkhIQE2/qsrCwlJiYqKirqqsf08PBQlSpV7D4AAKBiMnSOTM+ePTV16lTVqlVLjRs31t69e/Xmm29q0KBBkiSLxaIRI0bo9ddfV3h4uGrXrq0JEyYoJCREvXr1MrJ0AADgBAwNMu+8844mTJig5557TmfPnlVISIieffZZTZw40bbN2LFjlZ2drWeeeUYZGRlq27atNm3axPs2AACALNY/Pka3AsrKypKfn58yMzNvuWGmsPEbjS7BEN9Nf8DoEoAyx+83Krri/v3mXUsAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0CDIAAMC0KhldAFDqJvsZXYExJmcaXQEAlDt6ZAAAgGkRZAAAgGkRZAAAgGkRZAAAgGkRZAAAgGlx1xIAwDy4KxF/Qo8MAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLcODzI8//qinnnpK1apVk5eXl5o2bardu3fb1lutVk2cOFHBwcHy8vJSdHS0jh07ZmDFAADAWRgaZC5cuKA2bdrIzc1N//nPf3To0CHNmjVLt912m22bmTNnas6cOVqwYIESExPl4+OjLl26KDc318DKAQCAM6hk5MlnzJih0NBQxcfH29pq165t+9pqtWr27Nl65ZVX9NBDD0mS3nvvPQUGBmr9+vV68skny71mAADgPAztkdmwYYNatmypxx57TAEBAYqMjNSiRYts61NTU5WWlqbo6Ghbm5+fn1q1aqUdO3Zc9Zh5eXnKysqy+wAAgIrJ0CBz8uRJzZ8/X+Hh4dq8ebOGDRum4cOHa/ny5ZKktLQ0SVJgYKDdfoGBgbZ1fxYXFyc/Pz/bJzQ0tGwvAgAAGMbQIFNUVKS77rpL06ZNU2RkpJ555hkNHTpUCxYsKPExY2NjlZmZafucPn26FCsGAADOpFhzZObMmVPsAw4fPrzY2wYHB6tRo0Z2bQ0bNtSaNWskSUFBQZKk9PR0BQcH27ZJT09X8+bNr3pMDw8PeXh4FLsGAABgXsUKMm+99Zbd8rlz55STkyN/f39JUkZGhry9vRUQEOBQkGnTpo2OHDli13b06FHdcccdkn6b+BsUFKSEhARbcMnKylJiYqKGDRtW7PMAAICKqVhDS6mpqbbP1KlT1bx5c6WkpOj8+fM6f/68UlJSdNddd+m1115z6OQjR47Uzp07NW3aNB0/flwrV67U3//+d8XExEiSLBaLRowYoddff10bNmzQwYMH1a9fP4WEhKhXr14OXywAAKhYHL79esKECfr3v/+t+vXr29rq16+vt956S48++qj69u1b7GPdfffdWrdunWJjYzVlyhTVrl1bs2fPtjvG2LFjlZ2drWeeeUYZGRlq27atNm3aJE9PT0dLBwAAFYzDQebMmTO6fPnyFe2FhYVKT093uIAePXqoR48e11xvsVg0ZcoUTZkyxeFjAwCAis3hu5Y6deqkZ599Vnv27LG1JScna9iwYXbPewEAAChrDgeZpUuXKigoSC1btrTdIXTPPfcoMDBQixcvLosaAQAArsrhoaXq1avrk08+0dGjR3X48GFJUoMGDVSvXr1SLw4AAOB6SvyupbCwMFmtVtWpU0eVKhn6yiYAAHCLcnhoKScnR4MHD5a3t7caN26sU6dOSZJeeOEFTZ8+vdQLBAAAuBaHg0xsbKz279+vrVu32t0CHR0drX/+85+lWhwAAMD1ODwmtH79ev3zn//UvffeK4vFYmtv3LixTpw4UarFAQAAXI/DPTLnzp1TQEDAFe3Z2dl2wQYAAKCsORxkWrZsqY0bN9qWfw8vixcvVlRUVOlVBgAAcAMODy1NmzZN3bp106FDh3T58mW9/fbbOnTokL7++mtt27atLGoEAAC4Kod7ZNq2bav9+/fr8uXLatq0qT799FMFBARox44datGiRVnUCAAAcFUO9cgUFBTo2Wef1YQJE7Ro0aKyqgkAAKBYHOqRcXNz05o1a8qqFgAAAIc4PLTUq1cvrV+/vgxKAQAAcIzDk33Dw8M1ZcoUbd++XS1atJCPj4/d+uHDh5dacQAAANfjcJBZsmSJ/P39lZycrOTkZLt1FouFIAMAAMqNw0EmNTW1LOoAAABwmMNzZH6Xn5+vI0eO6PLly6VZDwAAQLHx9msAAGBavP0aAACYFm+/BgAApsXbrwEAgGnx9msAAGBavP0aAACYVonefr1v3z7efg0AAAzncI+MJNWpU4e3XwMAAMM53CMTHR2tZcuWKSsrqyzqAQAAKDaHg0zjxo0VGxuroKAgPfbYY/rwww9VUFBQFrUBAABcl8NB5u2339aPP/6o9evXy8fHR/369VNgYKCeeeYZJvsCAIByVaJ3Lbm4uOj+++/XsmXLlJ6eroULF2rXrl3q2LFjadcHAABwTSWa7Pu7tLQ0rVq1Sv/4xz904MAB3XPPPaVVFwAAwA053COTlZWl+Ph4de7cWaGhoZo/f74efPBBHTt2TDt37iyLGgEAAK7K4R6ZwMBA3XbbbXriiScUFxenli1blkVdAAAAN+RwkNmwYYM6deokF5cSTa8BAAAoNQ4Hmc6dO0v67eWRR44ckSTVr19f1atXL93KAAAAbsDhbpWcnBwNGjRIwcHBateundq1a6eQkBANHjxYOTk5ZVEjAADAVTkcZEaOHKlt27bpo48+UkZGhjIyMvThhx9q27ZtGj16dFnUCAAAcFUODy2tWbNG//73v9WhQwdbW/fu3eXl5aXHH39c8+fPL836AAAArqlEQ0uBgYFXtAcEBDC0BAAAypXDQSYqKkqTJk1Sbm6ure3SpUt69dVXFRUVVarFAQAAXI/DQ0tvv/22unTpopo1a6pZs2aSpP3798vT01ObN28u9QIBAACuxeEg06RJEx07dkwrVqzQ4cOHJUl9+vRR37595eXlVeoFAgAAXEuJ3rXk7e2toUOHlnYtAAAADnF4jkxcXJyWLl16RfvSpUs1Y8aMUikKAACgOBwOMgsXLlSDBg2uaG/cuLEWLFhQKkUBAAAUh8NBJi0tTcHBwVe0V69eXWfOnCmVogAAAIrD4SATGhqq7du3X9G+fft2hYSElEpRAAAAxeHwZN+hQ4dqxIgRKigoUMeOHSVJCQkJGjt2LK8oAAAA5crhIPPSSy/pl19+0XPPPaf8/HxJkqenp8aNG6fY2NhSLxAAAOBaHA4yFotFM2bM0IQJE5SSkiIvLy+Fh4fLw8OjLOoDAAC4JofnyPwuLS1N58+fV506deTh4SGr1VqadQEAANyQw0Hml19+UadOnVSvXj11797ddqfS4MGDmSMDAADKlcNBZuTIkXJzc9OpU6fk7e1ta3/iiSe0adOmUi0OAADgehyeI/Ppp59q8+bNqlmzpl17eHi4vv/++1IrDAAA4EYc7pHJzs6264n53fnz55nwCwAAypXDQeYvf/mL3nvvPduyxWJRUVGRZs6cqfvuu69UiwMAALgeh4eWZs6cqU6dOmn37t3Kz8/X2LFj9e233+r8+fNXfeIvAABAWXG4R6ZJkyY6evSo2rZtq4ceekjZ2dnq3bu39u7dqzp16pRFjQAAAFflcI+MJPn5+enll1+2a8vNzdUbb7yhMWPGlEphAAAAN+JQj8y5c+f08ccf69NPP1VhYaEkqaCgQG+//bbCwsI0ffr0MikSAADgaordI/PVV1+pR48eysrKksViUcuWLRUfH69evXqpUqVKmjx5svr371+WtQIAANgpdo/MK6+8ou7du+vAgQMaNWqUkpKS9PDDD2vatGk6dOiQ/ud//kdeXl5lWSsAAICdYgeZgwcP6pVXXlGTJk00ZcoUWSwWzZw5U48++mhZ1gcAAHBNxQ4yFy5c0O233y5J8vLykre3t5o0aVJmhQEAANyIQ3ctHTp0SGlpaZIkq9WqI0eOKDs7226biIiI0qsOAADgOhwKMp06dZLVarUt9+jRQ9JvT/e1Wq2yWCy2u5kAAADKWrGHllJTU3Xy5EmlpqZe8fm9/eTJkyUuZPr06bJYLBoxYoStLTc3VzExMapWrZp8fX31yCOPKD09vcTnAAAAFUuxe2TuuOOOMisiKSlJCxcuvGJYauTIkdq4caP+9a9/yc/PT88//7x69+7NqxAAAICkEryioLRdvHhRffv21aJFi3TbbbfZ2jMzM7VkyRK9+eab6tixo1q0aKH4+Hh9/fXX2rlzp4EVAwAAZ2F4kImJidEDDzyg6Ohou/bk5GQVFBTYtTdo0EC1atXSjh07rnm8vLw8ZWVl2X0AAEDFVKJ3LZWWVatWac+ePUpKSrpiXVpamtzd3eXv72/XHhgYaLtz6mri4uL06quvlnapAADACRnWI3P69Gm9+OKLWrFihTw9PUvtuLGxscrMzLR9Tp8+XWrHBgAAzuWmemR+/vlnJSYmqrCwUHfffbeCg4OLvW9ycrLOnj2ru+66y9ZWWFio//73v5o7d642b96s/Px8ZWRk2PXKpKenKygo6JrH9fDwkIeHR4muBwAAmEuJg8yaNWs0ePBg1atXTwUFBTpy5IjmzZungQMHFmv/Tp066eDBg3ZtAwcOVIMGDTRu3DiFhobKzc1NCQkJeuSRRyRJR44c0alTpxQVFVXSsgEAQAVS7CBz8eJF+fr62pZfffVV7dq1S/Xq1ZMkbdy4UUOHDi12kKlcufIVrzjw8fFRtWrVbO2DBw/WqFGjVLVqVVWpUkUvvPCCoqKidO+99xa3bAAAUIEVe45MixYt9OGHH9qWK1WqpLNnz9qW09PT5e7uXqrFvfXWW+rRo4ceeeQRtWvXTkFBQVq7dm2pngMAAJhXsXtkNm/erJiYGC1btkzz5s3T22+/rSeeeEKFhYW6fPmyXFxctGzZspsqZuvWrXbLnp6emjdvnubNm3dTxwUAABVTsYNMWFiYNm7cqA8++EDt27fX8OHDdfz4cR0/flyFhYVq0KBBqd59BAAAcCMO337dp08fJSUlaf/+/erQoYOKiorUvHlzQgwAACh3Dt219MknnyglJUXNmjXT4sWLtW3bNvXt21fdunXTlClT5OXlVVZ1AgAAXKHYPTKjR4/WwIEDlZSUpGeffVavvfaa2rdvrz179sjT01ORkZH6z3/+U5a1AgAA2Cl2kFm2bJk++eQTrVq1SklJSXr//fclSe7u7nrttde0du1aTZs2rcwKBQAA+LNiBxkfHx+lpqZK+u31An+eE9OoUSN9+eWXpVsdAADAdRQ7yMTFxalfv34KCQlR+/bt9dprr5VlXQAAADdU7Mm+ffv2VdeuXXXy5EmFh4df8VZqAACA8ubQXUvVqlVTtWrVyqoWAAAAhzj8HBkAAABnQZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmRZABAACmZWiQiYuL0913363KlSsrICBAvXr10pEjR+y2yc3NVUxMjKpVqyZfX1898sgjSk9PN6hiAADgTAwNMtu2bVNMTIx27typLVu2qKCgQPfff7+ys7Nt24wcOVIfffSR/vWvf2nbtm366aef1Lt3bwOrBgAAzqKSkSfftGmT3fKyZcsUEBCg5ORktWvXTpmZmVqyZIlWrlypjh07SpLi4+PVsGFD7dy5U/fee68RZQMAACfhVHNkMjMzJUlVq1aVJCUnJ6ugoEDR0dG2bRo0aKBatWppx44dhtQIAACch6E9Mn9UVFSkESNGqE2bNmrSpIkkKS0tTe7u7vL397fbNjAwUGlpaVc9Tl5envLy8mzLWVlZZVYzAAAwltP0yMTExOibb77RqlWrbuo4cXFx8vPzs31CQ0NLqUIAAOBsnCLIPP/88/r444/1xRdfqGbNmrb2oKAg5efnKyMjw2779PR0BQUFXfVYsbGxyszMtH1Onz5dlqUDAAADGRpkrFarnn/+ea1bt06ff/65ateubbe+RYsWcnNzU0JCgq3tyJEjOnXqlKKioq56TA8PD1WpUsXuAwAAKiZD58jExMRo5cqV+vDDD1W5cmXbvBc/Pz95eXnJz89PgwcP1qhRo1S1alVVqVJFL7zwgqKiorhjCQAAGBtk5s+fL0nq0KGDXXt8fLwGDBggSXrrrbfk4uKiRx55RHl5eerSpYvefffdcq4UAAA4I0ODjNVqveE2np6emjdvnubNm1cOFQEAADNxism+AAAAJUGQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApkWQAQAApmWKIDNv3jyFhYXJ09NTrVq10q5du4wuCQAAOAGnDzL//Oc/NWrUKE2aNEl79uxRs2bN1KVLF509e9bo0gAAgMGcPsi8+eabGjp0qAYOHKhGjRppwYIF8vb21tKlS40uDQAAGKyS0QVcT35+vpKTkxUbG2trc3FxUXR0tHbs2HHVffLy8pSXl2dbzszMlCRlZWWVbbFOqCgvx+gSDJFlsRpdgjFuwZ/xWxm/37eYW/D3+/e/21br9f/NnTrI/PzzzyosLFRgYKBde2BgoA4fPnzVfeLi4vTqq69e0R4aGlomNcL5+BldgFGm37JXjlvILftTfgv/fv/666/y87v29Tt1kCmJ2NhYjRo1yrZcVFSk8+fPq1q1arJYLAZWhvKQlZWl0NBQnT59WlWqVDG6HACliN/vW4vVatWvv/6qkJCQ627n1EHm9ttvl6urq9LT0+3a09PTFRQUdNV9PDw85OHhYdfm7+9fViXCSVWpUoX/0AEVFL/ft47r9cT8zqkn+7q7u6tFixZKSEiwtRUVFSkhIUFRUVEGVgYAAJyBU/fISNKoUaPUv39/tWzZUvfcc49mz56t7OxsDRw40OjSAACAwZw+yDzxxBM6d+6cJk6cqLS0NDVv3lybNm26YgIwIP02tDhp0qQrhhcBmB+/37gai/VG9zUBAAA4KaeeIwMAAHA9BBkAAGBaBBkAAGBaBBkAAGBaBBkAAGBaTn/7NQDg1rFhw4Zib/vggw+WYSUwC26/hqldvnxZK1euVJcuXXi2EFABuLgUb6DAYrGosLCwjKuBGRBkYHre3t5KSUnRHXfcYXQpAIByxhwZmN4999yjffv2GV0GAMAAzJGB6T333HMaNWqUTp8+rRYtWsjHx8dufUREhEGVAbhZ2dnZ2rZtm06dOqX8/Hy7dcOHDzeoKjgThpZgelcbU7dYLLJarYyjAya2d+9ede/eXTk5OcrOzlbVqlX1888/y9vbWwEBATp58qTRJcIJ0CMD00tNTTW6BABlYOTIkerZs6cWLFggPz8/7dy5U25ubnrqqaf04osvGl0enAQ9MgAAp+Tv76/ExETVr19f/v7+2rFjhxo2bKjExET1799fhw8fNrpEOAF6ZFAhnDhxQrNnz1ZKSookqVGjRnrxxRdVp04dgysDUFJubm62oeOAgACdOnVKDRs2lJ+fn06fPm1wdXAW3LUE09u8ebMaNWqkXbt2KSIiQhEREUpMTFTjxo21ZcsWo8sDUEKRkZFKSkqSJLVv314TJ07UihUrNGLECDVp0sTg6uAsGFqC6UVGRqpLly6aPn26Xfv48eP16aefas+ePQZVBuBm7N69W7/++qvuu+8+nT17Vv369dPXX3+t8PBwLVmyRM2bNze6RDgBggxMz9PTUwcPHlR4eLhd+9GjRxUREaHc3FyDKgMAlDWGlmB61atXv+oD8fbt26eAgIDyLwhAqejYsaMyMjKuaM/KylLHjh3LvyA4JSb7wvSGDh2qZ555RidPnlTr1q0lSdu3b9eMGTM0atQog6sDUFJbt2694iF4kpSbm6svv/zSgIrgjAgyML0JEyaocuXKmjVrlmJjYyVJISEhmjx5Mk/+BEzowIEDtq8PHTqktLQ023JhYaE2bdqkGjVqGFEanBBzZFCh/Prrr5KkypUrG1wJgJJycXGRxWKRJF3tT5SXl5feeecdDRo0qLxLgxMiyMD0OnbsqLVr18rf39+uPSsrS7169dLnn39uTGEASuT777+X1WrVnXfeqV27dql69eq2de7u7goICJCrq6uBFcKZEGRgei4uLkpLS7tiYu/Zs2dVo0YNFRQUGFQZAKCsMUcGpsU4OlDx8dRu3Ag9MjAtxtGBim3z5s168MEH1bx5c7Vp00bSb3ck7t+/Xx999JE6d+5scIVwBgQZmBbj6EDFxlO7URwEGQCAU+Kp3SgOnuwL01u+fLk2btxoWx47dqz8/f3VunVrff/99wZWBuBm8NRuFAdBBqY3bdo0eXl5SZJ27NihuXPnaubMmbr99ts1cuRIg6sD4KgpU6YoJyfH9tTuGTNm6Msvv9SXX36p6dOn69lnn9XQoUONLhNOgqElmJ63t7cOHz6sWrVqady4cTpz5ozee+89ffvtt+rQoYPOnTtndIkAHODq6qozZ86oevXqmj17tmbNmqWffvpJ0m9P7X7ppZc0fPhw22R/3NrokYHp+fr66pdffpEkffrpp7Y7GTw9PXXp0iUjSwNQAr///7XFYtHIkSP1ww8/KDMzU5mZmfrhhx/04osvEmJgw3NkYHqdO3fWkCFDFBkZqaNHj6p79+6SpG+//VZhYWHGFgegRP4cVHjtCK6FIAPTmzdvnl555RWdPn1aa9asUbVq1SRJycnJ6tOnj8HVASiJevXq3bDX5fz58+VUDZwZc2QAAE7FxcVFs2fPlp+f33W369+/fzlVBGdGkEGF8OWXX2rhwoU6efKk/vWvf6lGjRp6//33Vbt2bbVt29bo8gA44FrvTwOuhsm+ML01a9aoS5cu8vLy0p49e5SXlydJyszM1LRp0wyuDoCjmMgLRxBkYHqvv/66FixYoEWLFsnNzc3W3qZNGx5hDpgQAwVwBJN9YXpHjhxRu3btrmj38/NTRkZG+RcE4KYUFRUZXQJMhB4ZmF5QUJCOHz9+RftXX32lO++804CKAADlhSAD0xs6dKhefPFFJSYmymKx6KefftKKFSs0ZswYDRs2zOjyAABliKElmN748eNVVFSkTp06KScnR+3atZOHh4fGjBmjF154wejyAABliNuvUWHk5+fr+PHjunjxoho1aiRfX1+jSwIAlDGCDEzvH//4h3r37i1vb2+jSwEAlDOCDEyvevXqunTpkh588EE99dRT6tKli1xdXY0uCwBQDpjsC9M7c+aMVq1aJYvFoscff1zBwcGKiYnR119/bXRpAIAyRo8MKpScnBytW7dOK1eu1GeffaaaNWvqxIkTRpcFACgj3LWECsXb21tdunTRhQsX9P333yslJcXokgAAZYihJVQIOTk5WrFihbp3764aNWpo9uzZevjhh/Xtt98aXRoAoAwxtATTe/LJJ/Xxxx/L29tbjz/+uPr27auoqCijywIAlAOGlmB6rq6uWr16NXcrAcAtiB4ZAABgWvTIoEJISEhQQkKCzp49e8Wbc5cuXWpQVQCAskaQgem9+uqrmjJlilq2bKng4GBZLBajSwIAlBOGlmB6wcHBmjlzpp5++mmjSwEAlDNuv4bp5efnq3Xr1kaXAQAwAEEGpjdkyBCtXLnS6DIAAAZgjgxMLzc3V3//+9/12WefKSIiQm5ubnbr33zzTYMqAwCUNebIwPTuu+++667/4osvyqkSAEB5I8gAAADTYmgJptW7d+8bbmOxWLRmzZpyqAYAYASCDEzLz8/P6BIAAAZjaAkAAJgWt18DAADTIsgAAADTIsgAAADTIsgAqDAsFovWr18vSfruu+9ksVi0b98+Q2sCULYIMgDKxIABA2SxWK74HD9+vFzOHxoaqjNnzqhJkyblcj4AxuD2awBlpmvXroqPj7drq169ermc29XVVUFBQeVyLgDGoUcGQJnx8PBQUFCQ3Wfw4MHq1auX3XYjRoxQhw4dbMsdOnTQ8OHDNXbsWFWtWlVBQUGaPHmy3T7Hjh1Tu3bt5OnpqUaNGmnLli126/88tLR161ZZLBYlJCSoZcuW8vb2VuvWrXXkyBG7/V5//XUFBASocuXKGjJkiMaPH6/mzZuX0ncEQGkjyABwSsuXL5ePj48SExM1c+ZMTZkyxRZWioqK1Lt3b7m7uysxMVELFizQuHHjinXcl19+WbNmzdLu3btVqVIlDRo0yLZuxYoVmjp1qmbMmKHk5GTVqlVL8+fPL5PrA1A6GFoCUGY+/vhj+fr62pa7desmHx+fYu0bERGhSZMmSZLCw8M1d+5cJSQkqHPnzvrss890+PBhbd68WSEhIZKkadOmqVu3bjc87tSpU9W+fXtJ0vjx4/XAAw8oNzdXnp6eeueddzR48GANHDhQkjRx4kR9+umnunjxokPXDaD80CMDoMzcd9992rdvn+0zZ86cYu8bERFhtxwcHKyzZ89KklJSUhQaGmoLMZIUFRXl8HGDg4MlyXbcI0eO6J577rHb/s/LAJwLPTIAyoyPj4/q1q1r1+bi4qI/vxmloKDgin3d3Nzsli0Wi4qKim66pj8e12KxSFKpHBeAMeiRAVCuqlevrjNnzti1Ofqsl4YNG+r06dN2x9m5c+dN11a/fn0lJSXZtf15GYBzIcgAKFcdO3bU7t279d577+nYsWOaNGmSvvnmG4eOER0drXr16ql///7av3+/vvzyS7388ss3XdsLL7ygJUuWaPny5Tp27Jhef/11HThwwNZzA8D5EGQAlKsuXbpowoQJGjt2rO6++279+uuv6tevn0PHcHFx0bp163Tp0iXdc889GjJkiKZOnXrTtfXt21exsbEaM2aM7rrrLqWmpmrAgAHy9PS86WMDKBsW658HqwEANp07d1ZQUJDef/99o0sBcBVM9gWA/5OTk6MFCxaoS5cucnV11QcffKDPPvvsioftAXAe9MgAwP+5dOmSevbsqb179yo3N1f169fXK6+8ot69extdGoBrIMgAAADTYrIvAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwLYIMAAAwrf8HKjnapLHCXLkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'Funding': ['Investor', 'Total'],\n",
    "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
    "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df_bar = pd.DataFrame(data)\n",
    "display(df_bar)\n",
    "\n",
    "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position  in 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, any fully paid loans and charged off loans need to be excluded from consideration. Only those current loans and (potentially) those in default not yet charged off are going to bring in more money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        id  member_id  loan_amount  funded_amount  \\\n",
      "0               0   8588502   10330306         5000           5000   \n",
      "1               4   8148555    4640566        12000          12000   \n",
      "2               6   8977266   10779272        10000          10000   \n",
      "3              11   8629345   10401156         4375           4375   \n",
      "4              12  38361412   41145143         4000           4000   \n",
      "...           ...       ...        ...          ...            ...   \n",
      "19263       54213   7055315    8717456        11400          11400   \n",
      "19264       54214   7049426    8711455         7800           7800   \n",
      "19265       54215   6805066    8427208        16000          16000   \n",
      "19266       54222   8627224   10399185         7200           7200   \n",
      "19267       54223   8148232    9890178        12000          12000   \n",
      "\n",
      "       funded_amount_inv  term(mths)  int_rate  instalment grade  ...  \\\n",
      "0                 5000.0          36      7.90      156.46     A  ...   \n",
      "1                12000.0          36     13.98      410.02     C  ...   \n",
      "2                10000.0          36     13.67      340.18     B  ...   \n",
      "3                 4375.0          36     13.67      148.83     B  ...   \n",
      "4                 4000.0          36      6.99      123.50     A  ...   \n",
      "...                  ...         ...       ...         ...   ...  ...   \n",
      "19263            11350.0          60     16.78      281.98     C  ...   \n",
      "19264             7800.0          36     21.00      293.87     E  ...   \n",
      "19265            16000.0          36     18.85      585.29     D  ...   \n",
      "19266             7200.0          36     17.10      257.06     C  ...   \n",
      "19267            12000.0          36     10.99      392.81     B  ...   \n",
      "\n",
      "      total_rec_late_fee recoveries collection_recovery_fee  \\\n",
      "0                    0.0        0.0                     0.0   \n",
      "1                    0.0        0.0                     0.0   \n",
      "2                    0.0        0.0                     0.0   \n",
      "3                    0.0        0.0                     0.0   \n",
      "4                    0.0        0.0                     0.0   \n",
      "...                  ...        ...                     ...   \n",
      "19263                0.0        0.0                     0.0   \n",
      "19264                0.0        0.0                     0.0   \n",
      "19265                0.0        0.0                     0.0   \n",
      "19266                0.0        0.0                     0.0   \n",
      "19267                0.0        0.0                     0.0   \n",
      "\n",
      "       last_payment_date last_payment_amount last_credit_pull_date  \\\n",
      "0             2022-01-01              156.46            2022-01-01   \n",
      "1             2022-01-01              410.02            2022-01-01   \n",
      "2             2022-01-01              340.18            2022-01-01   \n",
      "3             2022-01-01              148.83            2022-01-01   \n",
      "4             2022-01-01              123.50            2022-01-01   \n",
      "...                  ...                 ...                   ...   \n",
      "19263         2022-01-01              281.98            2022-01-01   \n",
      "19264         2021-12-01              293.87            2022-01-01   \n",
      "19265         2022-01-01              585.29            2022-01-01   \n",
      "19266         2022-01-01              257.06            2022-01-01   \n",
      "19267         2022-01-01              392.81            2022-01-01   \n",
      "\n",
      "      collections_12_mths_ex_med  policy_code application_type  total_expected  \n",
      "0                              0            1       INDIVIDUAL         5632.56  \n",
      "1                              0            1       INDIVIDUAL        14760.72  \n",
      "2                              0            1       INDIVIDUAL        12246.48  \n",
      "3                              0            1       INDIVIDUAL         5357.88  \n",
      "4                              0            1       INDIVIDUAL         4446.00  \n",
      "...                          ...          ...              ...             ...  \n",
      "19263                          1            1       INDIVIDUAL        16918.80  \n",
      "19264                          1            1       INDIVIDUAL        10579.32  \n",
      "19265                          1            1       INDIVIDUAL        21070.44  \n",
      "19266                          2            1       INDIVIDUAL         9254.16  \n",
      "19267                          2            1       INDIVIDUAL        14141.16  \n",
      "\n",
      "[19268 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "df_current = df.loc[df['loan_status'] == 'Current']\n",
    "df_current = df_current.reset_index(drop=True)\n",
    "print(df_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The % of expected funded amount recovered is: 99.02%\n",
      "The % of expected invested amount recovered is: 97.33%\n",
      "The % of funded amount recovered is: 128.7%\n",
      "The % of invested amount recovered is: 128.53%\n"
     ]
    }
   ],
   "source": [
    "#creates new column with 6 months worth of payments added to the existing payments\n",
    "df_current['total_payment_six'] = df_current['total_payment'] + 6 * df_current['instalment']\n",
    "\n",
    "#calculate amount to be added to previously exitisng total\n",
    "payments_to_add = df_current['total_payment_six'].sum()\n",
    "\n",
    "#edit preexisting totals \n",
    "\n",
    "total_recovered_fun = total_recovered_fun + payments_to_add\n",
    "total_recovered_inv = total_recovered_inv + payments_to_add\n",
    "\n",
    "#print percentages by expected funded amount and by investor\n",
    "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
    "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
    "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
    "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
    "\n",
    "#print percentages of loaned amounts and invested amounts\n",
    "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
    "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
    "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
    "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These extra payments make a huge different to the state of the loans. The bank has recovered more than 90% of what it expected and has now made a profit on the amount they loaned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Funding</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Percent Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investor</td>\n",
       "      <td>128.70</td>\n",
       "      <td>99.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total</td>\n",
       "      <td>128.53</td>\n",
       "      <td>97.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Funding  Percent  Percent Expected\n",
       "0  Investor   128.70             99.02\n",
       "1     Total   128.53             97.33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Funding', ylabel='% Recovered'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'Funding': ['Investor', 'Total'],\n",
    "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
    "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df_bar = pd.DataFrame(data)\n",
    "display(df_bar)\n",
    "\n",
    "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company wants to check what percentage of loans have been a loss to the company:\n",
    "\n",
    "\n",
    "Loans marked as Charged Off in the loan_status column represent a loss to the company.\n",
    "\n",
    "\n",
    "Calculate the percentage of charged off loans and the total amount that was paid towards these loans before being charged off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The % of loans charged off is: 10.27%\n"
     ]
    }
   ],
   "source": [
    "#extract those loans marked as Charged off\n",
    "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
    "df_charged_off.reset_index\n",
    "\n",
    "#count rows for percentage required\n",
    "no_of_loans = len(df)\n",
    "no_of_charged_off = len(df_charged_off)\n",
    "\n",
    "#calculating percentage\n",
    "pct_charged_off = round(no_of_charged_off/no_of_loans*100, 2)\n",
    "\n",
    "print(f'The % of loans charged off is: {pct_charged_off}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that about 1 in 10 loans was charged off by the bank. Lets visualize this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loans_not_charged_off = len(df)-len(df_charged_off)\n",
    "\n",
    "visual.pie_chart([\"Loans Charged Off\", \"Loans not charged off\"], [no_of_charged_off,loans_not_charged_off], \"Pie Chart showing percentgae of loans charged off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total amount paid towards these charged off loans can be summarised simply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount paid on loans before they were charged off is : $37400589.04\n"
     ]
    }
   ],
   "source": [
    "total_paid_charged_off = round(df_charged_off['total_payment'].sum(),2)\n",
    "print(f'The total amount paid on loans before they were charged off is : ${total_paid_charged_off}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating projected loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to visualize this as a proportion of what was expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount lost on loans  charged off is : $64668770.84\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "amount_lost = round(df_charged_off['total_expected'].sum() - df_charged_off['total_payment'].sum(),2)\n",
    "print(f'The total amount lost on loans  charged off is : ${amount_lost}')\n",
    "\n",
    "visual.pie_chart([\"paid on charged off loans\",\"not paid on charged off loans\"],[total_paid_charged_off, amount_lost], \"Percentage of money recovered from charged off loans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this Pie Chart, it seems that when it comes to charged off loans, the bank only recovers just over a third of their money in payments. However, this does not include any fees recovered to lessen the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 2/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 3/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 3/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "page = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)#Methods')\n",
      "html = page.text # Get the content of the webpage\n",
      "soup = BeautifulSoup(html, 'html.parser') # Convert that into a BeautifulSoup object that contains methods to make the tag searcg easier\n",
      " 3/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "page = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)#Methods')\n",
      "html = page.text # Get the content of the webpage\n",
      "soup = BeautifulSoup(html, 'html.parser') # Convert that into a BeautifulSoup object that contains methods to make the tag searcg easier\n",
      " 3/4:\n",
      "\n",
      "#methods = soup.find('div', attrs={'class': 'mw-parser-output'})\n",
      "methods = soup.find_all('p')\n",
      "print(methods[28].text)\n",
      " 3/5:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 3/6:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 3/7:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/2:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/3:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "driver = webdriver.Firefox() \n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/4:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "from selenium.webdriver.firefox.options import Options\n",
      "options = Options()\n",
      "driver = webdriver.Firefox(options=options)\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/5:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "profile = FirefoxProfile(~/.mozilla/firefox)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/6:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "profile = FirefoxProfile(/.mozilla/firefox)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/7:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "profile = FirefoxProfile(~/.mozilla/firefox)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/8:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "profile = FirefoxProfile(~/.mozilla/firefox)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 4/9:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/10:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "profile = FirefoxProfile(home/andrew/miniconda3/lib/python3.9)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/11:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "profile = FirefoxProfile('home/andrew/miniconda3/lib/python3.9')\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/12:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "profile = FirefoxProfile('home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox')\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/13:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "profile = FirefoxProfile(home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/14:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Firefox(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/15:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome()#(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/16:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "4/17:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(10) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(10)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61925305\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/2:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/3:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@dclass=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/4:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/5:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/6:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV cPJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/7:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/8:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"listing_description\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 5/9:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"listing_description\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "5/10:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//[@data-testid=\"listing_description\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "5/11:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//[@data-testid=\"listing_description\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//[@data-testid=\"listing_description\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/2:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/3:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-kQvhQW-centered-true c-PJLV-iPJLV-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/4:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//[@id=\"main-content\"]/div[1]/div[1]/div/div/div[6]/div[1]/div[1]/div[1]/div/div]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/5:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//[@id=\"main-content\"]/div[1]/div[1]/div/div/div[6]/div[1]/div[1]/div[1]/div/div]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/6:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//*[@id=\"main-content\"/div[1]/div[1]/div/div/div[6]/div[1]/div[1]/div[1]/div/div]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/7:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//*[@id=\"main-content\"/div[1]/div[1]/div/div/div[6]/div[1]/div[1]/div[1]/div/div]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/8:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//*[@id=\"main-content\"/div[1]/div[1]/div/div/div[6]/div[1]/div[1]/div[1]/div/div]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      " 6/9:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//*[@id=\"main-content\"/div[1]/div[1]/div/div/div[6]/div[1]/div[1]/div[1]/div/div]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "6/10:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "6/11:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "6/12:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "#profile = FirefoxProfile(/home/andrew/miniconda3/lib/python3.9/site-packages/selenium/webdriver/firefox)\n",
      "driver = webdriver.Chrome() #(firefox_profile=profile)\n",
      "\n",
      "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "driver.get(URL)\n",
      "time.sleep(3) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
      "try:\n",
      "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "    accept_cookies_button = driver.find_element(By.XPATH, '//*[@id=\"save\"]')\n",
      "    accept_cookies_button.click()\n",
      "\n",
      "except:\n",
      "    pass # If there is no cookies button, we won't find it, so we can pass\n",
      "\n",
      "driver.switch_to.default_content()\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "6/13:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      " 7/1:\n",
      "from selenium import webdriver\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/2:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/3:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/4:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      " 7/5:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/6:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"e1hkqbpo10\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/7:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value=\"//div[@class='css-1itfubx e1llytg50']//a[@data-testid='listing-details-link']\")\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/8:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_element(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 7/9:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    \n",
      "    link_list = []\n",
      "'''\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "#big_list = []\n",
      "'''\n",
      "'''\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "'''\n",
      "7/10:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "print(prop_container)\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    \n",
      "    link_list = []\n",
      "'''\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "#big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "'''\n",
      "7/11:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "7/12:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_container:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/1:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_container:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/2:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      " 8/3:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_container:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/4:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_container:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        print(link)\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/5:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_container:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        print(link)\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/6:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        print(link)\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/7:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx emkbo6y0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        print(link)\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/8:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        print(link)\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 8/9:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        print(link)\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/10:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/11:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@id=PaginationItemNext')\n",
      "    time.sleep(2)\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/12:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@id=PaginationItemNext')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/13:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//button[text()=\"Next\"]')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/14:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH,'//button[text()=\"Next\"]')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/15:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH,'//button[text()=\"Next\"]')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/16:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//button[text()=\"Next\"]')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/17:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*button[text()=\"Next\"]')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/18:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.LINK_TEXT, value = 'Next')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/19:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, //*@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2')\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/20:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, //*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"])\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/21:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/22:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/23:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/24:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/25:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/26:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      "8/27:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/28:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/29:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx et6shp90\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/30:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    #//*[@id=\"__next\"]/div[3]/div[2]/main/div[2]/div[3]/ul/li[7]\n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/31:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//*[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    close_button.click\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/32:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//*[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    close_button.click\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/33:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//*[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    close_button.click\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "8/34:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      " 9/2:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "    url = 'any_url'\n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/3:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "    url = 'any_url'\n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/4:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "    url = 'any_url'\n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/5:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      " 9/6:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx etfv7s70\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "    url = 'any_url'\n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/7:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "    url = 'any_url'\n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/8:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    #close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    #close_button.click\n",
      "   \n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      " 9/9:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    #close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    #close_button.click\n",
      "   \n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "9/10:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    #close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    #close_button.click\n",
      "   \n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/1:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      "10/2:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "print(prop_container)\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    \n",
      "    link_list = []\n",
      "'''\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "#big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "'''\n",
      "10/3:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    #close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    #close_button.click\n",
      "   \n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/4:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    #close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    #close_button.click\n",
      "   \n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/5:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    #close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    #close_button.click\n",
      "   \n",
      "\n",
      "\n",
      "    try:\n",
      "        WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "        alert = browser.switch_to_alert()\n",
      "        alert.dismiss()\n",
      "        \n",
      "\n",
      "    except TimeoutException:\n",
      "        pass\n",
      "    \n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/6:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/7:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e11mabic0\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/8:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//*[@id = \"modal-close\"]')\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/9:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      "10/10:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.ID,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/11:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "10/12:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/1:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/2:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      "11/3:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/4:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/5:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/6:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    time.sleep(2) \n",
      "    close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/7:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/8:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/9:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/10:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class*=\"css-1itfubx\"]')\n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/11:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[contains(@class,'css-litfubx')]')\n",
      "     \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/12:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[contains(@class,\"css-litfubx\")]')\n",
      "     \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/13:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/14:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[contains(@class=\"css-1itfubx\")]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    # if i>=1:\n",
      "    #     time.sleep(2) \n",
      "    #     close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #     #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    #     close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/15:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[contains(@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    # if i>=1:\n",
      "    #     time.sleep(2) \n",
      "    #     close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #     #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    #     close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/16:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    # if i>=1:\n",
      "    #     time.sleep(2) \n",
      "    #     close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #     #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    #     close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/17:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    # if i>=1:\n",
      "    #     time.sleep(2) \n",
      "    #     close_button = driver.find_element(by=By.XPATH,value = '//button[@class=\"css-e4jnh6-CancelButton e13xjwxo6\"]')\n",
      "    #     #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "    #     close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/18:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT ,value = \"close\"]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/19:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT ,value = \"close\"')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/20:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT ,\"close\")\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/21:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "11/22:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/1:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/2:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/3:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "import By\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/4:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/5:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/6:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/7:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "\n",
      "load_and_accept_cookies()\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/8:\n",
      "\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "time.sleep(3)\n",
      "property = driver.find_element(By.XPATH, '//*[@id=\"listing_61936410\"]')\n",
      "a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "print(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "print(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "print(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "print(description)\n",
      "\n",
      "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "dict_properties['Price'].append(price)\n",
      "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "dict_properties['Address'].append(address)\n",
      "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "dict_properties['Bedrooms'].append(bedrooms)\n",
      "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "description = span_tag.text\n",
      "dict_properties['Description'] = description\n",
      "\n",
      "dict_properties\n",
      "12/9:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/10:\n",
      "from selenium import webdriver\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "print(prop_container)\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "\n",
      "    \n",
      "    link_list = []\n",
      "'''\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "\n",
      "    return link_list\n",
      "\n",
      "#big_list = []\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
      "\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "'''\n",
      "12/11:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT(\"Close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/12:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.ID(\"modal-close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/13:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.ID(\"modal-close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/14:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.ID(\"modal-close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/15:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "browser = webdriver.Chrome()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.ID(\"modal-close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/16:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.ID(\"modal-close\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/17:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CLASS_NAME(\"css-e4jnh6-CancelButton e13xjwxo6\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/18:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CLASS_NAME('css-e4jnh6-CancelButton e13xjwxo6'))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/19:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CLASS_NAME(\"css-e4jnh6-CancelButton e13xjwxo6\"))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/20:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CLASS_NAME('css-e4jnh6-CancelButton e13xjwxo6'))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M22.494.445a.75.75 0 011.061 1.06L13.061 12l10.494 10.495a.75.75 0 11-1.06 1.06L12 13.061 1.505 23.555a.75.75 0 01-1.06-1.06L10.939 12 .445 1.505a.75.75 0 011.06-1.06L12 10.939 22.494.445z\" clip-rule=\"evenodd\"></path></svg></button>\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/21:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CLASS_NAME(css-e4jnh6-CancelButton e13xjwxo6'))\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/22:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CLASS_NAME, value='css-e4jnh6-CancelButton e13xjwxo6')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/23:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.CSS_SELECTOR, value='css-e4jnh6-CancelButton e13xjwxo6')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/24:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value='//button[@class=css-e4jnh6-CancelButton e13xjwxo6]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/25:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    \n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    if i>=1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value='//button[@class=css-e4jnh6-CancelButton e13xjwxo6]')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/26:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    \n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT, value=\"close\")\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click()\n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/27:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT, value=\"close\")\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/28:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.LINK_TEXT, value='close')\n",
      "        #<button aria-label=\"Close\" data-testid=\"modal-close\" title=\"close\" class=\"css-e4jnh6-CancelButton e13xjwxo6\"><svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/29:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "12/30:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/1:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/2:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/3:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(2)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/4:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/5:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/6:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx evm8r390\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/7:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "   \n",
      "\n",
      "    # '''\n",
      "    # try:\n",
      "    #     WebDriverWait(browser, 5).until(EC.alert_is_present(), 'Waiting for alert timed out')\n",
      "\n",
      "    #     alert = browser.switch_to_alert()\n",
      "    #     alert.dismiss()\n",
      "        \n",
      "\n",
      "    # except TimeoutException:\n",
      "    #     pass\n",
      "    # '''\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
      "    pass # This pass should be removed once the code is complete\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/8:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/9:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    print(i)\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(1)\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/10:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "print(big_list)\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(1)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/11:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    #print(big_list)\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "print(big_list)\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(2)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/12:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(3)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/13:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(3)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/14:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(3)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/15:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(3)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "13/16:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(4)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "14/1:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(4)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "14/2:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(4)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "14/3:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(5)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "14/4:\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "\n",
      "\n",
      "\n",
      "def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "    '''\n",
      "    Open Zoopla and accept the cookies\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    driver: webdriver.Chrome\n",
      "        This driver is already in the Zoopla webpage\n",
      "    '''\n",
      "    driver = webdriver.Chrome() \n",
      "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
      "    driver.get(URL)\n",
      "    time.sleep(3) \n",
      "    try:\n",
      "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        accept_cookies_button.click()\n",
      "        time.sleep(1)\n",
      "\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    driver.switch_to.default_content()\n",
      "\n",
      "    return driver \n",
      "\n",
      "def get_links(driver: webdriver.Chrome) -> list:\n",
      "    '''\n",
      "    Returns a list with all the links in the current page\n",
      "    Parameters\n",
      "    ----------\n",
      "    driver: webdriver.Chrome\n",
      "        The driver that contains information about the current page\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    link_list: list\n",
      "        A list with all the links in the page\n",
      "    '''\n",
      "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e1ha5oka0\"]')\n",
      "    #//div[contains(@class,'css-litfubx')] \n",
      "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
      "    #prop_container = driver.find_elements(by=By.XPATH, value=\"//div[@class='css-1itfubx emkbo6y0']//a[@data-testid='listing-details-link']\")\n",
      "    #print(prop_container)\n",
      "    link_list = []\n",
      "\n",
      "    for house_property in prop_list:\n",
      "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
      "        link = a_tag.get_attribute('href')\n",
      "        link_list.append(link)\n",
      "    \n",
      "    #print(link_list)\n",
      "\n",
      "    return link_list\n",
      "    \n",
      "driver = load_and_accept_cookies()\n",
      "\n",
      "big_list = []\n",
      "\n",
      "\n",
      "\n",
      "for i in range(5): # The first 5 pages only\n",
      "    time.sleep(5)\n",
      "    if i==1:\n",
      "        time.sleep(2) \n",
      "        close_button = driver.find_element(by=By.XPATH, value=\"//button[@class='css-e4jnh6-CancelButton e13xjwxo6']\")\n",
      "        close_button.click()\n",
      "        \n",
      "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
      "    next_button = driver.find_element(by=By.XPATH, value='//*[@class= \"css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2\"]')\n",
      "    \n",
      "    time.sleep(2)\n",
      "    next_button.click()\n",
      "\n",
      "\n",
      "for link in big_list:\n",
      "    time.sleep(5)\n",
      "    print(link)\n",
      "    #a_tag = property.find_element(By.TAG_NAME, 'a')\n",
      "    #link = a_tag.get_attribute('href')\n",
      "    driver.get(link)\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    print(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    print(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    print(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//*[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    print(description)\n",
      "\n",
      "    dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
      "    price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
      "    dict_properties['Price'].append(price)\n",
      "    address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
      "    dict_properties['Address'].append(address)\n",
      "    bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
      "    dict_properties['Bedrooms'].append(bedrooms)\n",
      "    div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
      "    span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
      "    description = span_tag.text\n",
      "    dict_properties['Description'] = description\n",
      "\n",
      "driver.quit() # Close the browser when you finish\n",
      "15/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "page = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)#Methods')\n",
      "html = page.text # Get the content of the webpage\n",
      "soup = BeautifulSoup(html, 'html.parser') # Convert that into a BeautifulSoup object that contains methods to make the tag searcg easier\n",
      "18/1:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__():\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "    \n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if 'name' == \"__main__\":\n",
      "    self.load_and_accept_cookies()\n",
      "18/2:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if 'name' == \"__main__\":\n",
      "    self.load_and_accept_cookies()\n",
      "18/3:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if 'name' == \"__main__\":\n",
      "    self.load_and_accept_cookies()\n",
      "18/4:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if 'name' == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/5:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if 'name' == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/6:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/7:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//button[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/8:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        try:\n",
      "            driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "        except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "            driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "            accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "            accept_cookies_button.click()\n",
      "            time.sleep(1)\n",
      "\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/9:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        drivers_button= driver.find_element(by=By.XPATH, value=\"//a[@href='driver']\")\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/10:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        driver = webdriver.Chrome() \n",
      "        drivers_button= driver.find_element(by=By.XPATH, value=\"//a[@href='driver']\")\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/11:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        driver = webdriver.Chrome() \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/12:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        driver = webdriver.Chrome() \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/13:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        driver = webdriver.Chrome() \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        pass\n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/14:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        driver = webdriver.Chrome() \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "       \n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/15:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        driver = webdriver.Chrome() \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/16:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/17:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams():\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/18:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    self.load_and_accept_cookies()\n",
      "    self..get_drivers()\n",
      "18/19:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    self.load_and_accept_cookies()\n",
      "    self.get_drivers()\n",
      "18/20:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/21:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/22:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies() -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers():\n",
      "        \n",
      "        drivers_button= scraper.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/23:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/24:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/25:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_elements(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/26:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/27:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']//a[@href='driver']\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/28:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/29:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/30:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "        get_drivers()\n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "18/31:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        \n",
      "        get_drivers()\n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "18/32:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        \n",
      "        self.get_drivers()\n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "18/33:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/34:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(3) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/35:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "18/36:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        # try:\n",
      "        #     driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_elementh(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "        # except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
      "        #     driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
      "        #     accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
      "        #     accept_cookies_button.click()\n",
      "        #     time.sleep(1)\n",
      "\n",
      "        # except:\n",
      "        #     pass\n",
      "\n",
      "        #driver.switch_to.default_content()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "        scraper.get_drivers()\n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/37:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value=\"Drivers\")\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/38:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT: \"Drivers\")\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/39:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/40:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        drivers_button= self.driver.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/41:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//*[class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/42:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//*[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/43:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "        \n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/44:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/45:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        #return driver \n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/46:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        return driver \n",
      "        \n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "\n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/47:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        scraper.get_drivers()\n",
      "\n",
      "        return driver \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        driver = webdriver.Chrome()\n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/48:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "        return driver \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        driver = webdriver.Chrome()\n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.load_and_accept_cookies()\n",
      "18/49:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "        return driver \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        driver = webdriver.Chrome()\n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.load_and_accept_cookies()\n",
      "18/50:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "        return driver \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        driver = webdriver.Chrome()\n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.load_and_accept_cookies()\n",
      "18/51:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "        return driver \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        driver = webdriver.Chrome()\n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.load_and_accept_cookies()\n",
      "18/52:\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "\n",
      "    def load_and_accept_cookies(self) -> webdriver.Chrome:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        driver = webdriver.Chrome() \n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "        accept_button = driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "        return driver \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        driver = webdriver.Chrome()\n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "       \n",
      "\n",
      "    def get_teams(self):\n",
      "        driver = webdriver.Chrome()\n",
      "        URL = \"https://www.4mula1stats.com/\"\n",
      "        driver.get(URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper()\n",
      "    \n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "18/53:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        \n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='div class=\"col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/54:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver.get(URL)\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        \n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/55:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        \n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/56:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        \n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/57:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        \n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/58:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/59:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(https://www.4mula1stats.com/, webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/60:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/61:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str):\n",
      "        \n",
      "        self.driver = webdriver.Chrome()\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\")\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "18/62:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "20/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data(self)\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "        print(starting_letter_tag)\n",
      "\n",
      "        # for driver in starting_letter_tag:\n",
      "\n",
      "        #     a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "        #     link = a_tag.get_attribute('href')\n",
      "        #     self.driver_list.append(link)\n",
      "        \n",
      "        # print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']//a\")\n",
      "        print(starting_letter_tag)\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "        print(starting_letter_tag)\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        print(self.driver_list)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            nationality_column = self.driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "            print(nationality_row)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/9:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            nationality_column = self.driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "            print(nationality_row)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "22/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "print(nationality_row)\n",
      "22/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "print(nationality_row.get_text)\n",
      "22/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "print(nationality_row.get_text())\n",
      "22/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "print(nationality_row.text)\n",
      "22/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.xpath(\"./..\"))\n",
      "print(parent.text)\n",
      "22/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.xpath('..'))\n",
      "print(parent.text)\n",
      "22/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.xpath(\".//ancestor::tr\"))\n",
      "print(parent.text)\n",
      "22/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.xpath(\".//ancestor::td\"))\n",
      "print(parent.text)\n",
      "22/9:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.XPATH(\"..\"))\n",
      "print(parent.text)\n",
      "22/10:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.XPATH(\"..\"))\n",
      "print(parent)\n",
      "22/11:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.XPATH(\"parent::td\"))\n",
      "print(parent)\n",
      "22/12:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.XPATH(\"parent::*\"))\n",
      "print(parent)\n",
      "22/13:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.findElement(by=By.XPATH(\"..\"))\n",
      "print(parent)\n",
      "22/14:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "parent = nationality_row.parent\n",
      "print(parent)\n",
      "22/15:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent//next\n",
      "print(nationality_row.text())\n",
      "22/16:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent//:next\n",
      "print(nationality_row.text())\n",
      "22/17:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent//next\n",
      "print(nationality_row.text())\n",
      "22/18:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent\n",
      "print(nationality_row.text())\n",
      "22/19:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent\n",
      "nationality = nationality_row.find_element(by=By.TAG_NAME, 'td'[1])\n",
      "print(nationality.text)\n",
      "22/20:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent\n",
      "nationality = nationality_row.find_element(by=By.TAG_NAME, 'td' )\n",
      "print(nationality.text)\n",
      "22/21:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent\n",
      "nationality = nationality_row.find_element(By.TAG_NAME,'td'[1])\n",
      "print(nationality.text)\n",
      "22/22:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent\n",
      "nationality = nationality_row\n",
      "print(nationality.text)\n",
      "22/23:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b').parent\n",
      "print(type(nationality_row))\n",
      "22/24:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_element(By.TAG_NAME, 'b')\n",
      "print(type(nationality_row))\n",
      "22/25:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(type(nationality_row))\n",
      "22/26:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[2])\n",
      "22/27:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[2]/text)\n",
      "22/28:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[2].text)\n",
      "22/29:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "20/10:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "20/11:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "22/30:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/gianni_morbidelli\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "22/31:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver =webdriver.Chrome()\n",
      "driver.get(\"https://www.4mula1stats.com/driver/ayrton_senna\")\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "23/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "25/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'something123'\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            your_element = WebDriverWait(self.driver, 10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH, value=\"//div[@class='col-md-6']\"))\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'something123'\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            your_element = WebDriverWait(self.driver, 10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH, \"//div[@class='col-md-6']\"))\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'something123'\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            your_element = WebDriverWait(self.driver, 10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(\"//div[@class='col-md-6']\"))\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'something123'\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            your_element = WebDriverWait(webdriver.Chrome, 10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(\"//div[@class='col-md-6']\"))\n",
      "            self.driver.get(link)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            time.sleep(3)\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1) \n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            self.driver.get(link)\n",
      "            my_element_id = \"col-md-6\"\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(my_element_id))\n",
      "            \n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            \n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(my_element_id))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH,  value=\"//div[@class='col-md-6']\"))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/9:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH, \"//div[@class='col-md-6']\"))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/10:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(\"//div[@class='col-md-6']\"))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/11:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH, \"//div[@class='col-md-6']\"))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/12:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            self.driver.refresh()\n",
      "            # ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            # WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH, \"//div[@class='col-md-6']\"))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/13:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH(\"//div[@class='col-md-6']\")))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/14:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.ID(my_element_id)))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/15:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.ID: my_element_ID))\n",
      "\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/16:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.ID, my_element_id))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/17:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(my_element_id))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/18:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located(By.XPATH, \"//div[@class='col-md-6']\"))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/19:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='col-md-6']\")))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/20:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='col-md-6']\")))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/21:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, \"//*div[@class='col-md-6']\")))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/22:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, \"//*div[@class='col-md-6']\")))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/23:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = 'col-md-6'\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, \"//*[@class='col-md-6']\")))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/24:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,10,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/25:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            \n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/26:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "28/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"www.4mula1stats.com\")\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "28/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"www.4mula1stats.com\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "28/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"4mula1stats.com\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "28/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "28/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//*div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "28/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "27/27:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "28/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "27/28:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "27/29:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "30/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for driver in starting_letter_tag:\n",
      "\n",
      "            a_tag = driver.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "30/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag:\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            nationality_column = self.driver.find_element(by=By.XPATH, value=\"//div[@class='col-md-6']\")\n",
      "            nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "            print(nationality_row[1].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "29/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "nationality_row = nationality_column.find_elements(By.TAG_NAME, 'td')\n",
      "print(nationality_row[1].text)\n",
      "29/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "print(nationality_column.text)\n",
      "29/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "print(nationality_column)\n",
      "29/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "nationality_column = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(nationality_column)):\n",
      "    print(nationality_column[i].text)\n",
      "29/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column2_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column2_data)):\n",
      "    print(column2_data[i].text)\n",
      "column3_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column3_data)):\n",
      "    print(column3_data[i].text)\n",
      "30/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag:\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data)):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data)):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "33/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "play_button = driver.find_element(by=By.ID value=//button[@id=\"button-play\"])\n",
      "play_button.click()\n",
      "33/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "play_button = driver.find_element(by=By.ID, value=//button[@id=\"button-play\"])\n",
      "play_button.click()\n",
      "33/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id=\"button-play\"]\"\")\n",
      "play_button.click()\n",
      "33/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "34/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag:\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag:\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list:\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2)\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        pass\n",
      "\n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            img_src = self.driver.find_element(by=By.ID, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        pass\n",
      "\n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            img_src = self.driver.find_element(by=By.ID, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "        pass\n",
      "\n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/7:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Name.get_attribute(text)\n",
      "        print(Name)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/8:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Name.get_attribute(text)\n",
      "        print(Name)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "34/9:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Name.get_attribute(text)\n",
      "        print(Name)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "35/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "print(Name)\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "35/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "print(Name)\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "35/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = Name.split()[1:len(Name.split())]\n",
      "print(Driver_Surname)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "35/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \"\".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "35/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "37/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "uuid.uuid4()\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "37/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "uuid.uuid4()\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "37/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "print(uuid.uuid4())\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "37/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "print(uuid.uuid4())\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "36/1:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data)):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(column23_data)):\n",
      "                print(column23_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/2:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/3:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/4:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype=\"http://schema.org/SportsTeam\"]//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/5:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam'//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/6:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemscope itemtype='http://schema.org/SportsTeam'//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/7:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_element(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemscope itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/8:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_element(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/9:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/10:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "36/11:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            uuid.uuid4()\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(column1_data),2):\n",
      "                print(column1_data[i].text)\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(1,len(column23_data),2):\n",
      "                print(column23_data[i].text)\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "37/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "print(uuid.uuid4())\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "37/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "print(dict_entry)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data)):\n",
      "    print(column1_data[i].text)\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data)):\n",
      "    print(column23_data[i].text)\n",
      "37/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "print(dict_entry)\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "    for j in range(1,len(column1_data),2):\n",
      "        dict_entry[column1_data[i]] = column1_data[j]\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "# for i in range(0,len(column23_data)):\n",
      "#     print(column23_data[i].text)\n",
      "37/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "    for j in range(1,len(column1_data),2):\n",
      "        dict_entry[column1_data[i]] = column1_data[j]\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "# for i in range(0,len(column23_data)):\n",
      "#     print(column23_data[i].text)\n",
      "print(dict_entry)\n",
      "37/9:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "    for j in range(1,len(column1_data),2):\n",
      "        dict_entry[column1_data[i].text] = column1_data[j].text\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "# for i in range(0,len(column23_data)):\n",
      "#     print(column23_data[i].text)\n",
      "print(dict_entry)\n",
      "37/10:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "    for j in range(1,len(column1_data),2):\n",
      "        dict_entry[column1_data[i].text] = column1_data[j].text\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "    for j in range(1,len(column1_data),2):\n",
      "        dict_entry[column23_data[i].text] = column23_data[j].text\n",
      "print(dict_entry)\n",
      "37/11:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "    for j in range(1,len(column1_data),2):\n",
      "        dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "# for i in range(0,len(column1_data),2):\n",
      "#     #print(column1_data[i].text)\n",
      "#     for j in range(1,len(column1_data),2):\n",
      "#         dict_entry[column23_data[i].text] = column23_data[j].text\n",
      "print(dict_entry)\n",
      "37/12:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "        dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "# for i in range(0,len(column1_data),2):\n",
      "#     #print(column1_data[i].text)\n",
      "#     for j in range(1,len(column1_data),2):\n",
      "#         dict_entry[column23_data[i].text] = column23_data[j].text\n",
      "print(dict_entry)\n",
      "37/13:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "import uuid\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "website = \"https://www.4mula1stats.com/\"\n",
      "driver.get(website)\n",
      "navbar = driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "drivers_button.click()\n",
      "\n",
      "starting_letter_tag = driver.find_element(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "a_tag = starting_letter_tag.find_element(By.TAG_NAME, 'a')\n",
      "link = a_tag.get_attribute('href')\n",
      "print(link)\n",
      "driver.get(link)\n",
      "driver_dict = {}\n",
      "dict_entry = {}\n",
      "\n",
      "Page_Title = driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "Name = Page_Title.text\n",
      "Name.replace(\"Formula 1 Driver\", \"\")\n",
      "Driver_Forename = Name.split()[0]\n",
      "print(Driver_Forename)\n",
      "Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "print(Driver_Surname)\n",
      "ID = uuid.uuid4()\n",
      "dict_entry[\"ID\"] = ID\n",
      "dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "\n",
      "\n",
      "column1_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "for i in range(0,len(column1_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "        dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "column23_data = driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "for i in range(0,len(column23_data),2):\n",
      "    #print(column1_data[i].text)\n",
      "        dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "print(dict_entry)\n",
      "36/12:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.dict_entry = {}\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "        ID = uuid.uuid4()\n",
      "        self.dict_entry[\"ID\"] = ID\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "            dict_entry={}\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    scraper.get_champs()\n",
      "36/13:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.dict_entry = {}\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        print(Driver_Surname)\n",
      "        ID = uuid.uuid4()\n",
      "        self.dict_entry[\"ID\"] = ID\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "            dict_entry={}\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/14:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.dict_entry = {}\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        #print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        #print(Driver_Surname)\n",
      "        ID = uuid.uuid4()\n",
      "        self.dict_entry[\"ID\"] = ID\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "            dict_entry={}\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(1)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/15:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.dict_entry = {}\n",
      "        time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        #time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            #time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        #print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        #print(Driver_Surname)\n",
      "        ID = uuid.uuid4()\n",
      "        self.dict_entry[\"ID\"] = ID\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "            dict_entry={}\n",
      "            self.driver.get(link)\n",
      "            #time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        #time.sleep(1)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "36/16:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.dict_entry = {}\n",
      "        #time.sleep(1) \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        driver: webdriver.Chrome\n",
      "            This driver is already in the site webpage\n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def get_drivers(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        #time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            #time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        #print(Driver_Forename)\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "        #print(Driver_Surname)\n",
      "        # ID = uuid.uuid4()\n",
      "        # self.dict_entry[\"ID\"] = ID\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            #print(self.driver_list)\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "            self.dict_entry={}\n",
      "            self.driver.get(link)\n",
      "            #time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                #print(column1_data[i].text)\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "    def get_teams(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            #print(self.team_list)\n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #my_element_id = \"//*div[@class='col-md-6']\"\n",
      "\n",
      "            self.driver.get(link)\n",
      "            time.sleep(2) #to avoid being seen as a bot\n",
      "\n",
      "            #ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
      "            #WebDriverWait(self.driver,20,0.5,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH, my_element_id)))\n",
      "\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            uuid.uuid4()\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(1,len(team_history_data)):\n",
      "                print(team_history_data[i].text)\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(1,len(team_driver_data)):\n",
      "                print(team_driver_data[i].text)\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(1,len(team_data)):\n",
      "                print(team_data[i].text)\n",
      "        \n",
      "    def get_champs(self):\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        #time.sleep(1)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.get_drivers()\n",
      "    #scraper.get_teams()\n",
      "    #scraper.get_champs()\n",
      "    print(scraper.driver_dict)\n",
      "36/17:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        '''\n",
      "        Open site and accept the cookies\n",
      "        \n",
      "        '''\n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            #time.sleep(2) #to avoid being seen as a bot\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            \n",
      "            self.dict_entry={}\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            self.dict_entry={}\n",
      "\n",
      "            self.driver.get(link)\n",
      "            self.get_team_name() #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(1,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    #scraper.navigate_champs()\n",
      "    print(scraper.teams_dict)\n",
      "36/18:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data)):\n",
      "                print(champs_data[i].text)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    #scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    #print(scraper.teams_dict)\n",
      "36/19:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i]\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1]\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2]\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3]\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        print(self.champs_dict)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    #scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    #print(scraper.teams_dict)\n",
      "36/20:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        print(self.champs_dict)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    #scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    #print(scraper.teams_dict)\n",
      "36/21:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException#\n",
      "import uuid\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        print(self.champs_dict)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    print(scraper.driver_dict)\n",
      "    print(scraper.teams_dict)\n",
      "36/22:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        print(self.champs_dict)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"Data-Collection-Pipeline\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/\"\n",
      "    path = os.path.join(parent_dirr, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    # scraper.load_and_accept_cookies()\n",
      "    # scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/23:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        print(self.champs_dict)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"Data-Collection-Pipeline\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    # scraper.load_and_accept_cookies()\n",
      "    # scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/24:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for team in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.team_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        print(self.champs_dict)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"raw_data\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    # scraper.load_and_accept_cookies()\n",
      "    # scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/25:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "            #dump to json file\n",
      "            out_file = open(\"driver_data.json\", \"w\")\n",
      "            json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "            #dump to json file\n",
      "            out_file = open(\"teams_data.json\", \"w\")\n",
      "            json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"raw_data\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    print(scraper.driver_dict)\n",
      "    print(scraper.teams_dict)\n",
      "36/26:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"raw_data\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    print(scraper.driver_dict)\n",
      "    print(scraper.teams_dict)\n",
      "36/27:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4()] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4()] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"raw_data\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    print(scraper.driver_dict)\n",
      "    print(scraper.teams_dict)\n",
      "36/28:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"raw_data\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    print(scraper.driver_dict)\n",
      "    print(scraper.teams_dict)\n",
      "36/29:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        #self.get_driver_images()\n",
      "        self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            print(img)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    directory = \"raw_data\"\n",
      "    parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    path = os.path.join(parent_dir, directory)\n",
      "    raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "    print(scraper.driver_dict)\n",
      "    print(scraper.teams_dict)\n",
      "36/30:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "from urllib import request\n",
      "from urllib3 import request\n",
      "import urllib3\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = request.urlretrieve(img,\"driverimages.jpg\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/31:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "from urllib import request\n",
      "from urllib3 import request\n",
      "import urllib3\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = request.urlretrieve(img,\"driverimages.jpg\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/32:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "from urllib import request.RequestMethods\n",
      "from urllib3 import request.RequestMethods\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = request.urlretrieve(img,\"driverimages.jpg\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/33:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "from urllib import requests\n",
      "from urllib3 import requests\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = request.urlretrieve(img,\"driverimages.jpg\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/34:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "from urllib import request\n",
      "from urllib3 import request\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = request.urlretrieve(img,\"driverimages.jpg\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/35:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib3.request\n",
      "import urllib3.request\n",
      "import requests\n",
      "import shutil\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = requests.get(img, stream = True)\n",
      "            if image.status_code == 200:\n",
      "                with open(\"driverimages.jpg\", 'wb') as f:\n",
      "                    shutil.sopyfileobj(image.raw, f)\n",
      "            else:\n",
      "                print(\"Image not retrieved\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/36:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib3.request\n",
      "import urllib3.request\n",
      "import requests\n",
      "import shutil\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            image = requests.get(img, stream = True)\n",
      "            if image.status_code == 200:\n",
      "                with open(\"driverimages.jpg\", 'wb') as f:\n",
      "                    shutil.copyfileobj(image.raw, f)\n",
      "            else:\n",
      "                print(\"Image not retrieved\")\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/37:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            self.download_image(img, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images\", link)\n",
      "    \n",
      "            \n",
      "\n",
      "    def download_image(url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/38:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            self.download_image(img, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images\", link)\n",
      "    \n",
      "            \n",
      "\n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/39:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            self.download_image(img, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", link)\n",
      "    \n",
      "\n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "    \n",
      "    def get_driver_name(self): #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "36/40:\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.driver_list = []\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        drivers_button= navbar.find_element(by=By.LINK_TEXT, value = 'Drivers')\n",
      "        drivers_button.click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "        \n",
      "    def get_driver_images(self):\n",
      "\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "       \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\")\n",
      "            img = img_src.get_attribute('src')\n",
      "            self.download_image(img, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "\n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Name.replace(\"Formula 1 Driver\", \"\")\n",
      "        Driver_Forename = Name.split()[0]\n",
      "        Driver_Surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = Driver_Forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = Driver_Surname\n",
      "        return Driver_Forename + \"_\" + Driver_Surname\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        directory = \"driver_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            a_tag = pilot.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.driver_list.append(link)\n",
      "            \n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"driver_data.json\", \"w\")\n",
      "        json.dump(self.driver_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        time.sleep(2)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        teams_button= navbar.find_element(by=By.LINK_TEXT, value = 'Teams')\n",
      "        teams_button.click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Drivers Name and splits it into First name, Surname\n",
      "        \n",
      "\n",
      "        Page_Title = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\")\n",
      "        Name = Page_Title.text\n",
      "        Team_Name =Name.replace(\"Formula 1\", \"\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        directory = \"team_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        #collects all the drivers URLS in a list\n",
      "        for team in starting_letter_tag: \n",
      "\n",
      "            a_tag = team.find_element(By.TAG_NAME, 'a')\n",
      "            link = a_tag.get_attribute('href')\n",
      "            self.team_list.append(link)\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        out_file = open(\"teams_data.json\", \"w\")\n",
      "        json.dump(self.teams_dict, out_file, indent = 6)\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        directory = \"champs_data\"\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        out_file = open(\"champs_data.json\", \"w\")\n",
      "        json.dump(self.champs_dict, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "    # print(scraper.driver_dict)\n",
      "    # print(scraper.teams_dict)\n",
      "39/1:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_images()\n",
      "        #self.get_driver_data()\n",
      "\n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "        driver_list = []\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "            driver_list.append(link)\n",
      "\n",
      "        return driver_list\n",
      "        \n",
      "    def get_images(self,url_list):\n",
      "\n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "            self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "\n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "39/2:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_images()\n",
      "        #self.get_driver_data()\n",
      "\n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "        driver_list = []\n",
      "        \n",
      "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "            link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "            driver_list.append(link)\n",
      "\n",
      "        return driver_list\n",
      "        \n",
      "    def get_images(self,url_list):\n",
      "\n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "            self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "\n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "39/3:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_images()\n",
      "        #self.get_driver_data()\n",
      "\n",
      "        \n",
      "    def get_images(self,url_list):\n",
      "\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\"))\n",
      "\n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "            self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "39/4:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_images()\n",
      "        #self.get_driver_data()\n",
      "\n",
      "        \n",
      "    def get_images(self,url_list):\n",
      "\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\"))\n",
      "\n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "            self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    # scraper.navigate_teams()\n",
      "    # scraper.navigate_champs()\n",
      "39/5:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_images()\n",
      "        self.get_driver_data()\n",
      "\n",
      "        \n",
      "    def get_images(self,url_list):\n",
      "\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "            self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/6:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "\n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_images()\n",
      "        self.get_driver_data()\n",
      "\n",
      "        \n",
      "    def get_images(self):\n",
      "\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            self.driver.get(link)\n",
      "            img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "            self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    \n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "\n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/7:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/8:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in self.driver_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/9:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/10:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/11:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/12:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/13:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(dictionary, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/14:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list[:5]: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/15:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list[:5]: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/16:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list[:5]: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        print(self.driver_dict)\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/17:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list[:5]: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        print(self.driver_dict)\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in self.team_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/18:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list[:5]: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/19:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list[:5]: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/20:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        if directory.exists() = False;\n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/21:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        if directory.exists() == False;\n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/22:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        if directory.exists() == False:\n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/23:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        if directory.isfile().exists() == False:\n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/24:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        \n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            if path.exists() == False:\n",
      "                raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/25:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        \n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            if os.path.exists(path) == False:\n",
      "                raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome())\n",
      "    # directory = \"raw_data\"\n",
      "    # parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline\"\n",
      "    # path = os.path.join(parent_dir, directory)\n",
      "    # raw_data = os.mkdir(path)\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "39/26:\n",
      "import time\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import urllib.request\n",
      "import urllib\n",
      "from selenium.webdriver.common.by import By\n",
      "from cgitb import text\n",
      "from numpy import append\n",
      "from requests import request\n",
      "from selenium import webdriver\n",
      "\n",
      "class Scraper:\n",
      "\n",
      "    def __init__(self, URL : str, driver : webdriver.Chrome, parent_dir:str):\n",
      "        \n",
      "        self.driver = driver\n",
      "        self.team_list = []\n",
      "        self.URL = URL\n",
      "        self.driver_dict={}\n",
      "        self.teams_dict = {}\n",
      "        self.champs_dict = {}\n",
      "        self.dict_entry = {}\n",
      "        directory = \"raw_data\"\n",
      "        path = os.path.join(parent_dir, directory)\n",
      "        if os.path.exists(path) == False:\n",
      "            raw_data = os.mkdir(path)\n",
      "        \n",
      "\n",
      "    def load_and_accept_cookies(self) -> None:\n",
      "\n",
      "        #Open site and accept the cookies\n",
      "        \n",
      "        self.driver.get(self.URL)\n",
      "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
      "        accept_button.click()\n",
      "        \n",
      "    def navigate_drivers(self): # navigates to the list of drivers\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
      "        self.get_driver_data()\n",
      "\n",
      "    def get_image(self):\n",
      "\n",
      "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
      "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
      "    \n",
      "    def get_URL_list(self,element):\n",
      "\n",
      "            url_list = []\n",
      "            \n",
      "            starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
      "\n",
      "            for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
      "\n",
      "                link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
      "                url_list.append(link)\n",
      "\n",
      "            return url_list\n",
      "        \n",
      "    def download_image(self, url, file_path, file_name):\n",
      "        full_path = file_path + file_name + '.jpg'\n",
      "        urllib.request.urlretrieve(url, full_path)\n",
      "\n",
      "    def stripF1_text(self, tobereplaced):\n",
      "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
      "        return Name\n",
      "    \n",
      "    def get_driver_name(self) -> str: #gets the Drivers Name and splits it into First name, Surname and adds it to dictionary\n",
      "        \n",
      "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        driver_forename = Name.split()[0]\n",
      "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
      "\n",
      "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
      "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
      "        return driver_forename + \"_\" + driver_surname\n",
      "\n",
      "    def get_driver_data(self):\n",
      "        \n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"driver_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "\n",
      "        \n",
      "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each URL in the list\n",
      "            self.driver.get(link)\n",
      "\n",
      "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
      "            self.get_image()\n",
      "\n",
      "            #scrapes the data from the different columns\n",
      "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(column1_data),2):\n",
      "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
      "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
      "            for i in range(0,len(column23_data),2):\n",
      "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
      "\n",
      "            #add each entry as a nested dictionary\n",
      "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
      "\n",
      "    def navigate_teams(self): # navigates to the list of teams\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
      "        self.get_team_data()\n",
      "\n",
      "    def get_team_name(self): #gets the Team Name and strips the \"Formula 1\" from it\n",
      "        \n",
      "        Team_Name = self.stripF1_text(\"Formula 1 Driver\")\n",
      "        self.dict_entry[\"Team Name\"] = Team_Name\n",
      "        \n",
      "    def get_team_data(self):\n",
      "        \n",
      "        #creates directory for team data\n",
      "        self.create_dir(\"team_data\")\n",
      "\n",
      "        #find element containing individual driver URLS\n",
      "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
      "            \n",
      "        #loops through every URL in the list and scrapes the statistics\n",
      "        for link in url_list[:5]: \n",
      "            \n",
      "            #resets the dictionary entry to blank at the beginning of each URL\n",
      "            self.dict_entry={}\n",
      "\n",
      "            #opens each page in the list of URLs\n",
      "            self.driver.get(link)\n",
      "\n",
      "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
      "            self.get_team_name() \n",
      "\n",
      "            #scrape the data from the different tables on each page\n",
      "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
      "            for i in range(0,len(team_history_data),2):\n",
      "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
      "\n",
      "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
      "            for i in range(0,len(team_driver_data),2):\n",
      "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
      "\n",
      "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
      "            for i in range(0,len(team_data),2):\n",
      "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
      "            \n",
      "            #add each entry as a nested dictionary\n",
      "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "\n",
      "        #dump to json file\n",
      "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
      "\n",
      "    def navigate_champs(self): # navigates to the list of champions\n",
      "\n",
      "        self.driver.get(self.URL)\n",
      "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\")\n",
      "        champs_button= navbar.find_element(by=By.LINK_TEXT, value = 'Champions')\n",
      "        champs_button.click()\n",
      "        self.get_champs_data()\n",
      "\n",
      "    def get_champs_data(self):\n",
      "\n",
      "        #creates directory for driver data\n",
      "        self.create_dir(\"champs_data\")\n",
      "\n",
      "        #find elements that contain champion info\n",
      "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
      "\n",
      "        #loop through elements to separate into data by year\n",
      "        for i in range(0,len(champs_data),4):\n",
      "                self.dict_entry={}\n",
      "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
      "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
      "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
      "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
      "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
      "                \n",
      "        #dump to json file\n",
      "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
      "\n",
      "    def create_dir(self,directory):\n",
      "        \n",
      "            parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
      "            path = os.path.join(parent_dir, directory)\n",
      "            if os.path.exists(path) == False:\n",
      "                raw_data = os.mkdir(path)\n",
      "\n",
      "    def dumptojson(self, dictionary, out_file):\n",
      "        out_file = open(out_file, \"w\")\n",
      "        json.dump(dictionary, out_file, indent = 6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome(), \"/home/andrew/AICore_work/Data-Collection-Pipeline\")\n",
      "    scraper.load_and_accept_cookies()\n",
      "    #scraper.navigate_drivers()\n",
      "    scraper.navigate_teams()\n",
      "    scraper.navigate_champs()\n",
      "44/1:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "knownas = df.loc[df['Known as'].notnull()]\n",
      "\n",
      "print(df.head(5))\n",
      "44/2:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], format = \"%Y%m%d\")\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], format = \"%Y%m%d\")\n",
      "\n",
      "print(df.head(5))\n",
      "44/3:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce'))\n",
      "\n",
      "print(df.head(5))\n",
      "44/4:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "print(df.head(5))\n",
      "44/5:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "print(df.head(5))\n",
      "44/6:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "print(df.head(5))\n",
      "44/7:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "df[\"Debut Year\"] = first_race_data[0]\n",
      "df[\"Debut circuit\"] = first_race_data[1]\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "df[\"Final Year\"] = last_race_data[0]\n",
      "df[\"Final circuit\"] = last_race_data[1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "print(df.head(5))\n",
      "44/8:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data)\n",
      "# df[\"Debut Year\"] = first_race_data[0]\n",
      "# df[\"Debut circuit\"] = first_race_data[1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[0]\n",
      "# df[\"Final circuit\"] = last_race_data[1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/9:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[-1])\n",
      "print(first_race_data[0:-1])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/10:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[0:-1])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/11:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[0:-1])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/12:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[0:-2])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/13:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[:-2])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/14:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[0])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/15:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[:][0])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/16:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "#print(first_race_data[-1])\n",
      "print(first_race_data[0][0])\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/17:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[0:len(first_race_data)][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/18:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/19:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "for i in range (0:len(first_race_data)):\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/20:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "for i in range(0:len(first_race_data)):\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/21:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "for i in range[0:len(first_race_data)]:\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/22:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(type(first_race_data))\n",
      "# for i in range[0:len(first_race_data)]:\n",
      "#     print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/23:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(list(first_race_data))\n",
      "# for i in range[0:len(first_race_data)]:\n",
      "#     print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/24:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "for i in range[0:len(list(first_race_data))]:\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/25:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = list(df[\"First Race\"].str.split())\n",
      "for i in range[0:len(first_race_data)]:\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/26:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(list(first_race_data))\n",
      "# for i in range[0:len(first_race_data)]:\n",
      "#     print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/27:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[0])\n",
      "# for i in range[0:len(first_race_data)]:\n",
      "#     print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/28:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[0][0])\n",
      "# for i in range[0:len(first_race_data)]:\n",
      "#     print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/29:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[0][0])\n",
      "print(len(first_race_data))\n",
      "# for i in range[0:len(first_race_data)]:\n",
      "#     print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/30:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[0][0])\n",
      "print(len(first_race_data))\n",
      "for i in range[0:len(first_race_data)]:\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/31:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "print(first_race_data[0][0])\n",
      "print(len(first_race_data))\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][0])\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[-1]\n",
      "# df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# last_race_data = df[\"Last Race\"].str.split()\n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/32:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    df[\"Debut Year\"] = first_race_data[-1]\n",
      "    df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/33:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    #df[\"Debut Year\"] = first_race_data[-1]\n",
      "    #df[\"Debut circuit\"] = first_race_data[0:-1]\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/34:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/35:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    #df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/36:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/37:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data[0][0:-1])\n",
      "# for i in range(0,len(first_race_data)):\n",
      "#     print(first_race_data[i][-1])\n",
      "#     df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/38:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data[0][:-1])\n",
      "# for i in range(0,len(first_race_data)):\n",
      "#     print(first_race_data[i][-1])\n",
      "#     df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/39:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data[7][:-1])\n",
      "# for i in range(0,len(first_race_data)):\n",
      "#     print(first_race_data[i][-1])\n",
      "#     df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/40:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data[7][:-1].join())\n",
      "# for i in range(0,len(first_race_data)):\n",
      "#     print(first_race_data[i][-1])\n",
      "#     df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/41:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data[7][:-1].join(str(x)for x in list))\n",
      "# for i in range(0,len(first_race_data)):\n",
      "#     print(first_race_data[i][-1])\n",
      "#     df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"] = first_race_data[i][0:-1]\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "# \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/42:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "debut_circuit = ''.join(first_race_data[i][0:-1}])\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1}])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/43:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/44:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/45:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/46:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    # print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/47:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/48:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(i)\n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/49:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)-1):\n",
      "    print(i)\n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/50:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "\n",
      "for i in range(0,len(first_race_data)+1):\n",
      "    \n",
      "    print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/51:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data)\n",
      "\n",
      "for i in range(0,len(first_race_data)+1):\n",
      "    \n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/52:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data)\n",
      "\n",
      "#for i in range(0,len(first_race_data)+1):\n",
      "    \n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/53:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "print(first_race_data.to_list())\n",
      "\n",
      "#for i in range(0,len(first_race_data)+1):\n",
      "    \n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/54:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(type(first_race_data[i]))\n",
      "\n",
      "#for i in range(0,len(first_race_data)+1):\n",
      "    \n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/55:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(type(first_race_data[i]))\n",
      "\n",
      "#for i in range(0,len(first_race_data)+1):\n",
      "    \n",
      "    #print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/56:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "for i in range(0,len(first_race_data)):\n",
      "    print(type(first_race_data[i]))\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    \n",
      "    print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/57:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    \n",
      "    print(first_race_data[i][-1])\n",
      "    # df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    # df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "# print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/58:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    \n",
      "    df[\"Debut Year\"] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/59:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.split()\n",
      "last_race_data = df[\"Last Race\"].str.split()\n",
      "\n",
      "for i in range(0,len(first_race_data)):\n",
      "    \n",
      "    df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "    df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/60:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1)\n",
      "print(first_race_data)\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/61:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1)\n",
      "print(type(first_race_data))\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/62:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1)\n",
      "df[\"Debut Year\"] = first_race_data.str.split[1]\n",
      "df[\"Debut Circuit\"] = first_race_data.str.split[0]\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/63:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1)\n",
      "df[\"Debut Year\"] = first_race_data.str.split()[1]\n",
      "df[\"Debut Circuit\"] = first_race_data.str.split()[0]\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/64:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1)\n",
      "df[\"Debut Year\"] = first_race_data.str.split()\n",
      "df[\"Debut Circuit\"] = first_race_data.str.split()\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/65:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1)\n",
      "df[\"Debut Year\"] = first_race_data.str.split()\n",
      "#df[\"Debut Circuit\"] = first_race_data.str.split()\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/66:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "new = first_race_data.str.split(\",\", n=1, expand = True)\n",
      "\n",
      "df[\"Debut Year\"] = new[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data.str.split()\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/67:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "print(first_race_data)\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "\n",
      "#df[\"Debut Year\"] = new[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data.str.split()\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/68:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "print(first_race_data[0])\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "\n",
      "#df[\"Debut Year\"] = new[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data.str.split()\n",
      "\n",
      "# for i in range(0,len(first_race_data)):\n",
      "    \n",
      "#     df[\"Debut Year\"][i] = first_race_data[i][-1]\n",
      "#     df[\"Debut circuit\"][i] = ''.join(first_race_data[i][0:-1])\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/69:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data.str.split()\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "44/70:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/1:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/2:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/3:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "print(first_race_data[0])\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/4:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "print(first_race_data[0])\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/5:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "print(first_race_data[0])\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "#df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/6:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "print(first_race_data[0])\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/7:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "print(first_race_data[0])\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/8:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "#print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/9:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/10:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/11:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "#print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "print(df.head(5))\n",
      "45/12:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/13:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "#new = first_race_data.split(\",\", n=1, expand = True)\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "print(df[\"Debut Year\"])\n",
      "print(df[\"Debut Circuit\"])\n",
      " \n",
      "# df[\"Final Year\"] = last_race_data[-1]\n",
      "# df[\"Final circuit\"] = last_race_data[0:-1]\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/14:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "print(df[\"Final Year\"])\n",
      "print(df[\"Final Circuit\"])\n",
      "\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/15:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final circuit\"] = last_race_data[0]\n",
      "\n",
      "print(df[\"Final Year\"])\n",
      "print(df[\"Final Circuit\"])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/16:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "print(df[\"Final Year\"])\n",
      "print(df[\"Final Circuit\"])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "#df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "#df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/17:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/18:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best race finishing position] = df[\"Best race finishing position\"].str.split()[0]\n",
      "df[\"Best race starting position] = df[\"Best race starting position\"].str.split()[0]\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/19:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best race finishing position\"] = df[\"Best race finishing position\"].str.split()[0]\n",
      "df[\"Best race starting position\"] = df[\"Best race starting position\"].str.split()[0]\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/20:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best race finishing position\"] = df[\"Best race finishing position\"].str.split()[0]\n",
      "df[\"Best race starting position\"] = df[\"Best race starting position\"].str.split()[0]\n",
      "print(df[\"Best race finishing position\"])\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/21:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best race finishing position\"] = int(df[\"Best race finishing position\"].str.split()[0])\n",
      "df[\"Best race starting position\"] = int(df[\"Best race starting position\"].str.split()[0])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/22:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best race finishing position\"] = df[\"Best race finishing position\"].str.split()[1]\n",
      "df[\"Best race starting position\"] = df[\"Best race starting position\"].str.split()[1]\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/23:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best race finishing position\"] = df[\"Best race finishing position\"].str.split()[1]\n",
      "df[\"Best race starting position\"] = df[\"Best race starting position\"].str.split()[1]\n",
      "print(df[\"Best race finishing position\"])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/24:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "# df[\"Best race finishing position\"] = df[\"Best race finishing position\"].str.split()[1]\n",
      "# df[\"Best race starting position\"] = df[\"Best race starting position\"].str.split()[1]\n",
      "print(df[\"Best race finishing position\"])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/25:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "print(df[\"Best race starting position\"])\n",
      "print(df[\"Best race finishing position\"])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/26:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "print(df[\"Best race starting position\"])\n",
      "print(df[\"Best race finishing position\"][16])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/27:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "print(df[\"Best race starting position\"][16])\n",
      "print(df[\"Best race finishing position\"][16])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/28:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "print(df[\"Best race starting position\"][3])\n",
      "print(df[\"Best race finishing position\"][3])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/29:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "print(df[\"Best championship position\"])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/30:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "print(df[\"Best championship position\"].str.split())\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/31:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "best_champseason_data = print(df[\"Best championship position\"].str.split())\n",
      "df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "print(best_champseason_data[0])\n",
      "print(best_champseason_data[-1])\n",
      "print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/32:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "best_champseason_data = print(df[\"Best championship position\"].str.split())\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "print(best_champseason_data[0])\n",
      "print(best_champseason_data[-1])\n",
      "print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/33:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_data = print(df[\"Best championship position\"].str.split())\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "print(best_champseason_data[0])\n",
      "print(best_champseason_data[-1])\n",
      "print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/34:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "print(best_champseason_data[0])\n",
      "print(best_champseason_data[-1])\n",
      "print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/35:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "print(best_champseason_data[0])\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/36:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "print([\"Best_championship_position\"])\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "\n",
      "print(best_champseason_data[0])\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/37:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "print(df[\"Best championship position\"])\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "\n",
      "print(best_champseason_data[0])\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/38:\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "# df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "# df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "# df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/39:\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "for i in range(0,len(best_champseason_data)):\n",
      "    if best_champseason_data[i] == nan:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[0]\n",
      "        df[\"Best championship finish year\"] = best_champseason_data[-1]\n",
      "        df[\"Amount of times in this position\"] = best_champseason_data[1:-1]\n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/40:\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_data = df[\"Best championship position\"].str.split()\n",
      "for i in range(0,len(best_champseason_data)):\n",
      "    if best_champseason_data[i] == nan:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "        df[\"Best championship finish year\"] = best_champseason_data[i][-1]\n",
      "        df[\"Amount of times in this position\"] = best_champseason_data[i][1:-1]\n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/41:\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_data = df[\"Best championship position\"].str.split(expand=True)\n",
      "for i in range(0,len(best_champseason_data)):\n",
      "    if best_champseason_data[i] == nan:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "        df[\"Best championship finish year\"] = best_champseason_data[i][-1]\n",
      "        df[\"Amount of times in this position\"] = best_champseason_data[i][1:-1]\n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/42:\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "for i in range(0,len(best_champseason_place)):\n",
      "    if best_champseason_data[i] == nan:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "    \n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/43:\n",
      "\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "for i in range(0,len(best_champseason_place)):\n",
      "    if best_champseason_data.isnull() == True:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "    \n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/44:\n",
      "\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "for i in range(0,len(best_champseason_place)):\n",
      "    if best_champseason_data[i].isnull() == True:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "    \n",
      "\n",
      "print(best_champseason_data)\n",
      "# print(best_champseason_data[-1])\n",
      "# print(best_champseason_data[1:-1])\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/45:\n",
      "\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "# for i in range(0,len(best_champseason_place)):\n",
      "#     if best_champseason_data[i].isnull() == True:\n",
      "#         best_champseason_data = \"Not placed\"\n",
      "#     else:\n",
      "#         df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/46:\n",
      "\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "\n",
      "for i in range(0,len(best_champseason_place)):\n",
      "    if best_champseason_data[i].isnull() == True:\n",
      "        best_champseason_data = \"Not placed\"\n",
      "    else:\n",
      "        df[\"Best championship finish position\"] = best_champseason_data[i][0]\n",
      "\n",
      "print(df[best_champseason_place])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/47:\n",
      "\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "\n",
      "print(df[\"Best championship position\"])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/48:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "for i in range(0,len(best_champseason_place)):\n",
      "    if best_champseason_place[i] = nan:\n",
      "        best_champseason_place[i] = \"Not placed\"\n",
      "\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "\n",
      "print(df[\"Best championship position\"])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/49:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "for i in range(0,len(best_champseason_place)):\n",
      "    if best_champseason_place[i] == nan:\n",
      "        best_champseason_place[i] = \"Not placed\"\n",
      "\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "\n",
      "print(df[\"Best championship position\"])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "45/50:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "\n",
      "print(df[\"Best championship position\"])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "#print(df.head(5))\n",
      "46/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#print(df[\"Best championship position\"])\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "\n",
      "print(df[\"Best championship position\"])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "print(df.head(5))\n",
      "47/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "\n",
      "print(df[\"Best championship position\"])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "\n",
      "print(df.head(5))\n",
      "47/2:\n",
      "cols = df.columns.to_list()\n",
      "print(cols)\n",
      "47/3:\n",
      "cols = df.columns.to_list()\n",
      "print(cols)\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "47/4:\n",
      "cols = df.columns.to_list()\n",
      "\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "print(cols)\n",
      "47/5:\n",
      "cols = df.columns.to_list()\n",
      "\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "print(cols)\n",
      "47/6:\n",
      "cols = df.columns.to_list()\n",
      "\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "print(cols)\n",
      "47/7:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "print(cols)\n",
      "types = df.dtypes()\n",
      "print(types)\n",
      "47/8:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/9:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df.columns.drop(df[\"First Race\"],df[\"Last Race\"])\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/10:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df.columns.drop(columns = [\"First Race\"],df[\"Last Race\"])\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/11:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df.columns.drop(columns = [\"First Race\"], [\"Last Race\"])\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/12:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df.columns.drop(columns = ([\"First Race\"], [\"Last Race\"]))\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/13:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(columns= ([\"First Race\"], [\"Last Race\"]))\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/14:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(columns= ([\"First Race\"],[\"Last Race\"])))\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/15:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(columns= ([\"First Race\"],[\"Last Race\"]))\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/16:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop['First Race', axis = 1]\n",
      "df = df.drop['Last Race', axis = 1]\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/17:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop[\"First Race\", axis= 1]\n",
      "df = df.drop[\"Last Race\", axis= 1]\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/18:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/19:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(5))\n",
      "47/20:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/21:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(5))\n",
      "47/22:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/23:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=1, expand=True)\n",
      "print(best_champseason_year)\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(5))\n",
      "47/24:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=1, expand=True)\n",
      "print(best_champseason_year[1])\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(5))\n",
      "47/25:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "print(best_champseason_year[2])\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(5))\n",
      "47/26:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "print(best_champseason_year[2])\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/27:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Year\"] = df[\"Debut Year\"].astype(int)\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Year\"] = df[\"Final Year\"].astype(int)\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "print(best_champseason_year[2])\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/28:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "print(best_champseason_year[2])\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/29:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/30:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/31:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/32:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/33:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:7] + cols[-4:]+ cols[7:-4]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/34:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/35:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/36:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/37:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+cols[-7]+cols[-5:]+cols[[8:-5]]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/38:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+cols[-7]+cols[-5:]+cols[8:-5]]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/39:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+cols[-7]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/40:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/41:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/42:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/43:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/44:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "47/45:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "print(df.head[5])\n",
      "47/46:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "print(df.head(5)\n",
      "47/47:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "print(df.head(5)\n",
      "47/48:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/49:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/50:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "print(df.head(5)\n",
      "47/51:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "print(df.head(5))\n",
      "47/52:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes())\n",
      "47/53:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/54:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/55:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/56:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/57:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[2]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# #print(df.head(5))\n",
      "# print(df.dtypes)\n",
      "47/58:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[2]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# #print(df.head(5))\n",
      "# print(df.dtypes)\n",
      "47/59:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/60:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/61:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "#print(df.dtypes)\n",
      "47/62:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "#df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "#print(df.dtypes)\n",
      "47/63:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "print(best_champseason_year)\n",
      "#df[\"Best championship year\"] = best_champseason_year[2]\n",
      "#df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "#df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "#print(df.head(5))\n",
      "#print(df.dtypes)\n",
      "47/64:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/65:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World Champion\", \"Best championship position\"] == 1\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "#print(df.head(5))\n",
      "47/66:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World Champion\", \"Best championship position\"] == 1\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "47/67:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] == 1\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "47/68:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] == \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "47/69:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "47/70:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/71:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/72:\n",
      "f = open('champs_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "47/73:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df.head(5))\n",
      "print(df.dtypes)\n",
      "47/74:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df[\"Best championship position\"\"])\n",
      "print(df.dtypes)\n",
      "47/75:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df[\"Best championship position\"])\n",
      "print(df.dtypes)\n",
      "47/76:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df.loc[[58]])\n",
      "print(df.dtypes)\n",
      "47/77:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/78:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position (constructor\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/79:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position (constructor)\"] == \"World\", \"Best championship position (constructor)\"] = \"1st\"\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/80:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position (constructor)\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/81:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/82:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/83:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "# first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "# last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[1]\n",
      "# df[\"Debut Circuit\"] = first_race_data[0]\n",
      "# df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "# df[\"Final Year\"] = last_race_data[1]\n",
      "# df[\"Final Circuit\"] = last_race_data[0]\n",
      "# df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "# #Separate best position and year\n",
      "# df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "# best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# df[\"Best championship year\"] = best_champseason_place[1]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "\n",
      "print(df.loc[[57]])\n",
      "print(df.dtypes)\n",
      "47/84:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "# first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "# last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[1]\n",
      "# df[\"Debut Circuit\"] = first_race_data[0]\n",
      "# df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "# df[\"Final Year\"] = last_race_data[1]\n",
      "# df[\"Final Circuit\"] = last_race_data[0]\n",
      "# df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "# #Separate best position and year\n",
      "# df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "# best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# df[\"Best championship year\"] = best_champseason_place[1]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champdriverseason_place[57])\n",
      "\n",
      "#print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/85:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "# first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "# last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[1]\n",
      "# df[\"Debut Circuit\"] = first_race_data[0]\n",
      "# df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "# df[\"Final Year\"] = last_race_data[1]\n",
      "# df[\"Final Circuit\"] = last_race_data[0]\n",
      "# df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "# #Separate best position and year\n",
      "# df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "# best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# df[\"Best championship year\"] = best_champseason_place[1]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "\n",
      "#print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/86:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "# first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "# last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[1]\n",
      "# df[\"Debut Circuit\"] = first_race_data[0]\n",
      "# df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "# df[\"Final Year\"] = last_race_data[1]\n",
      "# df[\"Final Circuit\"] = last_race_data[0]\n",
      "# df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "# #Separate best position and year\n",
      "# df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "# best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# df[\"Best championship year\"] = best_champseason_place[1]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=1, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "\n",
      "#print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/87:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "# first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "# last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[1]\n",
      "# df[\"Debut Circuit\"] = first_race_data[0]\n",
      "# df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "# df[\"Final Year\"] = last_race_data[1]\n",
      "# df[\"Final Circuit\"] = last_race_data[0]\n",
      "# df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "# #Separate best position and year\n",
      "# df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "# best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# df[\"Best championship year\"] = best_champseason_place[1]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "\n",
      "#print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/88:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "# first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "# last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "# df[\"Debut Year\"] = first_race_data[1]\n",
      "# df[\"Debut Circuit\"] = first_race_data[0]\n",
      "# df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "# df[\"Final Year\"] = last_race_data[1]\n",
      "# df[\"Final Circuit\"] = last_race_data[0]\n",
      "# df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "# #Separate best position and year\n",
      "# df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "# best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "# df[\"Best championship position\"] = best_champseason_place[0]\n",
      "# df[\"Best championship year\"] = best_champseason_place[1]\n",
      "# df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "# df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "# df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "# best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "# df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "\n",
      "print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/89:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "\n",
      "print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/90:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position(constructor)\")\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/91:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\")\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/92:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "47/93:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/94:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "\n",
      "print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/95:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\")\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/96:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "47/97:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "\n",
      "print(df.loc[[57]])\n",
      "#print(df.dtypes)\n",
      "47/98:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "48/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Year active\"] = df[\"Year active\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "48/2:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year Active\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "48/3:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "48/4:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year Active\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "48/5:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "48/6:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "print(cols)\n",
      "types = df.dtypes\n",
      "print(types)\n",
      "51/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "\n",
      "\n",
      "print(df.head(25))\n",
      "51/2:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "51/3:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "51/4: clean_driver_data.to_sql('Drivers', engine, if_exists='append')\n",
      "51/5:\n",
      "idx = clean_driver_data[clean_driver_data['Driver Second Name'] == 'Senna'].index.values[0]\n",
      "clean_driver_data.iloc[idx:].to_sql('Drivers', engine, if_exists='append'))\n",
      "51/6:\n",
      "idx = clean_driver_data[clean_driver_data['Driver Second Name'] == 'Senna'].index.values[0]\n",
      "clean_driver_data.iloc[idx:].to_sql('Drivers', engine, if_exists='append')\n",
      "51/7:\n",
      "idx = clean_driver_data[clean_driver_data['Driver Second Name'] == 'Senna'].index.values[0]\n",
      "clean_driver_data.iloc[idx:].to_sql('Drivers', engine, if_exists='append', index=False)\n",
      "51/8:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "51/9:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\", axis=1)\n",
      "clean_team_data = df\n",
      "clean_team_data.to_sql('Teams', engine, if_exists='append')\n",
      "51/10:\n",
      "f = open('champs_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "df.to_sql('Champions', engine, if_exists='append')\n",
      "51/11:\n",
      "f = open('circuits_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "df.to_sql('Circuits', engine, if_exists='append')\n",
      "51/12:\n",
      "f = open('circuit_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "df.to_sql('Circuits', engine, if_exists='append')\n",
      "51/13:\n",
      "new_clean_driver_data = clean_driver_data.copy()\n",
      "old_clean_driver_data = pd.read_sql_table('''SELECT * from \"Drivers\"''', con =engine)\n",
      "51/14:\n",
      "new_clean_driver_data = clean_driver_data.copy()\n",
      "old_clean_driver_data = pd.read_sql_query'''SELECT * from \"Drivers\"''', con =engine)\n",
      "51/15:\n",
      "new_clean_driver_data = clean_driver_data.copy()\n",
      "old_clean_driver_data = pd.read_sql_query('''SELECT * from \"Drivers\"''', con =engine)\n",
      "51/16: pd.concat([old_clean_driver_data, new_clean_driver_data])\n",
      "51/17: pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicate()\n",
      "51/18: pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicated()\n",
      "51/19: ~pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicated()\n",
      "51/20: merged_dfs = pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicated()\n",
      "51/21: merged_dfs[~merged_dfs.duplicated()]\n",
      "51/22: merged_dfs[~merged_dfs.duplicated(keep=False)]\n",
      "51/23: merged_dfs = pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicated()\n",
      "51/24:\n",
      "new_clean_driver_data = clean_driver_data.copy()\n",
      "old_clean_driver_data = pd.read_sql_query('''SELECT * from \"Drivers\"''', con =engine)\n",
      "51/25: merged_dfs = pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicated()\n",
      "51/26: merged_dfs[~merged_dfs.duplicated(keep=False)]\n",
      "51/27:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "51/28:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "51/29: clean_driver_data.to_sql('Drivers', engine, if_exists='append')\n",
      "51/30:\n",
      "idx = clean_driver_data[clean_driver_data['Driver Second Name'] == 'Senna'].index.values[0]\n",
      "clean_driver_data.iloc[idx:].to_sql('Drivers', engine, if_exists='append', index=False)\n",
      "51/31:\n",
      "new_clean_driver_data = clean_driver_data.copy()\n",
      "old_clean_driver_data = pd.read_sql_query('''SELECT * from \"Drivers\"''', con =engine)\n",
      "51/32: merged_dfs = pd.concat([old_clean_driver_data, new_clean_driver_data]).duplicated()\n",
      "51/33:\n",
      "new_values = merged_dfs[~merged_dfs.duplicated(keep=False)]\n",
      "new_values.to_sql[]\n",
      "51/34:\n",
      "new_values = merged_dfs[~merged_dfs.duplicated(keep=False)]\n",
      "new_values.to_sql()\n",
      "51/35:\n",
      "new_values = merged_dfs[~merged_dfs.duplicated(keep=False)]\n",
      "new_values.to_sql('Drivers', engine, if_exists='append', index=False)\n",
      "52/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "53/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "53/2:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "53/3:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "53/4:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "53/5:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "54/1: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "54/2:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "54/3:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "54/4: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "54/5:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "engine.execute()\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "54/6:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "54/7: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "54/8: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "56/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "56/2:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "56/3:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "56/4: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "56/5: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "56/6:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "56/7:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "56/8:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "56/9: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/1: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/2:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/3:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "57/4:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "57/5: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/6: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/7:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/8:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/9:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "57/10:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/11:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "57/12:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "57/13: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/14:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/15:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "57/16:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/17:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/18:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "57/19:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "57/20: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/21:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/22:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/23:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/24:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/25:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/26:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/27:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/28:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "57/29:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "57/30:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "57/31: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "57/32:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/33:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/34:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/35:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/36:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "#df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/37:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "#df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "#df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "57/38:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "58/1:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\", axis=1)\n",
      "clean_team_data = df\n",
      "clean_team_data.to_sql('Teams', engine, if_exists='append')\n",
      "58/2:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "58/3:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "58/4:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "58/5:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "58/6:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "58/7:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\", axis=1)\n",
      "clean_team_data = df\n",
      "clean_team_data.to_sql('Teams', engine, if_exists='append')\n",
      "58/8:\n",
      "f = open('teams_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "print(df[\"First Race\"])\n",
      "\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position (constructor)\"] = df[\"Best championship position (constructor)\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position (constructor)\"].str.split(n=1, expand=True)\n",
      "print(best_champseason_place)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "df[\"Best championship year\"] = best_champseason_place[1]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[1]\n",
      "\n",
      "df[\"Best championship position (driver)\"] = df[\"Best championship position (driver)\"].astype(str)\n",
      "best_champdriverseason_place = df[\"Best championship position (driver)\"].str.split(n=2, expand=True)\n",
      "print(best_champdriverseason_place)\n",
      "df[\"Best championship position (driver)\"] = best_champdriverseason_place[0]\n",
      "df[\"Best championship year (driver)\"] = best_champdriverseason_place[1]\n",
      "df[\"Best championship driver\"] = best_champdriverseason_place[2]\n",
      "58/9:\n",
      "cols = df.columns.to_list()\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Best championship position (constructor)\", axis=1)\n",
      "clean_team_data = df\n",
      "clean_team_data.to_sql('Teams', engine, if_exists='append')\n",
      "62/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "62/2:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "62/3:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "62/4: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "62/5:\n",
      "pkey = engine.execute('SELECT EXISTS(SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'Drivers');')\n",
      "print(pkey)\n",
      "62/6:\n",
      "pkey = engine.execute('SELECT EXISTS(SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \"Drivers\");')\n",
      "print(pkey)\n",
      "62/7:\n",
      "\n",
      "pkey = engine.execute('SELECT EXISTS(SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'Drivers\\');')\n",
      "print(pkey)\n",
      "62/8:\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "pkey = engine.execute('SELECT EXISTS(SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'Drivers\\');')\n",
      "print(pkey)\n",
      "62/9:\n",
      "pkey = engine.execute('SELECT EXISTS(SELECT indexname, indexdef FROM pg_indexes WHERE indexname = \\'Drivers_pkey\\');')\n",
      "print(pkey)\n",
      "62/10:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE indexname = \\'Drivers_pkey\\';')\n",
      "print(pkey)\n",
      "62/11:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';')\n",
      "print(pkey)\n",
      "62/12:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_indexes()\n",
      "62/13:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_indexes(\"Drivers\")\n",
      "62/14:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Drivers\")\n",
      "62/15:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Drivers\")\n",
      "62/16:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "62/17:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")[0]\n",
      "62/18:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\"[0])\n",
      "62/19:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';').__str__\n",
      "print(pkey)\n",
      "62/20:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';').cursor\n",
      "print(pkey)\n",
      "62/21:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';').keys\n",
      "print(pkey)\n",
      "62/22:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "62/23:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';').\n",
      "print(pkey)\n",
      "62/24:\n",
      "pkey = engine.execute('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';')\n",
      "print(pkey)\n",
      "62/25:\n",
      "pkey = engine.execute(text('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';'))\n",
      "print(pkey)\n",
      "62/26:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "62/27:\n",
      "pkey = engine.execute(text('SELECT indexname, indexdef FROM pg_indexes WHERE tablename = \\'\"Drivers\"\\';'))\n",
      "print(pkey)\n",
      "64/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "64/2:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "64/3:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "clean_driver_data = df\n",
      "print(type(clean_driver_data))\n",
      "64/4:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "64/5: clean_driver_data.to_sql('Drivers', engine, if_exists='replace')\n",
      "66/1:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "inspector.\n",
      "66/2:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "66/3:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")[0]\n",
      "66/4:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "inspector.get_pk_constraint(\"Teams\")\n",
      "66/5:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "pk = inspector.get_pk_constraint(\"Teams\")\n",
      "if pk != None:\n",
      "    print(\"whoop\")\n",
      "else\n",
      "    print(\"awwww\")\n",
      "66/6:\n",
      "import psycopg2\n",
      "from sqlalchemy import create_engine\n",
      "import pandas as pd\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import exists\n",
      "from sqlalchemy import text\n",
      "import sqlalchemy\n",
      "\n",
      "DATABASE_TYPE = 'postgresql'\n",
      "DBAPI = 'psycopg2'\n",
      "HOST = 'formula1.c0ptp1rfwhvx.eu-west-2.rds.amazonaws.com'\n",
      "USER = 'postgres'\n",
      "PASSWORD = 'T00narmyf1s'\n",
      "DATABASE = 'Formula1'\n",
      "PORT = 5432\n",
      "\n",
      "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
      "engine.connect()\n",
      "\n",
      "inspector = inspect(engine)\n",
      "inspector.get_table_names()\n",
      "pk = inspector.get_pk_constraint(\"Teams\")\n",
      "if pk != None:\n",
      "    print(\"whoop\")\n",
      "else:\n",
      "    print(\"awwww\")\n",
      "69/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/2:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/3:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns.str)\n",
      "clean_driver_data = df\n",
      "69/4:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns\n",
      "clean_driver_data = df\n",
      "69/5:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns)\n",
      "clean_driver_data = df\n",
      "69/6:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/7:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns)\n",
      "clean_driver_data = df\n",
      "69/8:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/9:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns)\n",
      "clean_driver_data = df\n",
      "69/10: print(df.dtypes)\n",
      "69/11:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/12:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns)\n",
      "clean_driver_data = df\n",
      "69/13: print(df.dtypes)\n",
      "69/14:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/15:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.columns)\n",
      "print(df[14])\n",
      "clean_driver_data = df\n",
      "69/16:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df[14])\n",
      "clean_driver_data = df\n",
      "69/17:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/18:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df[14])\n",
      "clean_driver_data = df\n",
      "69/19:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df[2])\n",
      "clean_driver_data = df\n",
      "69/20:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/21:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df[2])\n",
      "clean_driver_data = df\n",
      "69/22:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/23:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df[2,:])\n",
      "clean_driver_data = df\n",
      "69/24:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/25:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.loc[2,:])\n",
      "clean_driver_data = df\n",
      "69/26:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/27:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.loc[14,:])\n",
      "clean_driver_data = df\n",
      "69/28:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/29:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.loc[14,:])\n",
      "clean_driver_data = df\n",
      "69/30:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/31:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.loc[2,:])\n",
      "clean_driver_data = df\n",
      "69/32:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/33:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.loc[14,:])\n",
      "clean_driver_data = df\n",
      "69/34:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/35:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "print(df.loc[13,:])\n",
      "clean_driver_data = df\n",
      "69/36:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/37:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/38:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "df[\"Best race starting position\"] = df[\"Best race starting position\"].astype('Int64')\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/39:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/40:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12])\n",
      "clean_driver_data = df\n",
      "69/41:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/42:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12])\n",
      "clean_driver_data = df\n",
      "69/43:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12])\n",
      "clean_driver_data = df\n",
      "69/44:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,1])\n",
      "clean_driver_data = df\n",
      "69/45:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/46:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,1])\n",
      "clean_driver_data = df\n",
      "69/47:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/48:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,:])\n",
      "clean_driver_data = df\n",
      "69/49:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12:12])\n",
      "clean_driver_data = df\n",
      "69/50:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/51:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12:12])\n",
      "clean_driver_data = df\n",
      "69/52:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/53:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,11:12])\n",
      "clean_driver_data = df\n",
      "69/54:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/55:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12])\n",
      "clean_driver_data = df\n",
      "69/56:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/57:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/58:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12])\n",
      "clean_driver_data = df\n",
      "69/59:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/60:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,12])\n",
      "clean_driver_data = df\n",
      "69/61:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,:])\n",
      "clean_driver_data = df\n",
      "69/62:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/63:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.loc[13,:])\n",
      "clean_driver_data = df\n",
      "69/64: print(df.iloc[12,2])\n",
      "69/65: print(df.iloc[13,2])\n",
      "69/66: print(df.iloc[13,6])\n",
      "69/67: print(df.iloc[13,11])\n",
      "69/68: print(df.iloc[13,12])\n",
      "69/69:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "import numpy as np\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/70:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.iloc[12,2])\n",
      "clean_driver_data = df\n",
      "69/71:\n",
      "print(df.iloc[13,12])\n",
      "df[].replace('',np.nan)\n",
      "69/72:\n",
      "print(df.iloc[13,12])\n",
      "df.best_race_starting_position.replace('',np.nan)\n",
      "69/73:\n",
      "print(df.iloc[13,12])\n",
      "df = df.best_race_starting_position.replace('',np.nan)\n",
      "69/74:\n",
      "print(df.iloc[13,12])\n",
      "df = df.best_race_starting_position.replace('',np.nan)\n",
      "df = df.astype(int)\n",
      "69/75:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "import numpy as np\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/76:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.iloc[12,2])\n",
      "clean_driver_data = df\n",
      "69/77:\n",
      "\n",
      "df = df.best_race_starting_position.replace('',np.nan)\n",
      "df = df.astype(int)\n",
      "69/78:\n",
      "\n",
      "df = df..replace('-',np.nan)\n",
      "print(df[8])\n",
      "69/79:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "print(df[8])\n",
      "69/80:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "import numpy as np\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "69/81:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.iloc[12,2])\n",
      "clean_driver_data = df\n",
      "69/82:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "print(df[8])\n",
      "69/83:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "print(df.loc[8])\n",
      "69/84:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "print(df.loc[830])\n",
      "69/85:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "print(df.loc[486])\n",
      "69/86:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "print(df.loc[485])\n",
      "69/87:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "df = df.replace('', np.nan)\n",
      "print(df.loc[485])\n",
      "70/1:\n",
      "\n",
      "from cmath import nan\n",
      "import json\n",
      "import pandas as pd\n",
      "import formula1_scrape\n",
      "import numpy as np\n",
      "\n",
      "f = open('raw_data/driver_data.json')\n",
      "data = json.load(f)\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "#fix other data types\n",
      "df[\"First Race\"] = df[\"First Race\"].astype(str)\n",
      "#Consistency for Nationalities\n",
      "df[\"Nationality\"] = df[\"Nationality\"].str.title()\n",
      "\n",
      "#Separate circuits and years for first race and last race\n",
      "\n",
      "first_race_data = df[\"First Race\"].str.rsplit(n=1, expand=True)\n",
      "last_race_data = df[\"Last Race\"].str.rsplit(n=1, expand=True)\n",
      "\n",
      "df[\"Debut Year\"] = first_race_data[1]\n",
      "df[\"Debut Circuit\"] = first_race_data[0]\n",
      "df[\"Debut Circuit\"] = df[\"Debut Circuit\"].astype(str)\n",
      "df[\"Final Year\"] = last_race_data[1]\n",
      "df[\"Final Circuit\"] = last_race_data[0]\n",
      "df[\"Final Circuit\"] = df[\"Final Circuit\"].astype(str)\n",
      "\n",
      "\n",
      "#Separate best position and year\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "best_champseason_place = df[\"Best championship position\"].str.split(n=1, expand=True)\n",
      "df[\"Best championship position\"] = best_champseason_place[0]\n",
      "best_champseason_year = best_champseason_place[1].str.split(n=2, expand=True)\n",
      "df[\"Best championship year\"] = best_champseason_year[2]\n",
      "df[\"Best championship position\"] = df[\"Best championship position\"].astype(str)\n",
      "df[\"Best championship year\"] = df[\"Best championship year\"].astype(str)\n",
      "df.loc[df[\"Best championship position\"] == \"World\", \"Best championship position\"] = \"1st\"\n",
      "\n",
      "    \n",
      "#Convert dates/times\n",
      "df[\"Date of birth\"] = pd.to_datetime(df[\"Date of birth\"], infer_datetime_format=True, errors = 'coerce')\n",
      "df[\"Date of death\"] = pd.to_datetime(df[\"Date of death\"], infer_datetime_format=True, errors = 'coerce')\n",
      "\n",
      "#fix other data types\n",
      "df[\"Driver First Name\"] = df[\"Driver First Name\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Nationality\"] = df[\"Nationality\"].astype(str)\n",
      "df[\"Hometown\"] = df[\"Hometown\"].astype(str)\n",
      "df[\"Driver Second Name\"] = df[\"Driver Second Name\"].astype(str)\n",
      "df[\"Points\"] = df[\"Points\"].astype(float)\n",
      "70/2:\n",
      "cols = df.columns.to_list()\n",
      "cols = cols[0:8]+[cols[-7]]+cols[-5:]+cols[8:-5]\n",
      "df = df.drop(\"First Race\", axis=1)\n",
      "df = df.drop(\"Last Race\", axis=1)\n",
      "df = df.drop(\"Year active\", axis=1)\n",
      "df.columns = df.columns.str.lower()\n",
      "df.columns = df.columns.str.replace(' ','_')\n",
      "\n",
      "\n",
      "print(df.iloc[12,2])\n",
      "clean_driver_data = df\n",
      "70/3:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "df = df.replace('', np.nan)\n",
      "print(df.loc[485])\n",
      "70/4:\n",
      "\n",
      "df = df.replace('-',np.nan)\n",
      "df = df.replace('', np.nan)\n",
      "np.where(df.applymap(lambda x: x == ''))\n",
      "72/1:\n",
      "import requests\n",
      "\n",
      "resporse = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "72/2:\n",
      "import requests\n",
      "\n",
      "resporse = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response)\n",
      "72/3:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response)\n",
      "72/4:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response.text)\n",
      "72/5:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response.json)\n",
      "72/6:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response.text.json)\n",
      "72/7:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response.text)\n",
      "72/8:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response.json)\n",
      "73/1:\n",
      "import requests\n",
      "\n",
      "response = requests.get(\"https://api.stackexchange.com/docs/posts#page=3&pagesize=100&fromdate=2022-10-01&todate=2022-10-31&order=desc&sort=activity&filter=default&site=stackoverflow\")\n",
      "print(response.json)\n",
      "75/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "76/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "76/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "76/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "76/4:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "77/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "78/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "78/2:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "78/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "78/4:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "78/5:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "78/6:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "78/7:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "78/8:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "78/9:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/1: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "79/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "79/3:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "79/4: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "79/5:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/6: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "79/7:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/8:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "79/9:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "79/10: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "79/11:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/12:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/13:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/14: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "79/15:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "79/16: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "79/17: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*.parquet\")\n",
      "79/18: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "79/19: df.display()\n",
      "79/20: df.show()\n",
      "79/21: df.show(20,False)\n",
      "80/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "80/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "80/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "80/4:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "80/5:\n",
      "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
      "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
      "# Use collect() to collect all elements from an RDD\n",
      "print(rddDistributedData.collect())\n",
      "print(rddDistributedFile.collect())\n",
      "80/6: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "80/7: df.show(20,False)\n",
      "80/8: df.describe.show()\n",
      "80/9: df.describe()).show()\n",
      "80/10: df.describe().show()\n",
      "80/11: df.describe().show()\n",
      "82/1: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "82/2: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "82/3:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "82/4:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "82/5: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "82/6: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "82/7: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "82/8: df.describe().show()\n",
      "82/9: df.show()\n",
      "82/10: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "82/11: df.show()\n",
      "82/12: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "82/13: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sellers_parquet/*\")\n",
      "82/14: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "82/15: df.show()\n",
      "82/16: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "82/17: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "82/18: df.show()\n",
      "82/19: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sellers_parquet/*\")\n",
      "82/20: df.show()\n",
      "82/21: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "82/22:\n",
      "df.groupBy('order_id').count()\n",
      "df.show()\n",
      "82/23:\n",
      "df.groupBy('product_id').count()\n",
      "df.show()\n",
      "82/24: df.groupBy('product_id').count().show()\n",
      "82/25: df.groupBy('order_id').count().show()\n",
      "82/26: df.groupBy('order_id').sum().show()\n",
      "82/27: df.groupBy('product_id').sum().show()\n",
      "82/28: df.show()\n",
      "82/29: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "82/30:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.printSchema()\n",
      "82/31:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "82/32: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "82/33: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "82/34:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "82/35:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "82/36:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "82/37: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "82/38:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "83/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "83/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "83/3:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "83/4: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "83/5: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "83/6: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "83/7: df.dtypes()\n",
      "83/8: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "83/9: df.dtypes()\n",
      "83/10: df.dtypes\n",
      "83/11:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "83/12: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "83/13: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "83/14:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "83/15:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "84/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "84/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "84/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "84/4: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "84/5: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "84/6: df.dtypes\n",
      "84/7: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "84/8: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "84/9: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/\")\n",
      "84/10: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "84/11:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "84/12: df.show()\n",
      "85/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "85/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "85/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "85/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/*\")\n",
      "85/5: df.dtypes\n",
      "85/6: df.show()\n",
      "85/7: df.groupBy('product_id').count().show()\n",
      "85/8: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/product_parquet/*\")\n",
      "85/9: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "85/10: df.dtypes\n",
      "85/11: df.groupBy('product_id').count().show()\n",
      "85/12: df.show()\n",
      "85/13:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "85/14:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "86/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "86/2:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "86/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "86/4:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "86/5: df.show()\n",
      "86/6:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "86/7:\n",
      "import findspark\n",
      "from pyspark import SparkConf\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "86/8:\n",
      "import findspark\n",
      "from pyspark import SparkConf\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "86/9:\n",
      "import findspark\n",
      "from pyspark import SparkConf\n",
      "\n",
      "findspark.init()\n",
      "findspark.find()\n",
      "\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = SparkConf() \\\n",
      "    .setMaster(f\"local[*]\")\\\n",
      "    .setAppName(\"TestApp\")\\\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "86/10:\n",
      "import findspark\n",
      "\n",
      "\n",
      "findspark.init()\n",
      "\n",
      "findspark.find()\n",
      "from pyspark import SparkConf\n",
      "\n",
      "conf = SparkConf()\\\n",
      "    .setAppName(\"Test\")\\\n",
      "    .setMaster(\"local[*]\")\n",
      "86/11:\n",
      "import findspark\n",
      "import os\n",
      "\n",
      "findspark.init(os.environ[\"SPARK_HOME\"])\n",
      "\n",
      "from pyspark import SparkConf\n",
      "\n",
      "conf = SparkConf()\\\n",
      "    .setAppName(\"Test\")\\\n",
      "    .setMaster(\"local[*]\")\n",
      "86/12:\n",
      "import findspark\n",
      "import os\n",
      "\n",
      "findspark.init(os.environ[\"SPARK_HOME\"])\n",
      "\n",
      "from pyspark import SparkConf\n",
      "\n",
      "conf = SparkConf()\\\n",
      "    .setAppName(\"Test\")\\\n",
      "    .setMaster(\"local[*]\")\n",
      "86/13:\n",
      "import findspark\n",
      "import pyspark\n",
      "import os\n",
      "\n",
      "findspark.init(os.environ[\"SPARK_HOME\"])\n",
      "\n",
      "from pyspark import SparkConf\n",
      "\n",
      "conf = SparkConf()\\\n",
      "    .setAppName(\"Test\")\\\n",
      "    .setMaster(\"local[*]\")\n",
      "86/14:\n",
      "import findspark\n",
      "findspark.init()\n",
      "87/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "87/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "87/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "87/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "87/5: df.dtypes\n",
      "87/6: df.show()\n",
      "87/7:\n",
      "df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df.describe()\n",
      "87/8:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "87/9: df.dtypes\n",
      "88/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "88/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "88/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "88/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "88/5: df.dtypes\n",
      "88/6: df.show()\n",
      "88/7:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "88/8:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "88/9:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "88/10: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "89/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "89/2: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "89/3: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "89/4: df.show()\n",
      "89/5:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "89/6:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "90/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "90/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "90/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "90/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "90/5:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "91/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"2g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "91/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "91/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "91/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "91/5: df.dtypes\n",
      "91/6:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "92/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "92/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "92/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "92/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "92/5:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "93/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "93/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "93/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "93/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "93/5: df.show()\n",
      "93/6:\n",
      "df2 = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/sales_parquet/*\")\n",
      "df2.describe()\n",
      "93/7: df.show()\n",
      "94/1:\n",
      "import multiprocessing\n",
      "\n",
      "import pyspark\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "94/2:\n",
      "import findspark\n",
      "findspark.init()\n",
      "94/3: session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()\n",
      "94/4: df = session.read.parquet(\"./DatasetToCompleteTheSixSparkExercises/products_parquet/*\")\n",
      "96/1:\n",
      "from os.path import expanduser\n",
      "import os\n",
      "\n",
      "home = expanduser(\"~\")\n",
      "airflow_dir = os.path.join(home, 'airflow')\n",
      "assert os.path.isdir(airflow_dir)\n",
      "97/1:\n",
      "accessKeyId=os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
      "print(accessKeyId)\n",
      "97/2:\n",
      "accessKeyId=os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
      "print(accessKeyId)\n",
      "97/3:\n",
      "accessKeyId=os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
      "print(accessKeyId)\n",
      "97/4:\n",
      "accessKeyId=os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
      "print(accessKeyId)\n",
      "97/5:\n",
      "accessKeyId=os.environ[\"aws_access_key_id\"]\n",
      "print(accessKeyId)\n",
      "97/6:\n",
      "import boto3\n",
      "\n",
      "session = boto3.Session(profile_name=\"Dave\")\n",
      "credentials = session.get_credentials()\n",
      "print(\"AWS_ACCESS_KEY_ID = {}\".format(credentials.access_key))\n",
      "print(\"AWS_SECRET_ACCESS_KEY = {}\".format(credentials.secret_key))\n",
      "print(\"AWS_SESSION_TOKEN = {}\".format(credentials.token))\n",
      "97/7:\n",
      "import boto3\n",
      "\n",
      "session = boto3.Session(profile_name='dev')\n",
      "credentials = session.get_credentials()\n",
      "print(\"AWS_ACCESS_KEY_ID = {}\".format(credentials.access_key))\n",
      "print(\"AWS_SECRET_ACCESS_KEY = {}\".format(credentials.secret_key))\n",
      "print(\"AWS_SESSION_TOKEN = {}\".format(credentials.token))\n",
      "97/8:\n",
      "import boto3\n",
      "\n",
      "a = boto3.session.Session().available_profiles\n",
      "print(a)\n",
      "session = boto3.Session(profile_name='dev')\n",
      "credentials = session.get_credentials()\n",
      "print(\"AWS_ACCESS_KEY_ID = {}\".format(credentials.access_key))\n",
      "print(\"AWS_SECRET_ACCESS_KEY = {}\".format(credentials.secret_key))\n",
      "print(\"AWS_SESSION_TOKEN = {}\".format(credentials.token))\n",
      "97/9:\n",
      "import boto3\n",
      "\n",
      "a = boto3.session.Session().available_profiles\n",
      "print(a)\n",
      "session = boto3.Session(profile_name='default')\n",
      "credentials = session.get_credentials()\n",
      "print(\"AWS_ACCESS_KEY_ID = {}\".format(credentials.access_key))\n",
      "print(\"AWS_SECRET_ACCESS_KEY = {}\".format(credentials.secret_key))\n",
      "print(\"AWS_SESSION_TOKEN = {}\".format(credentials.token))\n",
      "97/10:\n",
      "import boto3\n",
      "\n",
      "session = boto3.Session(profile_name='default')\n",
      "credentials = session.get_credentials()\n",
      "print(\"AWS_ACCESS_KEY_ID = {}\".format(credentials.access_key))\n",
      "print(\"AWS_SECRET_ACCESS_KEY = {}\".format(credentials.secret_key))\n",
      "print(\"AWS_SESSION_TOKEN = {}\".format(credentials.token))\n",
      "98/1:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "98/2:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "\n",
      "\n",
      "# We should always start with session in order to obtain\n",
      "# context and session if needed\n",
      "session = pyspark.sql.SparkSession.builder.config(\n",
      "    conf=pyspark.SparkConf()\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    .setAppName(\"TestApp\")\n",
      ").getOrCreate()\n",
      "98/3:\n",
      "from pyspark.streaming import StreamingContext\n",
      "\n",
      "ssc = StreamingContext(session.sparkContext, batchDuration=30)\n",
      "98/4:\n",
      "# We will send lines of data to this socketTextStream\n",
      "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
      "98/5: unique_words = lines.flatMap(lambda text: text.split()).countByValue()\n",
      "98/6: unique_words.pprint()\n",
      "98/7: ssc.start()\n",
      "98/8:\n",
      "seconds = 180\n",
      "ssc.awaitTermination(seconds)\n",
      "98/9: ssc.start()\n",
      "98/10:\n",
      "seconds = 180\n",
      "ssc.awaitTermination(seconds)\n",
      "98/11:\n",
      "import findspark\n",
      "\n",
      "findspark.init()\n",
      "98/12:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "\n",
      "\n",
      "# We should always start with session in order to obtain\n",
      "# context and session if needed\n",
      "session = pyspark.sql.SparkSession.builder.config(\n",
      "    conf=pyspark.SparkConf()\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    .setAppName(\"TestApp\")\n",
      ").getOrCreate()\n",
      "98/13:\n",
      "from pyspark.streaming import StreamingContext\n",
      "\n",
      "ssc = StreamingContext(session.sparkContext, batchDuration=30)\n",
      "98/14:\n",
      "# We will send lines of data to this socketTextStream\n",
      "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
      "98/15: unique_words = lines.flatMap(lambda text: text.split()).countByValue()\n",
      "98/16: unique_words.pprint()\n",
      "98/17: ssc.start()\n",
      "100/1:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        spark()\n",
      "100/2:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df2 = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        spark()\n",
      "100/3:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df2 = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        spark()\n",
      "101/1:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df2 = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        spark()\n",
      "101/2: print(df.show())\n",
      "101/3: print(df2.show())\n",
      "102/1:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df2 = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "    return df2\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        spark()\n",
      "103/1:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df2 = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "    return df2\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        df2 = spark()\n",
      "103/2: print(df2)\n",
      "103/3: print(df2.show())\n",
      "103/4: print(df)\n",
      "103/5: print(df)\n",
      "103/6: print(df2)\n",
      "104/1:\n",
      "import multiprocessing\n",
      "import pyspark\n",
      "import json\n",
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SparkSession\n",
      "import pyspark.sql.functions as f\n",
      "import boto3\n",
      "import os\n",
      "from airflow.models import DAG\n",
      "from datetime import datetime\n",
      "from datetime import timedelta\n",
      "from airflow.operators.bash_operator import BashOperator\n",
      "from airflow.models import Variable\n",
      "\n",
      "cfg = (\n",
      "    pyspark.SparkConf()\n",
      "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
      "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
      "    # Setting application name\n",
      "    .setAppName(\"TestApp\")\n",
      "    # Setting config value via string\n",
      "    .set(\"spark.eventLog.enabled\", False)\n",
      "    # Setting environment variables for executors to use\n",
      "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
      "    # Setting memory if this setting was not set previously\n",
      "    .setIfMissing(\"spark.executor.memory\", \"6g\")\n",
      ")\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'balany1',\n",
      "    'depends_on_past': False,\n",
      "    'email': ['andrewmcnamara@live.co.uk'],\n",
      "    'email_on_failure': False,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 1,\n",
      "    'start_date': datetime(2023, 2, 8), # If you set a datetime previous to the curernt date, it will try to backfill\n",
      "    'retry_delay': timedelta(minutes=5),\n",
      "    'end_date': datetime(2024, 1, 1),\n",
      "}\n",
      "\n",
      "\n",
      "def spark():\n",
      "    #configure and set credentials\n",
      "    s3_client = boto3.client('s3')\n",
      "    session = boto3.Session(profile_name='default')\n",
      "    credentials = session.get_credentials()\n",
      "    #s3 = boto3.resource('s3')#\n",
      "    #my_bucket = s3_client.bucket('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1')\n",
      "    accessKeyId=credentials.access_key\n",
      "    secretAccessKey=credentials.secret_key\n",
      "    cfg.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
      "    cfg.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
      "    cfg.set('spark.hadoop.fs.s3a.access.key', accessKeyId)\n",
      "    cfg.set('spark.hadoop.fs.s3a.secret.key', secretAccessKey)\n",
      "\n",
      "    #set Spark Context\n",
      "    sc = SparkContext(conf=cfg)\n",
      "\n",
      "    #start Spark session\n",
      "    spark = SparkSession(sc).builder.appName(\"TestApp\").getOrCreate()\n",
      "\n",
      "    #read json files from s3 bucket\n",
      "    df = spark.read.json(\"s3a://pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1/*.json\")\n",
      "    print(df)\n",
      "\n",
      "    #clean data\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"User Info Error\", \"0\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
      "    df = df.withColumn('follower_count', f.regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
      "    df = df.withColumn('follower_count', f.col(\"follower_count\").cast(\"Int\"))\n",
      "    df = df.withColumn('tag_list', f.regexp_replace(\"tag_list\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"None\"))\n",
      "\n",
      "    #narrow down fields necessary\n",
      "    df2 = df.select(\"category\",\"description\",\"follower_count\", \"tag_list\", \"title\",\"unique_id\").show()\n",
      "\n",
      "    #for i in range(0,10):\n",
      "        #s3_client.download_file('pinterest-data-decf2d83-23f1-4044-9aef-dda97e4934b1', )\n",
      "    return df\n",
      "\n",
      "\n",
      "\n",
      "# Getting a single variable\n",
      "print(cfg.get(\"spark.executor.memory\"))\n",
      "# Listing all of them in string readable format\n",
      "print(cfg.toDebugString())\n",
      "\n",
      "with DAG(dag_id='spark',\n",
      "         default_args=default_args,\n",
      "         schedule_interval='*/1 * * * *',\n",
      "         catchup=False,\n",
      "         tags=['test']\n",
      "         ) as dag:\n",
      "        df2 = spark()\n",
      "104/2: print(df2)\n",
      "104/3: print(df2.show())\n",
      "104/4: print(df2.groupBy('category').count().show())\n",
      "104/5: print(df2.groupBy('category').count()..sort(desc()).show())\n",
      "104/6: print(df2.groupBy('category').count().sort(desc()).show())\n",
      "104/7: print(df2.groupBy('category').count().sort().show())\n",
      "104/8: print(df2.groupBy('category').sort('category').show())\n",
      "104/9: print(df2.groupBy('category').count().sort('category').show())\n",
      "104/10: print(df2.groupBy('category').count().sort(desc('category')).show())\n",
      "104/11: print(df2.groupBy('category').count().show())\n",
      "104/12: print(df2.groupBy('category').count().sort(desc('count')).show())\n",
      "104/13: print(df2.groupBy('category').count().sort(('count').show())\n",
      "104/14: print(df2.groupBy('category').count().sort(('count').show()))\n",
      "104/15: print(df2.groupBy('category').count().sort('count').show())\n",
      "104/16: print(df2.groupBy('category').count().sort('count').desc().show())\n",
      "104/17: print(df2.groupBy('category').count().sort(col('count')).show())\n",
      "104/18:\n",
      "from pyspark.sql.functions import col\n",
      "print(df2.groupBy('category').count().sort(col('count')).show())\n",
      "104/19:\n",
      "from pyspark.sql.functions import col\n",
      "print(df2.groupBy('category').count().sort(col('count').desc()).show())\n",
      "104/20: print(df2.sort(col('follower_count').desc()).show())\n",
      "105/1:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/search?city=17193&checkIn=2023-03-03&los=14&rooms=1&adults=2&children=0&cid=1891458&locale=en-gb&ckuid=9cdf223f-a3ce-4890-965a-b2aab1ae50d6&prid=0&gclid=Cj0KCQjwqc6aBhC4ARIsAN06NmOZjaRodGcrUeKINOPnKgTZEfVb8z7hDSCrkddzvLoQZA07tdLlcnsaAg2qEALw_wcB&currency=GBP&correlationId=d880d555-ceda-41ba-b708-41c298e4db9b&pageTypeId=1&realLanguageId=16&languageId=1&origin=GB&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&userId=9cdf223f-a3ce-4890-965a-b2aab1ae50d6&whitelabelid=1&loginLvl=0&storefrontId=3&currencyId=2&currencyCode=GBP&htmlLanguage=en-gb&cultureInfoName=en-gb&machineName=am-pc-4f-acm-web-user-7d974b9749-vw7ck&trafficGroupId=5&sessionId=ug4znrtc3dsxzbtr2i4zpami&trafficSubGroupId=122&aid=82361&useFullPageLogin=true&cttp=4&isRealUser=true&mode=production&checkOut=2023-03-17&priceCur=GBP&textToSearch=Bali&productType=-1&travellerType=1&familyMode=off\"\n",
      "#TO-DO: Can URL be written over a few lines of code for readability? \n",
      "#       Length of line of code appears too long, should be limited at 79 characters\n",
      "driver.get(URL) \n",
      "\n",
      "\n",
      "time.sleep(2)\n",
      "try:\n",
      "    accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"consent-banner-container\"]/div/div[2]/div/button[2]')\n",
      "    accept_cookies_button.click()\n",
      "except:\n",
      "    pass #\n",
      "\n",
      "time.sleep(2)\n",
      "review_bar = driver.find_element(by=By.XPATH, value='//*[@id=\"sort-bar\"]/div/a[2]')\n",
      "review_bar.click() \n",
      "time.sleep(2)  \n",
      "\n",
      "time.sleep(2)\n",
      "try:\n",
      "    hotel_container = driver.find_element(by=By.XPATH, value='//*[@id=\"contentContainer\"]') # XPath corresponding to the Container\n",
      "    hotel_list = hotel_container.find_elements(by=By.XPATH, value='//li/div/a[contains(@class, \"PropertyCard__Link\")]')\n",
      "    for hotel_link in hotel_list:\n",
      "        time.sleep(10)\n",
      "        links = hotel_link.get_attribute('href')\n",
      "        hotel_link_list.append(links)  \n",
      "except:\n",
      "    pass  \n",
      "\n",
      "\n",
      "for link in extended_hotel_link_list[0:10]:\n",
      "    driver.get(link)\n",
      "    time.sleep(20)\n",
      "    try:\n",
      "        hotel_name = driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div[2]/div[1]/h1').text\n",
      "        dict_properties['Hotel Name'].append(hotel_name)\n",
      "    except:\n",
      "        hotel_name = 'Name not Found'\n",
      "        dict_properties['Hotel Name'].append(hotel_name)\n",
      "        continue\n",
      "\n",
      "print(dict_properties['Hotel Name'])\n",
      "105/2:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/search?city=17193&checkIn=2023-03-03&los=14&rooms=1&adults=2&children=0&cid=1891458&locale=en-gb&ckuid=9cdf223f-a3ce-4890-965a-b2aab1ae50d6&prid=0&gclid=Cj0KCQjwqc6aBhC4ARIsAN06NmOZjaRodGcrUeKINOPnKgTZEfVb8z7hDSCrkddzvLoQZA07tdLlcnsaAg2qEALw_wcB&currency=GBP&correlationId=d880d555-ceda-41ba-b708-41c298e4db9b&pageTypeId=1&realLanguageId=16&languageId=1&origin=GB&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&userId=9cdf223f-a3ce-4890-965a-b2aab1ae50d6&whitelabelid=1&loginLvl=0&storefrontId=3&currencyId=2&currencyCode=GBP&htmlLanguage=en-gb&cultureInfoName=en-gb&machineName=am-pc-4f-acm-web-user-7d974b9749-vw7ck&trafficGroupId=5&sessionId=ug4znrtc3dsxzbtr2i4zpami&trafficSubGroupId=122&aid=82361&useFullPageLogin=true&cttp=4&isRealUser=true&mode=production&checkOut=2023-03-17&priceCur=GBP&textToSearch=Bali&productType=-1&travellerType=1&familyMode=off\"\n",
      "#TO-DO: Can URL be written over a few lines of code for readability? \n",
      "#       Length of line of code appears too long, should be limited at 79 characters\n",
      "driver.get(URL) \n",
      "\n",
      "\n",
      "time.sleep(2)\n",
      "try:\n",
      "    accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"consent-banner-container\"]/div/div[2]/div/button[2]')\n",
      "    accept_cookies_button.click()\n",
      "except:\n",
      "    pass #\n",
      "\n",
      "time.sleep(2)\n",
      "review_bar = driver.find_element(by=By.XPATH, value='//*[@id=\"sort-bar\"]/div/a[2]')\n",
      "review_bar.click() \n",
      "time.sleep(2)  \n",
      "\n",
      "time.sleep(2)\n",
      "try:\n",
      "    hotel_container = driver.find_element(by=By.XPATH, value='//*[@id=\"contentContainer\"]') # XPath corresponding to the Container\n",
      "    hotel_list = hotel_container.find_elements(by=By.XPATH, value='//li/div/a[contains(@class, \"PropertyCard__Link\")]')\n",
      "    for hotel_link in hotel_list:\n",
      "        time.sleep(10)\n",
      "        links = hotel_link.get_attribute('href')\n",
      "        hotel_link_list.append(links)  \n",
      "except:\n",
      "    pass  \n",
      "\n",
      "\n",
      "for link in hotel_link_list[0:10]:\n",
      "    driver.get(link)\n",
      "    time.sleep(20)\n",
      "    try:\n",
      "        hotel_name = driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div[2]/div[1]/h1').text\n",
      "        dict_properties['Hotel Name'].append(hotel_name)\n",
      "    except:\n",
      "        hotel_name = 'Name not Found'\n",
      "        dict_properties['Hotel Name'].append(hotel_name)\n",
      "        continue\n",
      "\n",
      "print(dict_properties['Hotel Name'])\n",
      "106/1:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/gara-gara-sea-view/hotel/bali-id.html?finalPriceView=2&isShowMobileAppPrice=false&cid=1891458&numberOfBedrooms=&familyMode=false&adults=2&children=0&rooms=1&maxRooms=0&checkIn=2023-03-3&isCalendarCallout=false&childAges=&numberOfGuest=0&missingChildAges=false&travellerType=1&showReviewSubmissionEntry=false&currencyCode=GBP&isFreeOccSearch=false&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&isCityHaveAsq=false&los=14&searchrequestid=68a116fa-5dec-4ee0-a5fc-0df1cd39d52f\"\n",
      "#TO-DO: Can URL be written over a few lines of code for readability? \n",
      "#       Length of line of code appears too long, should be limited at 79 characters\n",
      "driver.get(URL)\n",
      "106/2:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/gara-gara-sea-view/hotel/bali-id.html?finalPriceView=2&isShowMobileAppPrice=false&cid=1891458&numberOfBedrooms=&familyMode=false&adults=2&children=0&rooms=1&maxRooms=0&checkIn=2023-03-3&isCalendarCallout=false&childAges=&numberOfGuest=0&missingChildAges=false&travellerType=1&showReviewSubmissionEntry=false&currencyCode=GBP&isFreeOccSearch=false&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&isCityHaveAsq=false&los=14&searchrequestid=68a116fa-5dec-4ee0-a5fc-0df1cd39d52f\"\n",
      "#TO-DO: Can URL be written over a few lines of code for readability? \n",
      "#       Length of line of code appears too long, should be limited at 79 characters\n",
      "driver.get(URL) \n",
      "\n",
      "print(driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div[2]/div[1]/h1').text)\n",
      "106/3:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/gara-gara-sea-view/hotel/bali-id.html?finalPriceView=2&isShowMobileAppPrice=false&cid=1891458&numberOfBedrooms=&familyMode=false&adults=2&children=0&rooms=1&maxRooms=0&checkIn=2023-03-3&isCalendarCallout=false&childAges=&numberOfGuest=0&missingChildAges=false&travellerType=1&showReviewSubmissionEntry=false&currencyCode=GBP&isFreeOccSearch=false&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&isCityHaveAsq=false&los=14&searchrequestid=68a116fa-5dec-4ee0-a5fc-0df1cd39d52f\"\n",
      "#TO-DO: Can URL be written over a few lines of code for readability? \n",
      "#       Length of line of code appears too long, should be limited at 79 characters\n",
      "driver.get(URL) \n",
      "\n",
      "print(driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div[2]/span[1]/h1').text)\n",
      "106/4:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/gara-gara-sea-view/hotel/bali-id.html?finalPriceView=2&isShowMobileAppPrice=false&cid=1891458&numberOfBedrooms=&familyMode=false&adults=2&children=0&rooms=1&maxRooms=0&checkIn=2023-03-3&isCalendarCallout=false&childAges=&numberOfGuest=0&missingChildAges=false&travellerType=1&showReviewSubmissionEntry=false&currencyCode=GBP&isFreeOccSearch=false&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&isCityHaveAsq=false&los=14&searchrequestid=68a116fa-5dec-4ee0-a5fc-0df1cd39d52f\"\n",
      "\n",
      "driver.get(URL) \n",
      "\n",
      "#/html/body/div[11]/div/div[5]/div[2]/div[2]/div[1]/div/div[2]/span[1]\n",
      "print(driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div[2]/span[1]').text)\n",
      "106/5:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/gara-gara-sea-view/hotel/bali-id.html?finalPriceView=2&isShowMobileAppPrice=false&cid=1891458&numberOfBedrooms=&familyMode=false&adults=2&children=0&rooms=1&maxRooms=0&checkIn=2023-03-3&isCalendarCallout=false&childAges=&numberOfGuest=0&missingChildAges=false&travellerType=1&showReviewSubmissionEntry=false&currencyCode=GBP&isFreeOccSearch=false&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&isCityHaveAsq=false&los=14&searchrequestid=68a116fa-5dec-4ee0-a5fc-0df1cd39d52f\"\n",
      "\n",
      "driver.get(URL) \n",
      "\n",
      "#/html/body/div[11]/div/div[5]/div[2]/div[2]/div[1]/div/div[2]/span[1]\n",
      "//*[@id=\"property-main-content\"]/div[1]/div/div[2]/span[1]\n",
      "print(driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div/div[2]/span[1]').text)\n",
      "106/6:\n",
      "from typing import Dict, List\n",
      "from selenium import webdriver \n",
      "from selenium.webdriver.common.action_chains import ActionChains\n",
      "from selenium.webdriver.common.by import By\n",
      "import datetime\n",
      "import uuid\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "\n",
      "hotel_link_list = []\n",
      "extended_hotel_link_list = []\n",
      "dict_properties = {'ID': [],'Timestamp': [],'Hotel Name': [],'Price': [],'Location': [],'Rating/10': [],'Image URL': []}\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "URL = \"https://www.agoda.com/en-gb/gara-gara-sea-view/hotel/bali-id.html?finalPriceView=2&isShowMobileAppPrice=false&cid=1891458&numberOfBedrooms=&familyMode=false&adults=2&children=0&rooms=1&maxRooms=0&checkIn=2023-03-3&isCalendarCallout=false&childAges=&numberOfGuest=0&missingChildAges=false&travellerType=1&showReviewSubmissionEntry=false&currencyCode=GBP&isFreeOccSearch=false&tag=24bd4b3f-b6a1-50d5-e639-d9c49ca41c49&isCityHaveAsq=false&los=14&searchrequestid=68a116fa-5dec-4ee0-a5fc-0df1cd39d52f\"\n",
      "\n",
      "driver.get(URL) \n",
      "\n",
      "#/html/body/div[11]/div/div[5]/div[2]/div[2]/div[1]/div/div[2]/span[1]\n",
      "#//*[@id=\"property-main-content\"]/div[1]/div/div[2]/span[1]\n",
      "print(driver.find_element(by=By.XPATH, value= '//*[@id=\"property-main-content\"]/div[1]/div/div[2]/span[1]').text)\n",
      "110/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@\")\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "110/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@\")\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "110/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@\")\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "    \n",
      "driver = webdriver.Chrome()\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@\")\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@\")\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@title='accept]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(by=By.NAME, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id=\"notice\"]/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/9:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/10:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(2)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//html/body/div/div[2]/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/11:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(5)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//html/body/div/div[2]/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/12:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(5)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/13:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(5)\n",
      "accept_cookies = driver.find_element(by=By.CSS_SELECTOR, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/14:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(5)\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/15:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(5)\n",
      "accept_cookies_frame = driver.switch_to\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/16:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "accept_cookies_frame = driver.switch_to.frame(\"//*[@id=\"sp_message_iframe_756623\"]\")\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/17:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "accept_cookies_frame = driver.switch_to.frame(\"//*[@id='sp_message_iframe_756623']\")\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/18:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to().frame(\"//*[@id='sp_message_iframe_756623']\")\n",
      "accept_cookies = accept_cookies_frame.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/19:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.findElement(By.xpath\"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = accept_cookies_frame.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/20:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.findElement(by=By.XPATH, value =\"//button[@title='accept']\"))\n",
      "accept_cookies = driver.find_element(by=By.ID, value=\"//button[@id='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/21:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by=By.XPATH, value =\"//button[@title='accept']\"))\n",
      "accept_cookies = driver.find_element(by=By.ID, value=\"//button[@id='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/22:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.findElement(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = accept_cookies_frame.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/23:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = accept_cookies_frame.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/24:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//button[@title='accept']\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/25:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id=\"notice\"]/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/26:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//button[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/27:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.ID, value=\"//*[@id='button-play']\")\n",
      "play_button.click()\n",
      "111/28:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "play_button.click()\n",
      "112/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "play_button.click()\n",
      "112/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "time.sleep(3)\n",
      "\n",
      "accept_cookies_frame = driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "accept_cookies = driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "accept_cookies.click()\n",
      "\n",
      "play_button = driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "play_button.click()\n",
      "\n",
      "answer_button = driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "answer_button.click()\n",
      "113/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot(self):\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def accept_cookies(self)\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self)\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ = \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "113/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot(self):\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self)\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ = \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "113/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot(self):\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ = \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "113/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot(self):\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "113/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "113/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "114/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "115/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "        #sends keys with credentials\n",
      "        #self.driver.\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "116/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "116/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"/html/body/div[12]/div/div/div/div[3]/form/button\")#//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "117/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"/html/body/div[12]/div/div/div/div[3]/form/button\")#//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "117/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='bcx_local_storage_frame']\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "117/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "117/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "        sign_in_button.click()\n",
      "\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to().active_element()\n",
      "        user_name.sendkeys(\"andrewmcnamara@live.co.uk\")\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element()\n",
      "        user_name.sendkeys(\"andrewmcnamara@live.co.uk\")\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element\n",
      "        user_name.sendkeys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.sendkeys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(\"T00narmyspc\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element\n",
      "        user_name.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(\"T00narmyspc\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element\n",
      "        user_name.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.TAB)\n",
      "        user_name.send_keys(\"T00narmyspc\")\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "121/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element\n",
      "        user_name.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(\"T00narmyspc\")\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "122/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        user_name = self.driver.switch_to.active_element\n",
      "        user_name.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(\"T00narmyspc\")\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.TAB)\n",
      "        user_name.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "123/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(\"T00narmyspc\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "123/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(\"T00narmyspc\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        #self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "123/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(\"T00narmyspc\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"/html/body/div[12]/div/iframe\"))\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "124/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(\"T00narmyspc\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(keys.Keys.ENTER)\n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "126/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        email_address.send_keys(\"T00narmyspc\")\n",
      "        email_address.send_keys(keys.Keys.TAB)\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    #bot.play_game()\n",
      "    #bot.find_answer()\n",
      "    bot.login()\n",
      "126/2:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "    bot.login()\n",
      "126/3:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "    bot.login()\n",
      "126/4:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "126/5:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(2)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(2)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "        time.sleep(2)\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        \n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(2)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(2)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "126/6:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(2)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "        time.sleep(2)\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        \n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(2)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(2)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "126/7:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "        time.sleep(3)\n",
      "        \n",
      "\n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        \n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(3)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(3)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "126/8:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        \n",
      "        #switches to login frame and sends keys with credentials\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='log-in-button']\")\n",
      "        login_button.click\n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "\n",
      "        time.sleep(3)\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(3)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(3)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "126/9:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "    \n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        \n",
      "        time.sleep(3)\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(3)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(3)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "126/10:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "    \n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        \n",
      "        time.sleep(3)\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(3)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(3)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "127/1:\n",
      "from numpy import append\n",
      "from selenium import webdriver\n",
      "import time\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.common import keys\n",
      "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import NoSuchElementException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "class SporcleAutobot():\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        super().__init__()\n",
      "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "    \n",
      "    def load_page(self):\n",
      "        '''Opens the Meatloaf page'''\n",
      "        self.driver.get(\"https://www.sporcle.com/games/armeenrashid/what-is-the-only-thing-a-meatloaf-wont-do-for-love\")\n",
      "        time.sleep(3)\n",
      "\n",
      "    def login(self):\n",
      "        '''logs user in if not already done so'''\n",
      "        try:\n",
      "            sign_in_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='user-not-logged-in']\")\n",
      "            sign_in_button.click()\n",
      "        except NoSuchElementException:\n",
      "            pass\n",
      "        \n",
      "        time.sleep(3)\n",
      "        self.driver.find_element(by=By.XPATH, value=\"/html/body/div[12]/div/div/div/div[3]\")\n",
      "        email_address = self.driver.find_element(by=By.XPATH, value=\"//*[@id='email']\")\n",
      "        email_address.send_keys(\"andrewmcnamara@live.co.uk\")\n",
      "        password = self.driver.find_element(by=By.XPATH, value=\"//*[@id='password']\")\n",
      "        password.send_keys(\"T00narmyspc\")\n",
      "        login_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='log-in-button']\")\n",
      "        login_button.click()\n",
      "    \n",
      "\n",
      "    def accept_cookies(self):\n",
      "        '''Switches to cookies frame and clicks the accept button'''\n",
      "        accept_cookies_frame = self.driver.switch_to.frame(self.driver.find_element(by= By.XPATH, value = \"//*[@id='sp_message_iframe_756623']\"))\n",
      "        accept_cookies = self.driver.find_element(by=By.XPATH, value=\"//*[@id='notice']/div[5]/button[2]\")\n",
      "        accept_cookies.click()\n",
      "\n",
      "    def play_game(self):\n",
      "        \n",
      "        time.sleep(3)\n",
      "        play_button = self.driver.find_element(by=By.XPATH, value=\"//*[@id='button-play']\")\n",
      "        play_button.click()\n",
      "        time.sleep(3)\n",
      "\n",
      "\n",
      "    def find_answer(self):\n",
      "        '''Clicks the button(s) required to finish the quiz'''\n",
      "        time.sleep(3)\n",
      "        answer_button = self.driver.find_element(by=By.XPATH, value = \"//*[@id='slot0']/div\")\n",
      "        answer_button.click()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    bot = SporcleAutobot()\n",
      "    bot.load_page()\n",
      "    bot.accept_cookies()\n",
      "    bot.login()\n",
      "    bot.play_game()\n",
      "    bot.find_answer()\n",
      "130/1: fetch('https://books.toscrape.com/')\n",
      "130/2: response\n",
      "130/3: response.css('article.product_pod')\n",
      "130/4: response.css('article.product_pod').get()\n",
      "130/5: books = response.css('article.product_pod').get()\n",
      "130/6: len(books)\n",
      "130/7: book = books[0]\n",
      "130/8: book.css('h3 a ::text').get()\n",
      "130/9:  response\n",
      "131/1: fetch('https://books.toscrape.com/')\n",
      "131/2:  response\n",
      "131/3: books = response.css('article.product_pod')\n",
      "131/4: len(books)\n",
      "131/5: book =\n",
      "131/6: book = books[0]\n",
      "131/7: book.css('h3 a ::text').get()\n",
      "131/8: book.css('.product_price .price_color::text').get()\n",
      "131/9: book.css('h3 a::href').get()\n",
      "131/10: book.css('h3 a').attrib['href']\n",
      "132/1: fetch('https://books.toscrape.com/')\n",
      "132/2:  response.css('li.next a ::attr(href)').get()\n",
      "133/1: fetch('https://books.toscrape.com/')\n",
      "133/2: response.css('.product_page')\n",
      "133/3: fetch('https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html')\n",
      "133/4: response.css('.product_page')\n",
      "133/5: response.css('product_main h1::text')\n",
      "133/6: response.css('.product_main h1::text')\n",
      "133/7: response.css('.product_main h1::text').get()\n",
      "134/1: fetch('https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html')\n",
      "134/2: response.css('.product_page')\n",
      "134/3: response.css('.product_main h1::text').get()\n",
      "134/4: response.xpath(22//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()\").get()\n",
      "134/5: response.xpath(\"//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()\").get()\n",
      "134/6: table_rows = response.css(\"table tr\")\n",
      "134/7: len(table_rows)\n",
      "135/1: fetch('https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html')\n",
      "135/2: table_rows = response.css(\"table tr\")\n",
      "135/3: len(table_rows)\n",
      "135/4: table_rows[1].css('td ::text').get()\n",
      "135/5: table_rows[2].css('td ::text').get()\n",
      "135/6: response.css(\"p.star-rating\").atrrib['class']\n",
      "135/7: response.css(\"p.star-rating\").atrrib['class']\n",
      "135/8: response.css(\"p.star-rating\").atrrib['class']\n",
      "136/1:\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals')\n",
      "sf_sal\n",
      "136/2:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals')\n",
      "sf_sal\n",
      "137/1:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals')\n",
      "sf_sal\n",
      "137/2:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/salaries.csv')\n",
      "sf_sal\n",
      "137/3:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('salaries.csv')\n",
      "sf_sal\n",
      "137/4:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "137/5:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "137/6:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.info()\n",
      "137/7:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal['Agency']\n",
      "137/8:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "137/9:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "\n",
      "sf_sal[[\"JobTitle\", \"isPolice\"]]\n",
      "137/10:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "sf_sal[[\"JobTitle\", \"isPolice\",\"idFire\"]]\n",
      "137/11:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "sf_sal[[\"JobTitle\", \"isPolice\",\"isFire\"]]\n",
      "137/12:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "print(sf_sal[\"isFire\"] + \"\" + sf_sal[\"isPolice\"])\n",
      "137/13:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "print(sf_sal[\"isFire\"].sum() + \"\" + sf_sal[\"isPolice\"].sum())\n",
      "137/14:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "print(no_of_fuzz)\n",
      "no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "137/15:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "print(no_of_fuzz)\n",
      "no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "print(no_of_fire)\n",
      "137/16:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "print(no_of_fuzz)\n",
      "no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "print(avg_fuzz_pay)\n",
      "137/17:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "print(avg_fuzz_pay)\n",
      "137/18:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "print(avg_fuzz_pay)\n",
      "137/19:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "print(avg_fuzz_pay)\n",
      "137/20:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "#avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "#print(avg_fuzz_pay)\n",
      "137/21:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "#avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "#print(avg_fuzz_pay)\n",
      "137/22:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.info()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "#avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].mean()\n",
      "#print(avg_fuzz_pay)\n",
      "137/23:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, sf_sal['BasePay']].sum()\n",
      "print(avg_fuzz_pay)\n",
      "137/24:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, 'BasePay'].sum()\n",
      "print(avg_fuzz_pay)\n",
      "137/25:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, 'BasePay'].mean()\n",
      "print(avg_fuzz_pay)\n",
      "137/26:\n",
      "import pandas as pd\n",
      "\n",
      "sf_sal = pd.read_csv('/home/andrew/AICore_work/Pandas Practicals/Salaries.csv')\n",
      "sf_sal.describe()\n",
      "\n",
      "def find_police(x):\n",
      "    return \"POLICE\" in x\n",
      "\n",
      "def find_fire(x):\n",
      "    return \"FIRE\" in x\n",
      "\n",
      "sf_sal[\"isPolice\"] = sf_sal[\"JobTitle\"].apply(find_police)\n",
      "sf_sal[\"isFire\"] = sf_sal[\"JobTitle\"].apply(find_fire)\n",
      "\n",
      "# no_of_fuzz = sf_sal[\"isPolice\"].sum()\n",
      "# print(no_of_fuzz)\n",
      "# no_of_fire = sf_sal[\"isFire\"].sum()\n",
      "# print(no_of_fire)\n",
      "\n",
      "avg_fuzz_pay = sf_sal.loc[sf_sal[\"isPolice\"] == True, 'BasePay'].mean()\n",
      "print(avg_fuzz_pay)\n",
      "\n",
      "avg_fire_pay = sf_sal.loc[sf_sal[\"isFire\"] == True, 'BasePay'].mean()\n",
      "print(avg_fire_pay)\n",
      "138/1:\n",
      "import numpy as np\n",
      "\n",
      "five_ele_array = np.array([3,6,5,12,19])\n",
      "print(five_ele_array[4])\n",
      "138/2:\n",
      "import numpy as np\n",
      "\n",
      "five_ele_array = np.array([3,6,5,12,19])\n",
      "print(five_ele_array[4])\n",
      "\n",
      "twod_array = np.ndarray((3,4),np.random.randint(0,10))\n",
      "138/3:\n",
      "import numpy as np\n",
      "\n",
      "five_ele_array = np.array([3,6,5,12,19])\n",
      "print(five_ele_array[4])\n",
      "\n",
      "twod_array = np.ndarray((3,4),np.random.randint(0,10))\n",
      "138/4:\n",
      "import numpy as np\n",
      "\n",
      "five_ele_array = np.array([3,6,5,12,19])\n",
      "print(five_ele_array[4])\n",
      "\n",
      "twod_array = np.ndarray((3,4),np.random.randint(0,10))\n",
      "138/5:\n",
      "import numpy as np\n",
      "\n",
      "five_ele_array = np.array([3,6,5,12,19])\n",
      "print(five_ele_array[4])\n",
      "\n",
      "twod_array = np.ndarray((3,4),np.random.randint(0,10))\n",
      "138/6:\n",
      "import numpy as np\n",
      "\n",
      "five_ele_array = np.array([3,6,5,12,19])\n",
      "print(five_ele_array[4])\n",
      "\n",
      "twod_array = np.random.randint(0,10,(3,4))\n",
      "print(twod_array)\n",
      "138/7:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.array((6,10))\n",
      "print(my_array)\n",
      "138/8:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "print(my_array)\n",
      "138/9:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "print(my_array)\n",
      "new_array = my_array(:,(0,2,4))\n",
      "138/10:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "print(my_array)\n",
      "new_array = my_array(:,0,2,4)\n",
      "138/11:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array(:,0)\n",
      "print(new_array)\n",
      "138/12:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array(:,0)\n",
      "print(new_array)\n",
      "138/13:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,0]\n",
      "print(new_array)\n",
      "138/14:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/15:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/16:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/17:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/18:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/19:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.ndarray((6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/20:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "138/21:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "\n",
      "my_array.shape()\n",
      "new_array.shape()\n",
      "138/22:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "#print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "\n",
      "my_array.shape\n",
      "new_array.shape\n",
      "138/23:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "\n",
      "my_array.shape\n",
      "new_array.shape\n",
      "138/24:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "\n",
      "my_array.shape\n",
      "#new_array.shape\n",
      "138/25:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "print(my_array)\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "\n",
      "my_array.shape\n",
      "new_array.shape\n",
      "138/26:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "print(my_array)\n",
      "my_array.shape\n",
      "\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "new_array.shape\n",
      "138/27:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "print(my_array)\n",
      "my_array.shape\n",
      "\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "new_array.shape\n",
      "print(my_array*new_array)\n",
      "138/28:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "np.transpose(my_array)\n",
      "print(my_array)\n",
      "my_array.shape\n",
      "\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "new_array.shape\n",
      "print(my_array*new_array)\n",
      "138/29:\n",
      "import numpy as np\n",
      "\n",
      "# five_ele_array = np.array([3,6,5,12,19])\n",
      "# print(five_ele_array[4])\n",
      "\n",
      "# twod_array = np.random.randint(0,10,(3,4))\n",
      "# print(twod_array)\n",
      "\n",
      "my_array = np.random.randint(0,10,(6,10))\n",
      "np.transpose(my_array)\n",
      "print(my_array)\n",
      "my_array.shape\n",
      "\n",
      "new_array = my_array[:,(0,2,4)]\n",
      "print(new_array)\n",
      "new_array.shape\n",
      "\n",
      "my_array = np.transpose(my_array)\n",
      "print(my_array*new_array)\n",
      "140/1:\n",
      "import pandas\n",
      "import seaborn\n",
      "import missingno\n",
      "140/2: df = pd.read_csv('loan_payments.csv')\n",
      "140/3: df = pd.read_csv('loan_payments.csv')\n",
      "140/4: df = pd.read_csv('loan_payments.csv')\n",
      "140/5: df = pd.read_csv('loan_payments.csv')\n",
      "140/6:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "140/7: df = pd.read_csv('loan_payments.csv')\n",
      "140/8: msno.bar(df)\n",
      "140/9: msno.heatmap(df)\n",
      "140/10: df[\"funded_amount\"] = df.fillna(value=df[\"funded_amount\"],inplace=True )\n",
      "140/11: msno.bar(df)\n",
      "140/12:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "140/13: df = pd.read_csv('loan_payments.csv')\n",
      "140/14: msno.bar(df)\n",
      "140/15: df[\"funded_amount\"] = df.fillna(value=df[\"loan_amount\"],inplace=True )\n",
      "140/16: msno.bar(df)\n",
      "140/17: msno.bar(df)\n",
      "140/18: df = pd.read_csv('loan_payments.csv')\n",
      "140/19: msno.bar(df)\n",
      "140/20:\n",
      "\n",
      "\n",
      "dataframe = db_utils.DataTransform()\n",
      "dataframe.set_data_frame('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "140/21:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/22:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/23:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/24:\n",
      "dataframe = db_utils.DataTransform()\n",
      "dataframe.set_data_frame('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "140/25: msno.bar(df)\n",
      "140/26:\n",
      "dataframe = db_utils.DataTransform()\n",
      "df = dataframe.set_data_frame('loan_payments.csv')\n",
      "df.fix_data_types('loan_payments.csv')\n",
      "140/27:\n",
      "dataframe = db_utils.DataTransform()\n",
      "dataframe.set_data_frame('loan_payments.csv')\n",
      "df= dataframe.fix_data_types('loan_payments.csv')\n",
      "140/28: msno.bar(df)\n",
      "140/29: msno.bar(df)\n",
      "140/30:\n",
      "dataframe = db_utils.DataTransform()\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "140/31: msno.bar(df)\n",
      "140/32:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "140/33: msno.bar(df)\n",
      "140/34:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "140/35: msno.bar(df)\n",
      "140/36:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(df)\n",
      "140/37:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types(df)\n",
      "print(df)\n",
      "140/38:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/39:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types(df)\n",
      "print(df)\n",
      "140/40:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(df)\n",
      "140/41:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types(df)\n",
      "print(df)\n",
      "140/42:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types(df)\n",
      "#print(df)\n",
      "140/43:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "dataframe.fix_data_types(df)\n",
      "#print(df)\n",
      "140/44:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "#dataframe.fix_data_types(df)\n",
      "print(df)\n",
      "140/45:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "#dataframe.fix_data_types(df)\n",
      "print(type(df))\n",
      "140/46:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(type(df))\n",
      "140/47:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(type(df))\n",
      "140/48:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "print(type(df))\n",
      "df = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(type(df))\n",
      "140/49:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "print(type(df))\n",
      "df = dataframe.fix_data_types(df)\n",
      "print(type(df))\n",
      "140/50:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/51:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "print(type(df))\n",
      "df = dataframe.fix_data_types(df)\n",
      "print(type(df))\n",
      "140/52:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/53:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "print(type(df))\n",
      "df2 = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(type(df2))\n",
      "140/54:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df2 = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(type(df2))\n",
      "140/55:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df2 = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(df2))\n",
      "140/56:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df2 = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(df2)\n",
      "140/57:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "140/58:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df2 = dataframe.fix_data_types('loan_payments.csv')\n",
      "print(df2)\n",
      "141/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "141/2:\n",
      "dataframe = db_utils.DataTransform()\n",
      "\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "141/3: df = pd.read_csv('loan_payments.csv')\n",
      "141/4: df.describe()\n",
      "141/5: df.head()\n",
      "141/6: df.dtypes()\n",
      "141/7: print(df.dtypes)\n",
      "141/8: msno.bar(df)\n",
      "141/9: msno.heatmap(df)\n",
      "141/10:\n",
      "#Fills missing funded amounts with that of loan amount\n",
      "df[\"funded_amount\"].drop()\n",
      "\n",
      "#df[\"funded_amount\"] = df.fillna(value=df[\"loan_amount\"], inplace=True )\n",
      "141/11:\n",
      "df['term'].str.replace(\" months \", \"\")\n",
      "df.rename(columns={\"term\": \"term(mths)\"}\n",
      "df.head()\n",
      "141/12:\n",
      "df['term'].str.replace(\" months \", \"\")\n",
      "df.rename(columns={\"term\": \"term(mths)\"})\n",
      "df.head()\n",
      "141/13:\n",
      "df['term'].str.replace(\" months \", \"\")\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df.head()\n",
      "141/14:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term'].str.replace(\" months \", \"\")\n",
      "df.head()\n",
      "141/15:\n",
      "#df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term'].str.replace(\" months \", \"\")\n",
      "df.head()\n",
      "141/16:\n",
      "#df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term'].str.replace(\" months \", \"\")\n",
      "df.head()\n",
      "141/17:\n",
      "#df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'].str.replace(\" months \", \"\")\n",
      "df.head()\n",
      "141/18:\n",
      "#df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'].str.replace(\" months \", \" \")\n",
      "df.head()\n",
      "141/19: df = pd.read_csv('loan_payments.csv')\n",
      "141/20:\n",
      "df['term'].str.replace(\" months \", \" \")\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df.head()\n",
      "141/21:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\" months \", \" \")\n",
      "141/22:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\" months \", \" \")\n",
      "df.head()\n",
      "141/23:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "141/24:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = dataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(col)\n",
      "\n",
      "df.head(25)\n",
      "141/25:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "import dataTransform\n",
      "141/26:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = dataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(col)\n",
      "\n",
      "df.head(25)\n",
      "141/27:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = dataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(col)\n",
      "\n",
      "df.head(25)\n",
      "141/28:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "141/29:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = dataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(col)\n",
      "\n",
      "df.head(25)\n",
      "141/30:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(col)\n",
      "\n",
      "df.head(25)\n",
      "141/31:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "142/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/2: df = pd.read_csv('loan_payments.csv')\n",
      "142/3: df.head()\n",
      "142/4: df.head(50)\n",
      "142/5: df = pd.read_csv('loan_payments.csv')\n",
      "142/6: df.head(50)\n",
      "142/7: df.head(5)\n",
      "142/8:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "142/9:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "142/10:\n",
      "for col in cols_to_fill:\n",
      "    print(df[col].str.count)\n",
      "142/11:\n",
      "for col in cols_to_fill:\n",
      "    print(df[col].str.count(\"N/A\"))\n",
      "142/12:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.int_type(col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/13:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.int_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/14:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.int_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/15:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.int_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/16:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/17:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/18:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/19:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/20:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/21:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/22:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/23:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "print(type(df))\n",
      "142/24: df.head(50)\n",
      "142/25: df.head(10)\n",
      "142/26: print(df.dtypes)\n",
      "142/27:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "142/28:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "142/29:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "print(type(df))\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/30:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/31:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/32:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/33:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/34:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "142/35:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/36:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/37:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/38:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/39:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/40:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "print(type(df))\n",
      "142/41: df.head(10)\n",
      "142/42: print(df.dtypes)\n",
      "142/43:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "142/44:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "142/45: print(df.dtypes)\n",
      "142/46:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "142/47:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "142/48:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/49:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/50:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/51:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/52:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/53:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/54:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/55:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/56:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "142/57:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/58:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "# for col in float_data:\n",
      "#     transform.float_type(df,col)\n",
      "\n",
      "# for col in bool_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "142/59:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "# for col in float_data:\n",
      "#     transform.float_type(df,col)\n",
      "\n",
      "# for col in bool_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "143/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "143/3: print(df.dtypes)\n",
      "143/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "143/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "143/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "# for col in float_data:\n",
      "#     transform.float_type(df,col)\n",
      "\n",
      "# for col in bool_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/7:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    df[column] = df[column].astype(int)\n",
      "\n",
      "\n",
      "# for col in float_data:\n",
      "#     transform.float_type(df,col)\n",
      "\n",
      "# for col in bool_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/8:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    df[col] = df[col].astype(int)\n",
      "\n",
      "\n",
      "# for col in float_data:\n",
      "#     transform.float_type(df,col)\n",
      "\n",
      "# for col in bool_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/9:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = ['term(mths)']\n",
      "float_data = ['mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    df[col] = df[col].astype(int)\n",
      "\n",
      "\n",
      "# for col in float_data:\n",
      "#     transform.float_type(df,col)\n",
      "\n",
      "# for col in bool_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/10:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/11:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "# for col in date_data:\n",
      "#     transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/12:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "# for col in cat_data:\n",
      "#     transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/13:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/14:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/15:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/16:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "143/17:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "143/18:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "143/19: print(df.dtypes)\n",
      "143/20:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "143/21:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "143/22:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "144/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "import dataFrameInfo\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "144/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "144/3:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "144/4:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "144/5:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "144/6: msno.bar(df)\n",
      "144/7: df.head()\n",
      "144/8: msno.heatmap(df)\n",
      "144/9:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import data_FrameInfo\n",
      "from dataFrameTransform import DataFrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "145/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import data_FrameInfo\n",
      "from dataFrameTransform import DataFrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "145/2:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import DataFrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "145/3:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import DataFrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "145/4:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "145/5:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "145/6: print(df.dtypes)\n",
      "145/7: print(df.dtypes)\n",
      "145/8:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "145/9:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "145/10:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "145/11: msno.bar(df)\n",
      "145/12: msno.heatmap(df)\n",
      "145/13:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "145/14:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "df.shape()\n",
      "145/15:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "145/16: msno.bar(df)\n",
      "145/17:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "146/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "146/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "146/3:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "146/4:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "146/5:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "146/6:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "146/7:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "146/8:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "146/9:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "146/10:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "147/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "147/2:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "147/3:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "147/4:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "147/5:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "147/6:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "147/7:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "147/8:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "147/9:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "147/10:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "147/11:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "147/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "147/13:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "147/14:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "148/1:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "148/2:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "148/3:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "148/4: print(df.dtypes)\n",
      "148/5:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "148/6:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "148/7:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "148/8: df.head()\n",
      "148/9: msno.bar(df)\n",
      "148/10: msno.heatmap(df)\n",
      "148/11:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "148/12: msno.bar(df)\n",
      "148/13:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "149/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "149/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "149/3: print(df.dtypes)\n",
      "149/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "149/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "149/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "149/7: df.head()\n",
      "149/8: msno.bar(df)\n",
      "149/9: msno.heatmap(df)\n",
      "149/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "149/11: msno.bar(df)\n",
      "149/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "150/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "150/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "150/3: print(df.dtypes)\n",
      "150/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "150/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "150/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "150/7: df.head()\n",
      "150/8: msno.bar(df)\n",
      "150/9: msno.heatmap(df)\n",
      "150/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "150/11: msno.bar(df)\n",
      "150/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "151/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "151/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "151/3: print(df.dtypes)\n",
      "151/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "151/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "151/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "151/7: df.head()\n",
      "151/8: msno.bar(df)\n",
      "151/9: msno.heatmap(df)\n",
      "151/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "151/11: msno.bar(df)\n",
      "151/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "151/13:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "dataframeinfo.count_distinct(df,'application_type')\n",
      "152/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "152/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "152/3: print(df.dtypes)\n",
      "152/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "152/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "152/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "152/7: df.head()\n",
      "152/8: msno.bar(df)\n",
      "152/9: msno.heatmap(df)\n",
      "152/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "152/11: msno.bar(df)\n",
      "152/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "dataframeinfo.count_distinct(df,'application_type')\n",
      "153/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "153/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "153/3: print(df.dtypes)\n",
      "153/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "153/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "153/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "153/7: df.head()\n",
      "153/8: msno.bar(df)\n",
      "153/9: msno.heatmap(df)\n",
      "153/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "153/11: msno.bar(df)\n",
      "153/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "dataframeinfo.count_distinct(df,'application_type')\n",
      "154/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "154/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "154/3: print(df.dtypes)\n",
      "154/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "154/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "154/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "154/7: df.head()\n",
      "154/8: msno.bar(df)\n",
      "154/9: msno.heatmap(df)\n",
      "154/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "154/11: msno.bar(df)\n",
      "154/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "dataframeinfo.count_distinct(df,'application_type')\n",
      "155/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "155/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "155/3: print(df.dtypes)\n",
      "155/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "155/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "155/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "155/7: df.head()\n",
      "155/8: msno.bar(df)\n",
      "155/9: msno.heatmap(df)\n",
      "155/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "155/11: msno.bar(df)\n",
      "155/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "\n",
      "\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','subgrade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "155/13:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "155/14:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar()\n",
      "156/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "156/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "156/3: print(df.dtypes)\n",
      "156/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "156/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "156/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "156/7: df.head()\n",
      "156/8: msno.bar(df)\n",
      "156/9: msno.heatmap(df)\n",
      "156/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "156/11: msno.bar(df)\n",
      "156/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "156/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar()\n",
      "157/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "157/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "157/3: print(df.dtypes)\n",
      "157/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "157/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "157/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "157/7: df.head()\n",
      "157/8: msno.bar(df)\n",
      "157/9: msno.heatmap(df)\n",
      "157/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "157/11: msno.bar(df)\n",
      "157/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "157/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar()\n",
      "157/14:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "157/15:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "157/16: print(df.dtypes)\n",
      "157/17:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "157/18:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "157/19:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "157/20: df.head()\n",
      "157/21: msno.bar(df)\n",
      "157/22: msno.heatmap(df)\n",
      "157/23:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "157/24: msno.bar(df)\n",
      "157/25:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "157/26:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar()\n",
      "157/27:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "157/28:\n",
      "for col in df:\n",
      "    \n",
      "    print(df[col].name())\n",
      "    print(f\"Skew of  column is {df[col].skew()}\")\n",
      "157/29:\n",
      "for col in df:\n",
      "    \n",
      "    print(df[col].name)\n",
      "    print(f\"Skew of  column is {df[col].skew()}\")\n",
      "157/30:\n",
      "for col in df:\n",
      "    if is_float_dtype(df[column]) == True or is_int64_dtype(df[column]) == True:\n",
      "        print(df[col].name)\n",
      "        print(f\"Skew of  column is {df[col].skew()}\")\n",
      "157/31:\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64':\n",
      "        print(df[col].name)\n",
      "        print(f\"Skew of  column is {df[col].skew()}\")\n",
      "157/32:\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64':\n",
      "        print(df[col].name)\n",
      "        print(f\"Skew of {col} is {df[col].skew()}\")\n",
      "157/33:\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64':\n",
      "        print(f\"Skew of {col} is {df[col].skew()}\")\n",
      "157/34: dataframeinfo.get_skew_info(df)\n",
      "157/35:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "157/36: dataframeinfo.get_skew_info(df)\n",
      "158/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "158/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "158/3: print(df.dtypes)\n",
      "158/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "158/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "158/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "158/7: df.head()\n",
      "158/8: msno.bar(df)\n",
      "158/9: msno.heatmap(df)\n",
      "158/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "158/11: msno.bar(df)\n",
      "158/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "158/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "158/14: dataframeinfo.get_skew_info(df)\n",
      "159/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "159/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "159/3: print(df.dtypes)\n",
      "159/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "159/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "159/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "159/7: df.head()\n",
      "159/8: msno.bar(df)\n",
      "159/9: msno.heatmap(df)\n",
      "159/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "159/11: msno.bar(df)\n",
      "159/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "159/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "159/14: dataframeinfo.get_skew_info(df)\n",
      "160/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "160/2:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "160/3:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "160/4: print(df.dtypes)\n",
      "160/5:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "160/6:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "160/7:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "160/8: df.head()\n",
      "160/9: msno.bar(df)\n",
      "160/10: msno.heatmap(df)\n",
      "160/11:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "160/12:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "160/13: visuals.visualize_high_skew(df)\n",
      "161/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "161/2:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "161/3:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "161/4:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "161/5: print(df.dtypes)\n",
      "161/6:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "161/7:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "161/8:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "161/9: df.head()\n",
      "161/10: msno.bar(df)\n",
      "161/11: msno.heatmap(df)\n",
      "161/12:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "161/13:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "161/14:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "161/15:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "161/16: dataframeinfo.get_skew_info(df)\n",
      "161/17: visuals.visualize_high_skew(df)\n",
      "161/18: visuals.visualize_high_skew(df, Data_FrameInfo.find_high_skew_cols())\n",
      "161/19:\n",
      "\n",
      "visuals.visualize_high_skew(df, dataframeinfo.find_high_skew_cols(df))\n",
      "161/20:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "161/21:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "161/22:\n",
      "\n",
      "visuals.visualize_high_skew(df, dataframeinfo.find_high_skew_cols(df))\n",
      "161/23:\n",
      "\n",
      "visuals.visualize_high_skew(df, dataframeinfo.find_high_skew_cols(df))\n",
      "161/24:\n",
      "\n",
      "visuals.visualize_high_skew(df, high_skew_cols=dataframeinfo.find_high_skew_cols(df))\n",
      "161/25:\n",
      "\n",
      "visuals.visualize_high_skew(df, high_skew_cols=dataframeinfo.find_high_skew_cols(df))\n",
      "161/26:\n",
      "\n",
      "visuals.visualize_high_skew(df, high_skew_cols=dataframeinfo.find_high_skew_cols(df))\n",
      "161/27:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "161/28:\n",
      "\n",
      "visuals.visualize_high_skew(df, high_skew_cols=dataframeinfo.find_high_skew_cols(df))\n",
      "161/29:\n",
      "\n",
      "visuals.visualize_high_skew(df, high_skew_cols=dataframeinfo.find_high_skew_cols(df))\n",
      "161/30:\n",
      "\n",
      "visuals.visualize_high_skew(df, high_skew_cols=dataframeinfo.find_high_skew_cols(df))\n",
      "161/31:\n",
      "visuals = plotter.Plotter()\n",
      "visuals.visualize_high_skew(df, dataframeinfo.find_high_skew_cols(df))\n",
      "161/32:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "161/33:\n",
      "visuals = plotter.Plotter()\n",
      "visuals.visualize_high_skew(df, dataframeinfo.find_high_skew_cols(df))\n",
      "161/34:\n",
      "visuals = plotter.Plotter()\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "161/35:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "161/36:\n",
      "visuals = plotter.Plotter()\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "162/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "162/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "162/3: print(df.dtypes)\n",
      "162/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "162/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "162/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "162/7: df.head()\n",
      "162/8: msno.bar(df)\n",
      "162/9: msno.heatmap(df)\n",
      "162/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "162/11:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "162/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "162/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "162/14: dataframeinfo.get_skew_info(df)\n",
      "162/15:\n",
      "visuals = plotter.Plotter()\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "163/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "163/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "163/3: print(df.dtypes)\n",
      "163/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "163/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "163/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "163/7: df.head()\n",
      "163/8: msno.bar(df)\n",
      "163/9: msno.heatmap(df)\n",
      "163/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "163/11:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "163/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "163/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "163/14: dataframeinfo.get_skew_info(df)\n",
      "163/15:\n",
      "visuals = plotter.Plotter()\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "164/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "164/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "164/3: print(df.dtypes)\n",
      "164/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "164/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "164/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "164/7: df.head()\n",
      "164/8: msno.bar(df)\n",
      "164/9: msno.heatmap(df)\n",
      "164/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "164/11:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "164/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "164/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "164/14: dataframeinfo.get_skew_info(df)\n",
      "164/15:\n",
      "visuals = plotter.Plotter()\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "164/16:\n",
      "visuals = plotter.Plotter()\n",
      "visuals.visualise_skewness()\n",
      "165/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "166/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "166/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "166/3: print(df.dtypes)\n",
      "166/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "166/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "166/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "166/7: df.head()\n",
      "166/8: msno.bar(df)\n",
      "166/9: msno.heatmap(df)\n",
      "166/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "166/11:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "166/12:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "166/13:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "166/14: print(df.dtypes)\n",
      "166/15:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "166/16:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "166/17:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "166/18: df.head()\n",
      "166/19: msno.bar(df)\n",
      "166/20: msno.heatmap(df)\n",
      "166/21:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "166/22:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "167/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "167/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "167/3: print(df.dtypes)\n",
      "167/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "167/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "167/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "167/7: df.head()\n",
      "167/8: msno.bar(df)\n",
      "167/9: msno.heatmap(df)\n",
      "167/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "167/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "167/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "167/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "167/14: dataframeinfo.get_skew_info(df)\n",
      "167/15: visuals.visualise_skewness()\n",
      "167/16:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "168/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "168/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "168/3: print(df.dtypes)\n",
      "168/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "168/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "168/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "168/7: df.head()\n",
      "168/8: msno.bar(df)\n",
      "168/9: msno.heatmap(df)\n",
      "168/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "168/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "168/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "168/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "168/14: dataframeinfo.get_skew_info(df)\n",
      "168/15: visuals.visualise_skewness()\n",
      "168/16:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "168/17:\n",
      "DataTransform.log_transform_skewed_columns(df)\n",
      "DataTransform.yjt_transform_skewed_columns(df)\n",
      "168/18:\n",
      "transform.log_transform_skewed_columns(df)\n",
      "transform.yjt_transform_skewed_columns(df)\n",
      "169/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "169/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "169/3: print(df.dtypes)\n",
      "169/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "169/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "169/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "169/7: df.head()\n",
      "169/8: msno.bar(df)\n",
      "169/9: msno.heatmap(df)\n",
      "169/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "169/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "169/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "169/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "169/14: dataframeinfo.get_skew_info(df)\n",
      "169/15: visuals.visualise_skewness()\n",
      "169/16:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "169/17:\n",
      "transform.log_transform_skewed_columns(df)\n",
      "transform.yjt_transform_skewed_columns(df)\n",
      "169/18:\n",
      "skewtransforms = DataTransform()\n",
      "\n",
      "skewtransforms.log_transform_skewed_columns(df)\n",
      "skewtransforms.yjt_transform_skewed_columns(df)\n",
      "170/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "170/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "170/3: print(df.dtypes)\n",
      "170/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "170/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "170/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "170/7: df.head()\n",
      "170/8: msno.bar(df)\n",
      "170/9: msno.heatmap(df)\n",
      "170/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "170/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "170/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "170/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "170/14: dataframeinfo.get_skew_info(df)\n",
      "170/15: visuals.visualise_skewness()\n",
      "170/16:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "170/17:\n",
      "skewtransforms = DataTransform()\n",
      "\n",
      "skewtransforms.log_transform_skewed_columns(df)\n",
      "skewtransforms.yjt_transform_skewed_columns(df)\n",
      "171/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "171/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "171/3: print(df.dtypes)\n",
      "171/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "171/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "171/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "171/7: df.head()\n",
      "171/8: msno.bar(df)\n",
      "171/9: msno.heatmap(df)\n",
      "171/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "171/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "171/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "171/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "171/14: dataframeinfo.get_skew_info(df)\n",
      "171/15: visuals.visualise_skewness()\n",
      "171/16:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "171/17:\n",
      "skewtransforms = DataTransform()\n",
      "\n",
      "skewtransforms.log_transform_skewed_columns(df)\n",
      "skewtransforms.yjt_transform_skewed_columns(df)\n",
      "172/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "172/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "172/3: print(df.dtypes)\n",
      "172/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "172/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "172/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "172/7: df.head()\n",
      "172/8: msno.bar(df)\n",
      "172/9: msno.heatmap(df)\n",
      "172/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "172/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "172/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "172/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "172/14: dataframeinfo.get_skew_info(df)\n",
      "172/15: visuals.visualise_skewness()\n",
      "172/16:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "172/17:\n",
      "skewtransforms = DataTransform()\n",
      "\n",
      "skewtransforms.log_transform_skewed_columns(df)\n",
      "skewtransforms.yjt_transform_skewed_columns(df)\n",
      "172/18:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "172/19:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "172/20:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "172/21: print(df.dtypes)\n",
      "172/22:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "172/23:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "172/24:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "172/25: df.head()\n",
      "172/26: msno.bar(df)\n",
      "172/27: msno.heatmap(df)\n",
      "172/28:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "172/29:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "172/30:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "172/31:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "172/32: dataframeinfo.get_skew_info(df)\n",
      "172/33: visuals.visualise_skewness()\n",
      "172/34:\n",
      "\n",
      "high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "visuals.visualize_high_skew(df, high_skew_cols)\n",
      "172/35:\n",
      "skewtransforms = DataTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "visuals.visualise_skewness()\n",
      "173/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "173/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "173/3: print(df.dtypes)\n",
      "173/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "173/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "174/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "174/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "174/3: print(df.dtypes)\n",
      "174/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "174/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "175/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "175/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "175/3: print(df.dtypes)\n",
      "175/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "175/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "175/6:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "175/7:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "175/8:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "175/9: print(df.dtypes)\n",
      "175/10:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "175/11:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "175/12:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "175/13:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "175/14: df.head()\n",
      "175/15: msno.bar(df)\n",
      "175/16:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "175/17:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "175/18:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "175/19:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "175/20:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "175/21: dataframeinfo.get_skew_info(df)\n",
      "175/22: visuals.visualise_skewness()\n",
      "176/1:\n",
      "visuals = plotter.Plotter()\n",
      "\n",
      "msno.bar(df)\n",
      "177/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "177/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "177/3: print(df.dtypes)\n",
      "177/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "177/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "177/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "177/7: df.head()\n",
      "177/8: msno.bar(df)\n",
      "177/9: msno.heatmap(df)\n",
      "177/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "177/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "177/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "177/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "177/14: dataframeinfo.get_skew_info(df)\n",
      "177/15: visuals.visualise_skewness()\n",
      "177/16:\n",
      "\n",
      "# high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "# visuals.visualize_high_skew(df, high_skew_cols)\n",
      "177/17:\n",
      "skewtransforms = DataTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "visuals.visualise_skewness()\n",
      "177/18:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "visuals.visualise_skewness()\n",
      "177/19: visuals.visualise_skewness()\n",
      "178/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "178/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "178/3: print(df.dtypes)\n",
      "178/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "178/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "178/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "178/7: df.head()\n",
      "178/8: msno.bar(df)\n",
      "178/9: msno.heatmap(df)\n",
      "178/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "178/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "178/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "178/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "178/14: dataframeinfo.get_skew_info(df)\n",
      "178/15: visuals.visualise_skewness(df)\n",
      "178/16:\n",
      "\n",
      "# high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "# visuals.visualize_high_skew(df, high_skew_cols)\n",
      "178/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "178/18:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "print(type(nonskewed_df))\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "178/19:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "print(type(nonskewed_df))\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "179/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "179/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "179/3: print(df.dtypes)\n",
      "179/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "179/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "179/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "179/7: df.head()\n",
      "179/8: msno.bar(df)\n",
      "179/9: msno.heatmap(df)\n",
      "179/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "179/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "179/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "179/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "179/14: dataframeinfo.get_skew_info(df)\n",
      "179/15: visuals.visualise_skewness(df)\n",
      "179/16:\n",
      "\n",
      "# high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "# visuals.visualize_high_skew(df, high_skew_cols)\n",
      "179/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "print(type(nonskewed_df))\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "179/18:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "print(type(nonskewed_df))\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "180/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "180/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "180/3: print(df.dtypes)\n",
      "180/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "180/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "180/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "180/7: df.head()\n",
      "180/8: msno.bar(df)\n",
      "180/9: msno.heatmap(df)\n",
      "180/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "180/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "180/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "180/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "180/14: dataframeinfo.get_skew_info(df)\n",
      "180/15: visuals.visualise_skewness(df)\n",
      "180/16:\n",
      "\n",
      "# high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "# visuals.visualize_high_skew(df, high_skew_cols)\n",
      "180/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('yjt_df')\n",
      "visuals.visualise_skewness(new_df)\n",
      "180/18:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('yjt_df.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "181/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "181/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "181/3: print(df.dtypes)\n",
      "181/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "181/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "181/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "181/7: df.head()\n",
      "181/8: msno.bar(df)\n",
      "181/9: msno.heatmap(df)\n",
      "181/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "181/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "181/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "181/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "181/14: dataframeinfo.get_skew_info(df)\n",
      "181/15: visuals.visualise_skewness(df)\n",
      "181/16:\n",
      "\n",
      "# high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "# visuals.visualize_high_skew(df, high_skew_cols)\n",
      "181/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('yjt_df.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "181/18:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('yjt_df.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "181/19:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "181/20:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "181/21: print(df.dtypes)\n",
      "181/22:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "181/23:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "181/24:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "181/25: df.head()\n",
      "181/26: msno.bar(df)\n",
      "181/27: msno.heatmap(df)\n",
      "181/28:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "181/29:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "181/30:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "181/31:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "181/32: dataframeinfo.get_skew_info(df)\n",
      "181/33: visuals.visualise_skewness(df)\n",
      "181/34:\n",
      "\n",
      "# high_skew_cols = dataframeinfo.find_high_skew_cols(df)\n",
      "# visuals.visualize_high_skew(df, high_skew_cols)\n",
      "181/35:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('yjt_df.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "181/36:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('df_yjt.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "181/37: visuals.visualise_outliers(df)\n",
      "182/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "182/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "182/3: print(df.dtypes)\n",
      "182/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "182/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "182/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "182/7: df.head()\n",
      "182/8: msno.bar(df)\n",
      "182/9: msno.heatmap(df)\n",
      "182/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "182/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "182/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "182/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "182/14: dataframeinfo.get_skew_info(df)\n",
      "182/15: visuals.visualise_skewness(df)\n",
      "182/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('df_yjt.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "182/17: visuals.visualise_outliers(df)\n",
      "182/18: visuals.visualise_outliers(new_df)\n",
      "182/19:\n",
      "outlier_trans = DataTransform()\n",
      "\n",
      "outlier_trans.treat_outliers(new_df)\n",
      "183/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "183/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "183/3: print(df.dtypes)\n",
      "183/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "183/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "183/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "183/7: df.head()\n",
      "183/8: msno.bar(df)\n",
      "183/9: msno.heatmap(df)\n",
      "183/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "183/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "183/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "183/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "183/14: dataframeinfo.get_skew_info(df)\n",
      "183/15: visuals.visualise_skewness(df)\n",
      "183/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('df_yjt.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "183/17: visuals.visualise_outliers(new_df)\n",
      "183/18:\n",
      "outlier_trans = DataF\n",
      "\n",
      "outlier_trans.treat_outliers(new_df)\n",
      "183/19:\n",
      "outlier_trans = dataFrameTransform()\n",
      "\n",
      "outlier_trans.treat_outliers(new_df)\n",
      "183/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outlier_trans.treat_outliers(new_df)\n",
      "184/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "184/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "184/3: print(df.dtypes)\n",
      "184/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "184/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "184/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "184/7: df.head()\n",
      "184/8: msno.bar(df)\n",
      "184/9: msno.heatmap(df)\n",
      "184/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "184/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "184/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "184/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "184/14: dataframeinfo.get_skew_info(df)\n",
      "184/15: visuals.visualise_skewness(df)\n",
      "184/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('df_yjt.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "184/17: visuals.visualise_outliers(new_df)\n",
      "184/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outlier_trans.treat_outliers(new_df)\n",
      "185/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "185/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "185/3: print(df.dtypes)\n",
      "185/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "185/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "185/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "185/7: df.head()\n",
      "185/8: msno.bar(df)\n",
      "185/9: msno.heatmap(df)\n",
      "185/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "185/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "185/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "185/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "185/14: dataframeinfo.get_skew_info(df)\n",
      "185/15: visuals.visualise_skewness(df)\n",
      "185/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "nonskewed_df = skewtransforms.yjt_transform_skewed_columns(df)\n",
      "\n",
      "new_df = pd.read_csv('df_yjt.csv')\n",
      "visuals.visualise_skewness(new_df)\n",
      "185/17: visuals.visualise_outliers(new_df)\n",
      "185/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outlier_trans.treat_outliers(new_df)\n",
      "186/1:\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import PowerTransformer\n",
      "187/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "187/2: visuals.visualise_skewness(df)\n",
      "187/3:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "187/4:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "187/5:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "187/6:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "187/7:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "187/8: print(df.dtypes)\n",
      "187/9:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "187/10:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "187/11:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "187/12: df.head()\n",
      "187/13: msno.bar(df)\n",
      "187/14: msno.heatmap(df)\n",
      "187/15:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "187/16:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "187/17:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "187/18:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "187/19: dataframeinfo.get_skew_info(df)\n",
      "187/20: visuals.visualise_skewness(df)\n",
      "187/21:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    nonskewed_df = skewtransforms.transform_column(col)\n",
      "\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "188/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "188/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "188/3: print(df.dtypes)\n",
      "188/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "188/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "188/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "188/7: df.head()\n",
      "188/8: msno.bar(df)\n",
      "188/9: msno.heatmap(df)\n",
      "188/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "188/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "188/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "188/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "188/14: dataframeinfo.get_skew_info(df)\n",
      "188/15: visuals.visualise_skewness(df)\n",
      "188/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "188/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "188/18:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "188/19: visuals.visualise_outliers(nonskewed_df)\n",
      "188/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outlier_trans.treat_outliers(nonskewed_df)\n",
      "188/21: visuals.visualise_outliers(nonskewed_df)\n",
      "188/22:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers(nonskewed_df)\n",
      "188/23: visuals.visualise_outliers(nonskewed_df)\n",
      "188/24: visuals.visualise_outliers(outliers_removed_df)\n",
      "189/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "189/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "189/3: print(df.dtypes)\n",
      "189/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "189/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "189/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "189/7: df.head()\n",
      "189/8: msno.bar(df)\n",
      "189/9: msno.heatmap(df)\n",
      "189/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "189/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "189/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "189/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "189/14: dataframeinfo.get_skew_info(df)\n",
      "189/15: visuals.visualise_skewness(df)\n",
      "189/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "189/17: visuals.visualise_outliers(nonskewed_df)\n",
      "189/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers(nonskewed_df)\n",
      "189/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "189/20: msno.heatmap(nonskewed_df)\n",
      "189/21: msno.heatmap(df)\n",
      "189/22:\n",
      "dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
      "\n",
      "#plt.show()\n",
      "190/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "190/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "190/3: print(df.dtypes)\n",
      "190/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "190/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "190/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "190/7: df.head()\n",
      "190/8: msno.bar(df)\n",
      "190/9: msno.heatmap(df)\n",
      "190/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "190/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "190/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "190/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "190/14: dataframeinfo.get_skew_info(df)\n",
      "190/15: visuals.visualise_skewness(df)\n",
      "190/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "190/17: visuals.visualise_outliers(nonskewed_df)\n",
      "190/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "190/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "190/20:\n",
      "dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
      "\n",
      "#plt.show()\n",
      "190/21: dataplot = visuals.show_correlation_heatmap(df)\n",
      "191/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "191/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "191/3: print(df.dtypes)\n",
      "191/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "191/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "191/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "191/7: df.head()\n",
      "191/8: msno.bar(df)\n",
      "191/9: msno.heatmap(df)\n",
      "191/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "191/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "191/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "191/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "191/14: dataframeinfo.get_skew_info(df)\n",
      "191/15: visuals.visualise_skewness(df)\n",
      "191/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "191/17: visuals.visualise_outliers(nonskewed_df)\n",
      "191/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "191/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "191/20: dataplot = visuals.show_correlation_heatmap(df)\n",
      "192/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "192/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "192/3: print(df.dtypes)\n",
      "192/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "192/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "192/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "192/7: df.head()\n",
      "192/8: msno.bar(df)\n",
      "192/9: msno.heatmap(df)\n",
      "192/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "192/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "192/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "192/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "192/14: dataframeinfo.get_skew_info(df)\n",
      "192/15: visuals.visualise_skewness(df)\n",
      "192/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "192/17: visuals.visualise_outliers(nonskewed_df)\n",
      "192/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "192/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "192/20: dataplot = visuals.show_correlation_heatmap(df)\n",
      "192/21: dataplot = visuals.show_correlation_heatmap(nonskewed_df)\n",
      "192/22: dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "193/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "193/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "193/3: print(df.dtypes)\n",
      "193/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "193/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "193/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "193/7: df.head()\n",
      "193/8: msno.bar(df)\n",
      "193/9: msno.heatmap(df)\n",
      "193/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "193/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "193/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "193/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "193/14: dataframeinfo.get_skew_info(df)\n",
      "193/15: visuals.visualise_skewness(df)\n",
      "193/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "193/17: visuals.visualise_outliers(nonskewed_df)\n",
      "193/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "193/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "193/20: dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "193/21:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.corr()\n",
      "print(check_corr)\n",
      "193/22:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.corr()\n",
      "print(check_corr[0])\n",
      "193/23:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.corr()\n",
      "print(check_corr['annual_inc'])\n",
      "193/24:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "#print(check_corr['annual_inc'])\n",
      "193/25:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr['annual_inc'])\n",
      "193/26:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr['annual_inc'][1])\n",
      "193/27:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr['annual_inc'][1])\n",
      "193/28:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr['annual_inc']['loan_amount'])\n",
      "193/29:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.columns())\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/30:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.columns)\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/31:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.columns)\n",
      "\n",
      "for row in check_corr.columns:\n",
      "    if check_corr.columns[row]> 0.9:\n",
      "        print(\"True\")\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/32:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.columns)\n",
      "\n",
      "for row in check_corr.columns:\n",
      "    print(row)\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/33:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "#print(check_corr.columns)\n",
      "\n",
      "for row in check_corr.columns:\n",
      "    print(row)\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/34:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.columns)\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/35:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/36:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.shape())\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/37:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr.shape)\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/38:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr[3])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/39:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/40:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(i + j)\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/41:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            check_corr.drop(check_corr[j])\n",
      "\n",
      "print(check_corr)\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/42:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append[j]\n",
      "\n",
      "check_corr.drop(cols_to_drop)\n",
      "print(check_corr)\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/43:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "check_corr.drop(cols_to_drop)\n",
      "print(check_corr)\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/44:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/45:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns[i,len(check_corr.columns)]:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/46:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns[i:]:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/47:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns,i):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/48:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns,check_corr.columns[i]):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/49:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(check_corr[i]:):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/50:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(check_corr[i],check_corr[-1]):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/51:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(check_corr[i],check_corr[check_corr.columns[-1]]):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/52:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(check_corr[i],check_corr[check_corr.columns[-1]]):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/53:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(check_corr[i],check_corr[check_corr.columns[-1]]):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(j)\n",
      "            #cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/54:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(check_corr[i],check_corr[check_corr.columns[-1]]):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            #cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/55:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in range(i,len(check_corr.columns)):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            #cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/56:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/57:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/58:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            cols_to_drop.append(check_corr[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/59:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/60:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            #print(check_corr.columns[j])\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/61:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns[i:check_corr.columns[len(check_corr.columns)]]:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            #print(check_corr.columns[j])\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/62:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns[i:check_corr.columns[len(check_corr.columns)]]:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            #print(check_corr.columns[j])\n",
      "            cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/63:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns[i:check_corr.columns[len(check_corr.columns)-1]]:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            #print(check_corr.columns[j])\n",
      "            cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/64:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            print(check_corr.columns[j])\n",
      "            #cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/65:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(check_corr.columns[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/66:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/67:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/68:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in check_corr.columns:\n",
      "    for j in check_corr.columns:\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(j)\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/69:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i,j)\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/70:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i,j)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/71:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/72:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/73:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/74:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[0],j)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/75:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],check_corr[j])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/76:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],check_corr[j])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/77:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/78:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/79:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/80:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns, i):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/81:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns, check_corr.columns[i]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/82:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns, i[0]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/83:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i,len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/84:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/85:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i,j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/86:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i,j)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/87:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i,j)\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/88:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns[start=i[0]]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/89:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns[start=2]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/90:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns[start=2]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/91:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "for i in enumerate(check_corr.columns):\n",
      "    for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "        print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/92:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns()\n",
      "list_b = check_corr.columns()\n",
      "\n",
      "for i in list_a:\n",
      "    for j in range(i,len(list_b)):\n",
      "        print(i + j)\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/93:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in list_a:\n",
      "    for j in range(i,len(list_b)):\n",
      "        print(i + j)\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/94:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in list_a:\n",
      "    for j in range(list_b[i],len(list_b)):\n",
      "        print(i + j)\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/95:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a):\n",
      "    for j in range(list_b[i],len(list_b)):\n",
      "        print(i + j)\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/96:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(list_b[i],len(list_b)):\n",
      "        print(i + j)\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/97:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        print(i + j)\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/98:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        print(list_a[i] + list_b[j])\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "#         # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "#         #     if j not in cols_to_drop:\n",
      "#         #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "#print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/99:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "# for i in enumerate(check_corr.columns):\n",
      "#     for j in enumerate(check_corr.columns[check_corr.columns[i]:len(check_corr.columns)]):\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(j)\n",
      "#         print(i[1],j[1])\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/100:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        print(check_corr[i][j])\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/101:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        print(check_corr[i])\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/102:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        print(check_corr)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/103:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(list_b)):\n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/104:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "\n",
      "for i in range(0,len(list_a)):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/105:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "\n",
      "for i in range(0,len(check_corr.columns)):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/106:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "\n",
      "for i in range(0,len(check_corr.columns)):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/107:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "\n",
      "for i in range(0,len(check_corr.columns)):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(list_a[i][1],list_b[j][1])\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/108:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "193/109:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "\n",
      "for i in range(0,len(check_corr.columns)):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(list_a[i],list_b[j])\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/110:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = enumerate(check_corr.columns)\n",
      "list_b = enumerate(check_corr.columns)\n",
      "print(list_a[1])\n",
      "\n",
      "for i in range(0,len(check_corr.columns)):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(list_a,list_b)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/111:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "print(list_a[1])\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i, len(check_corr.columns)):\n",
      "        print(list_a,list_b)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/112:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "print(list_a[1])\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in enumerate(list_b):\n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/113:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "print(list_a[1])\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in enumerate(list_b):\n",
      "        j = i\n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/114:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "print(list_a[1])\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    j = i\n",
      "    for j in enumerate(list_b):\n",
      "        \n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/115:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "print(list_a[1])\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in enumerate(list_b):\n",
      "        \n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/116:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i,len(check_corr.columns)):\n",
      "        \n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/117:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i,len(check_corr.columns)):\n",
      "        \n",
      "        print(i)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/118:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/119:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i,j)\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/120:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i[1],list_b[j])\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/121:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i[1],list_b[j])\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/122:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i[1],list_b[j])\n",
      "        # if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "        #     if j not in cols_to_drop:\n",
      "        #         cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/123:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i[1],list_b[j])\n",
      "        if check_corr[i][j] > 0.9 and check_corr[i][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/124:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        print(i[1],list_b[j])\n",
      "        if check_corr[i[1]][j] > 0.9 and check_corr[i[1]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/125:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr[i[1]][j] > 0.9 and check_corr[i[1]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                #cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/126:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr[i[1]][j] > 0.9 and check_corr[i[1]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/127:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr.iloc[i[1]][j] > 0.9 and check_corr.iloc[i[1]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/128:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/129:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/130:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b.iloc[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/131:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "list_b = check_corr.columns\n",
      "\n",
      "\n",
      "for i in enumerate(list_a):\n",
      "    for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "        #print(i[1],list_b[j])\n",
      "        if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "            if j not in cols_to_drop:\n",
      "                cols_to_drop.append(list_b[j])\n",
      "        #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/132:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in (0,len(list_a)):\n",
      "    for j in (i,len(list_a)):\n",
      "        print(list_a[i], list_a[j]):\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/133:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in (0,len(list_a)):\n",
      "    for j in (i,len(list_a)):\n",
      "        print(list_a[i], list_a[j])\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/134:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in (0,len(list_a)-1):\n",
      "    for j in (i,len(list_a)-1):\n",
      "        print(list_a[i], list_a[j])\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/135:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "print(len(list_a))\n",
      "\n",
      "for i in (0,len(list_a)-1):\n",
      "    for j in (i,len(list_a)-1):\n",
      "        print(list_a[i], list_a[j])\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/136:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "print(len(list_a))\n",
      "\n",
      "for i in (0,len(list_a)-1,1):\n",
      "    for j in (i,len(list_a)-1,1):\n",
      "        print(list_a[i], list_a[j])\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/137:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "print(len)\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        print(list_a[i], list_a[j])\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/138:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "print(len)\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            print(list_a[i], list_a[j])\n",
      "\n",
      "\n",
      "# for i in enumerate(list_a):\n",
      "#     for j in range(i[0],len(check_corr.columns)):\n",
      "        \n",
      "#         #print(i[1],list_b[j])\n",
      "#         if check_corr.iloc[i[0]][j] > 0.9 and check_corr.iloc[i[0]][j]<1:\n",
      "#             if j not in cols_to_drop:\n",
      "#                 cols_to_drop.append(list_b[j])\n",
      "#         #print(list_a[i] + list_b[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/139:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            print(list_a[i], list_a[j])\n",
      "            cols_to_drop.append(list_a[j])\n",
      "\n",
      "\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/140:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/141:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "corr_removed_df = outliers_removed_df.drop(cols_to_drop)\n",
      "\n",
      "print(corr_removed_df.shape)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/142:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df.drop(col)\n",
      "\n",
      "print(outliers_removed_df.shape)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/143:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df.drop(col)\n",
      "\n",
      "print(outliers_removed_df.shape)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/144:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df.drop(col)\n",
      "\n",
      "print(cols_to_drop)\n",
      "print(outliers_removed_df.shape)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/145:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.shape)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/146:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "# for col in cols_to_drop:\n",
      "#     outliers_removed_df = outliers_removed_df.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.shape)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/147:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "# for col in cols_to_drop:\n",
      "#     outliers_removed_df = outliers_removed_df.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/148:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df.columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/149:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/150:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/151:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "193/152:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "193/153: print(df.dtypes)\n",
      "193/154:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "193/155:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "193/156:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "193/157: df.head()\n",
      "193/158: msno.bar(df)\n",
      "193/159: msno.heatmap(df)\n",
      "193/160:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "193/161:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "193/162:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "193/163:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "193/164: dataframeinfo.get_skew_info(df)\n",
      "193/165: visuals.visualise_skewness(df)\n",
      "193/166:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "193/167: visuals.visualise_outliers(nonskewed_df)\n",
      "193/168:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "193/169: visuals.visualise_outliers(outliers_removed_df)\n",
      "193/170:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "193/171:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/172:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = outliers_removed_df.columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/173:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/174:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = pd.DataFrame(outliers_removed_df).select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/175:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "\n",
      "check_corr = pd.DataFrame(outliers_removed_df).select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/176:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "outliers_removed_df = pd.DataFrame(outliers_removed_df)\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/177:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "193/178:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "193/179:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "outliers_removed_df = pd.DataFrame(outliers_removed_df)\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/180:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "outliers_removed_df = pd.DataFrame(outliers_removed_df)\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/181:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "193/182:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/183:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    print(col)\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/184:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "print(cols_to_drop)\n",
      "\n",
      "for col in cols_to_drop:\n",
      "    print(col)\n",
      "    print(outliers_removed_df.columns)\n",
      "    outliers_removed_df = pd.DataFrame(outliers_removed_df).columns.drop(col)\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/185:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "new_df = outliers_removed_df.drop(cols_to_drop)\n",
      "\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/186:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "new_df = outliers_removed_df.drop(cols_to_drop)\n",
      "\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/187:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                print(list_a[i], list_a[j])\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "new_df = outliers_removed_df.drop(cols_to_drop)\n",
      "\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/188:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "\n",
      "\n",
      "new_df = outliers_removed_df.drop(columns=cols_to_drop)\n",
      "\n",
      "\n",
      "\n",
      "print(outliers_removed_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/189:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "193/190:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "193/191: print(df.dtypes)\n",
      "193/192:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "193/193:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "193/194:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "193/195: df.head()\n",
      "193/196: msno.bar(df)\n",
      "193/197: msno.heatmap(df)\n",
      "193/198:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "193/199:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "193/200:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "193/201:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "193/202: dataframeinfo.get_skew_info(df)\n",
      "193/203: visuals.visualise_skewness(df)\n",
      "193/204:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "193/205: visuals.visualise_outliers(nonskewed_df)\n",
      "193/206:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "193/207: visuals.visualise_outliers(outliers_removed_df)\n",
      "193/208:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "193/209:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "\n",
      "\n",
      "new_df = outliers_removed_df.drop(columns=cols_to_drop)\n",
      "\n",
      "\n",
      "\n",
      "print(new_df.columns)\n",
      "\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "193/210:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "\n",
      "#print(check_corr)\n",
      "\n",
      "cols_to_drop = []\n",
      "\n",
      "list_a = check_corr.columns\n",
      "\n",
      "for i in range(0,len(list_a)-1):\n",
      "    for j in range(i,len(list_a)-1):\n",
      "        if check_corr.iloc[i,j] > 0.9 and check_corr.iloc[i,j] < 1:\n",
      "            if list_a[j] not in cols_to_drop:\n",
      "                cols_to_drop.append(list_a[j])\n",
      "\n",
      "\n",
      "\n",
      "new_df = outliers_removed_df.drop(columns=cols_to_drop)\n",
      "\n",
      "\n",
      "\n",
      "print(new_df.columns)\n",
      "print(new_df.shape)\n",
      "#print(check_corr['loan_amount']['annual_inc'])\n",
      "\n",
      "#print(check_corr['annual_inc']['loan_amount'])\n",
      "194/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "194/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "194/3: print(df.dtypes)\n",
      "194/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "194/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "194/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "194/7: df.head()\n",
      "194/8: msno.bar(df)\n",
      "194/9: msno.heatmap(df)\n",
      "194/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "194/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "194/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "194/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "194/14: dataframeinfo.get_skew_info(df)\n",
      "194/15: visuals.visualise_skewness(df)\n",
      "194/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "194/17: visuals.visualise_outliers(nonskewed_df)\n",
      "194/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "194/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "194/20:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "194/21:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "outlier_trans.remove_overcorrelation(outliers_removed_df, new_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "195/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "195/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "195/3: print(df.dtypes)\n",
      "195/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "195/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "195/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "195/7: df.head()\n",
      "195/8: msno.bar(df)\n",
      "195/9: msno.heatmap(df)\n",
      "195/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "195/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "195/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "195/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "195/14: dataframeinfo.get_skew_info(df)\n",
      "195/15: visuals.visualise_skewness(df)\n",
      "195/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "195/17: visuals.visualise_outliers(nonskewed_df)\n",
      "195/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "195/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "195/20:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "195/21:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(outliers_removed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "195/22:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(outliers_removed_df, 0.9)\n",
      "\n",
      "print(new_df.shape)\n",
      "195/23:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(outliers_removed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "195/24:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(nonskewed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "195/25:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "196/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "196/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "196/3: print(df.dtypes)\n",
      "196/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "196/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "196/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "196/7: df.head()\n",
      "196/8: msno.bar(df)\n",
      "196/9: msno.heatmap(df)\n",
      "196/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "196/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "196/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "196/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "196/14: dataframeinfo.get_skew_info(df)\n",
      "196/15: visuals.visualise_skewness(df)\n",
      "196/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "198/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "198/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "198/3: print(df.dtypes)\n",
      "198/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "198/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "198/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "198/7: df.head()\n",
      "198/8: msno.bar(df)\n",
      "198/9: msno.heatmap(df)\n",
      "198/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "198/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "198/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "198/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "198/14: dataframeinfo.get_skew_info(df)\n",
      "198/15: visuals.visualise_skewness(df)\n",
      "198/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "198/17: visuals.visualise_outliers(nonskewed_df)\n",
      "198/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "198/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "198/20:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "198/21:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(nonskewed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "199/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "200/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "200/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "200/3: print(df.dtypes)\n",
      "200/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "200/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "200/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "200/7: df.head()\n",
      "200/8: msno.bar(df)\n",
      "200/9: msno.heatmap(df)\n",
      "200/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "200/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "200/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "200/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "200/14: dataframeinfo.get_skew_info(df)\n",
      "200/15: visuals.visualise_skewness(df)\n",
      "200/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "200/17: visuals.visualise_outliers(nonskewed_df)\n",
      "200/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "200/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "200/20:\n",
      "check_corr = outliers_removed_df.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "200/21:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(nonskewed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "200/22: dataframeinfo.z_scores(nonskewed_df)\n",
      "200/23:\n",
      "data_to_check = nonskewed_df.select_dtypes('float64')\n",
      "\n",
      "dataframeinfo.z_scores(data_to_check)\n",
      "200/24:\n",
      "data_to_check = nonskewed_df.select_dtypes('float64')\n",
      "\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "200/25: print(z_scoresdf)\n",
      "200/26: print(z_scoresdf.loc[>abs(3.5)])\n",
      "200/27: print(z_scoresdf.loc[z_scoresdf>abs(3.5)])\n",
      "200/28: z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "200/29:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "print(rows_to_drop[0])\n",
      "200/30:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "print(rows_to_drop)\n",
      "200/31:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "print(rows_to_drop.get())\n",
      "200/32:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "print(rows_to_drop.index)\n",
      "200/33:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "print(rows_to_drop.index)\n",
      "\n",
      "df.drop(rows_to_drop)\n",
      "\n",
      "print(df)\n",
      "200/34:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "rwos_to_drop = rows_to_drop.index\n",
      "\n",
      "df.drop(rows_to_drop)\n",
      "\n",
      "print(df)\n",
      "200/35:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "rows_to_drop = rows_to_drop.index\n",
      "\n",
      "df.drop(rows_to_drop)\n",
      "\n",
      "print(df)\n",
      "200/36:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "rows_to_drop = rows_to_drop.index\n",
      "\n",
      "df1 = df.drop([rows_to_drop])\n",
      "\n",
      "print(df1)\n",
      "200/37:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "\n",
      "df1 = df.drop([rows_to_drop].index)\n",
      "\n",
      "print(df1)\n",
      "200/38:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "\n",
      "df1 = df.drop([rows_to_drop.index])\n",
      "\n",
      "print(df1)\n",
      "200/39:\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "\n",
      "df1 = df.drop(rows_to_drop.index)\n",
      "\n",
      "print(df1)\n",
      "201/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "201/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "201/3: print(df.dtypes)\n",
      "201/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "201/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "201/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "201/7: df.head()\n",
      "201/8: msno.bar(df)\n",
      "201/9: msno.heatmap(df)\n",
      "201/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "201/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "201/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "201/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "201/14: dataframeinfo.get_skew_info(df)\n",
      "201/15: visuals.visualise_skewness(df)\n",
      "201/16:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "201/17: visuals.visualise_outliers(nonskewed_df)\n",
      "201/18:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "201/19: visuals.visualise_outliers(outliers_removed_df)\n",
      "201/20:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = nonskewed_df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "201/21:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "201/22:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(nonskewed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "201/23: visuals.visualise_outliers(df1)\n",
      "201/24: pd.DataFrame.to_csv(df)\n",
      "201/25: pd.DataFrame.to_csv(nonskewed_df)\n",
      "201/26: visuals.visualise_outliers(df)\n",
      "201/27:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "201/28:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "201/29: print(df.dtypes)\n",
      "201/30:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "201/31:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "201/32:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "201/33: df.head()\n",
      "201/34: msno.bar(df)\n",
      "201/35: msno.heatmap(df)\n",
      "201/36:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "201/37:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "201/38:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "201/39:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "201/40: pd.DataFrame.to_csv(df)\n",
      "201/41: dataframeinfo.get_skew_info(df)\n",
      "201/42: visuals.visualise_skewness(df)\n",
      "201/43:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "201/44: pd.DataFrame.to_csv(nonskewed_df)\n",
      "201/45: visuals.visualise_outliers(df)\n",
      "201/46:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "201/47: visuals.visualise_outliers(outliers_removed_df)\n",
      "201/48:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = nonskewed_df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "201/49: visuals.visualise_outliers(df1)\n",
      "201/50:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "201/51:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(nonskewed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "201/52: df.to_csv('dfnonulls.csv')\n",
      "202/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "202/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "202/3: print(df.dtypes)\n",
      "202/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "202/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "202/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "202/7: df.head()\n",
      "202/8: msno.bar(df)\n",
      "202/9: msno.heatmap(df)\n",
      "202/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "202/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "202/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "202/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "202/14: df.to_csv('dfnonulls.csv')\n",
      "202/15: dataframeinfo.get_skew_info(df)\n",
      "202/16: visuals.visualise_skewness(df)\n",
      "202/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "202/18: pd.DataFrame.to_csv(nonskewed_df)\n",
      "202/19: visuals.visualise_outliers(df)\n",
      "202/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(nonskewed_df)\n",
      "202/21: visuals.visualise_outliers(outliers_removed_df)\n",
      "202/22:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = nonskewed_df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "202/23: visuals.visualise_outliers(df1)\n",
      "202/24:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "print(check_corr)\n",
      "202/25:\n",
      "#dataplot = visuals.show_correlation_heatmap(outliers_removed_df)\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(nonskewed_df, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "202/26: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "202/27:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(df)\n",
      "202/28: visuals.visualise_outliers(outliers_removed_df)\n",
      "202/29:\n",
      "visuals.visualise_outliers(outliers_removed_df)\n",
      "\n",
      "outliers_removed_df.to_csv('dfnooutliersIQR.csv')\n",
      "202/30:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "203/1:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "203/2:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "203/3:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "203/4:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "203/5: print(df.dtypes)\n",
      "203/6:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "203/7:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "203/8:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "203/9: df.head()\n",
      "203/10: msno.bar(df)\n",
      "203/11: msno.heatmap(df)\n",
      "203/12:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "203/13:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "203/14:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "203/15:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "203/16: df.to_csv('dfnonulls.csv')\n",
      "203/17: dataframeinfo.get_skew_info(df)\n",
      "203/18: visuals.visualise_skewness(df)\n",
      "203/19:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "203/20: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "203/21: visuals.visualise_outliers(df)\n",
      "203/22:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df = outlier_trans.treat_outliers_IQR(df)\n",
      "203/23:\n",
      "visuals.visualise_outliers(outliers_removed_df)\n",
      "\n",
      "outliers_removed_df.to_csv('dfnooutliersIQR.csv')\n",
      "203/24:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = nonskewed_df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "203/25:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "203/26:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "203/27:\n",
      "\n",
      "\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "203/28:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "203/29: df1.to_csv('dfdropcorrelation.csv')\n",
      "204/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "204/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "204/3: print(df.dtypes)\n",
      "204/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "204/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "204/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "204/7: df.head()\n",
      "204/8: msno.bar(df)\n",
      "204/9: msno.heatmap(df)\n",
      "204/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "204/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "204/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "204/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "204/14: df.to_csv('dfnonulls.csv')\n",
      "204/15: dataframeinfo.get_skew_info(df)\n",
      "204/16: visuals.visualise_skewness(df)\n",
      "204/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "visuals.visualise_skewness(nonskewed_df)\n",
      "204/18: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "204/19: visuals.visualise_outliers(df)\n",
      "204/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df_IQR = outlier_trans.treat_outliers_IQR(df)\n",
      "204/21:\n",
      "visuals.visualise_outliers(outliers_removed_df_IQR)\n",
      "\n",
      "outliers_removed_df_IQR.to_csv('dfnooutliersIQR.csv')\n",
      "204/22:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "204/23:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "204/24:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "204/25:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "204/26: new_df.to_csv('dfdropcorrelation.csv')\n",
      "204/27:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "204/28:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "for col in df:\n",
      "    if df[col].dtype == 'float64' or df[col].dtype == 'Int64' or df[col].dtype == 'int64' or df[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "204/29: df.head\n",
      "204/30:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "204/31:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "204/32: print(df.dtypes)\n",
      "204/33:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "204/34:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "204/35:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "204/36: df.head()\n",
      "204/37: msno.bar(df)\n",
      "204/38: msno.heatmap(df)\n",
      "204/39:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "204/40:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "204/41:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "204/42:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "204/43: df.to_csv('dfnonulls.csv')\n",
      "204/44: dataframeinfo.get_skew_info(df)\n",
      "204/45: visuals.visualise_skewness(df)\n",
      "204/46:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "#in order to keep the intergrity of my data it is necessary to create a copy of the dataframe before applying skew transforms\n",
      "\n",
      "df_to_unskew = df.copy()\n",
      "\n",
      "for col in df_to_unskew:\n",
      "    if df_to_unskew[col].dtype == 'float64' or df_to_unskew[col].dtype == 'Int64' or df_to_unskew[col].dtype == 'int64' or df_to_unskew[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "204/47: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "204/48: visuals.visualise_outliers(df)\n",
      "204/49:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df_IQR = outlier_trans.treat_outliers_IQR(df)\n",
      "204/50:\n",
      "visuals.visualise_outliers(outliers_removed_df_IQR)\n",
      "\n",
      "outliers_removed_df_IQR.to_csv('dfnooutliersIQR.csv')\n",
      "204/51:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "204/52:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "204/53:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "204/54:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "204/55: new_df.to_csv('dfdropcorrelation.csv')\n",
      "204/56: df.head\n",
      "204/57:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "#in order to keep the intergrity of my data it is necessary to create a copy of the dataframe before applying skew transforms\n",
      "\n",
      "df_to_unskew = df.copy()\n",
      "\n",
      "for col in df_to_unskew:\n",
      "    if df_to_unskew[col].dtype == 'float64' or df_to_unskew[col].dtype == 'Int64' or df_to_unskew[col].dtype == 'int64' or df_to_unskew[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df_to_unskew, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "204/58:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "204/59:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "204/60: print(df.dtypes)\n",
      "204/61:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "204/62:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "204/63:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "204/64: df.head()\n",
      "204/65: msno.bar(df)\n",
      "204/66: msno.heatmap(df)\n",
      "204/67:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "204/68:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "204/69:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "204/70:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "204/71: df.to_csv('dfnonulls.csv')\n",
      "204/72: dataframeinfo.get_skew_info(df)\n",
      "204/73: visuals.visualise_skewness(df)\n",
      "204/74:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "#in order to keep the intergrity of my data it is necessary to create a copy of the dataframe before applying skew transforms\n",
      "\n",
      "df_to_unskew = df.copy()\n",
      "\n",
      "for col in df_to_unskew:\n",
      "    if df_to_unskew[col].dtype == 'float64' or df_to_unskew[col].dtype == 'Int64' or df_to_unskew[col].dtype == 'int64' or df_to_unskew[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df_to_unskew, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "204/75: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "204/76: visuals.visualise_outliers(df)\n",
      "204/77:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df_IQR = outlier_trans.treat_outliers_IQR(df)\n",
      "204/78:\n",
      "visuals.visualise_outliers(outliers_removed_df_IQR)\n",
      "\n",
      "outliers_removed_df_IQR.to_csv('dfnooutliersIQR.csv')\n",
      "204/79:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "204/80:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "204/81:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "204/82:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "204/83: new_df.to_csv('dfdropcorrelation.csv')\n",
      "204/84: df.head\n",
      "204/85: dataframeinfo.get_skew_info(df)\n",
      "204/86: dataframeinfo.get_skew_info(df1)\n",
      "204/87: dataframeinfo.get_skew_info(df)\n",
      "204/88: df['loan_status'].unique()\n",
      "204/89: df2 = df['loan_status'].unique()\n",
      "204/90:\n",
      "df2 = df['loan_status'].unique()\n",
      "print(df2)\n",
      "204/91:\n",
      "df2 = df['loan_status'].unique()\n",
      "df2.shape\n",
      "204/92:\n",
      "df2 = df['loan_status'].unique()\n",
      "df2[0]\n",
      "204/93:\n",
      "df2 = df['loan_status'].unique()\n",
      "for i in range(0,len(df2)):\n",
      "    print(df2[i])\n",
      "204/94:\n",
      "new_df['total_repayments_inc_fees'] = new_df['total_payment']+ new_df['total_rec_late_fee']+new_df['recoveries'] + new_df['collection_recovery_fee']\n",
      "new_df['expected_payments'] = new_df['term(mths)'] * new_df['instalment']\n",
      "205/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "205/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "205/3: print(df.dtypes)\n",
      "205/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "205/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "205/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "205/7: df.head()\n",
      "205/8: msno.bar(df)\n",
      "205/9: msno.heatmap(df)\n",
      "205/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "205/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "205/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "205/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "205/14: df.to_csv('dfnonulls.csv')\n",
      "205/15: dataframeinfo.get_skew_info(df)\n",
      "205/16: visuals.visualise_skewness(df)\n",
      "205/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "#in order to keep the intergrity of my data it is necessary to create a copy of the dataframe before applying skew transforms\n",
      "\n",
      "df_to_unskew = df.copy()\n",
      "\n",
      "for col in df_to_unskew:\n",
      "    if df_to_unskew[col].dtype == 'float64' or df_to_unskew[col].dtype == 'Int64' or df_to_unskew[col].dtype == 'int64' or df_to_unskew[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df_to_unskew, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "205/18: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "205/19: visuals.visualise_outliers(df)\n",
      "205/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df_IQR = outlier_trans.treat_outliers_IQR(df)\n",
      "205/21:\n",
      "visuals.visualise_outliers(outliers_removed_df_IQR)\n",
      "\n",
      "outliers_removed_df_IQR.to_csv('dfnooutliersIQR.csv')\n",
      "205/22:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "205/23:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "205/24:\n",
      "check_corr = df1.select_dtypes('float64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "205/25:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "205/26: new_df.to_csv('dfdropcorrelation.csv')\n",
      "205/27:\n",
      "df2 = df['loan_status'].unique()\n",
      "for i in range(0,len(df2)):\n",
      "    print(df2[i])\n",
      "205/28:\n",
      "new_df['total_repayments_inc_fees'] = new_df['total_payment']+ new_df['total_rec_late_fee']+new_df['recoveries'] + new_df['collection_recovery_fee']\n",
      "new_df['expected_payments'] = new_df['term(mths)'] * new_df['instalment']\n",
      "205/29:\n",
      "new_df['total_repayments_inc_fees'] = new_df['total_payment']+ new_df['total_rec_late_fee']+new_df['recoveries'] + new_df['collection_recovery_fee']\n",
      "new_df['expected_payments'] = new_df['term(mths)'] * df['instalment']\n",
      "205/30:\n",
      "new_df['total_repayments_inc_fees'] = new_df['total_payment']+ new_df['total_rec_late_fee']+new_df['recoveries'] + new_df['collection_recovery_fee']\n",
      "new_df['expected_payments'] = new_df['term(mths)'] * df['instalment']\n",
      "\n",
      "pct_rcvrd = new_df['total_repayments_inc_fees'].sum()*100/new_df['expected_payments']\n",
      "\n",
      "print(pct_rcvrd)\n",
      "205/31:\n",
      "mean = df.loc[:, column].mean()\n",
      "\n",
      "new_df['total_repayments_inc_fees'] = new_df['total_payment']+ new_df['total_rec_late_fee']+new_df['recoveries'] + new_df['collection_recovery_fee']\n",
      "new_df['expected_payments'] = new_df['term(mths)'] * df['instalment']\n",
      "\n",
      "pct_rcvrd = new_df['total_repayments_inc_fees'].sum()*100/new_df['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "205/32:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "new_df['total_repayments_inc_fees'] = new_df['total_payment']+ new_df['total_rec_late_fee']+new_df['recoveries'] + new_df['collection_recovery_fee']\n",
      "new_df['expected_payments'] = new_df['term(mths)'] * df['instalment']\n",
      "\n",
      "pct_rcvrd = new_df['total_repayments_inc_fees'].sum()*100/new_df['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "205/33: dataframeinfo.get_skew_info(new_df)\n",
      "205/34:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "df['total_repayments_inc_fees'] = df['total_payment']+ df['total_rec_late_fee']+ f['recoveries'] + df['collection_recovery_fee']\n",
      "df['expected_payments'] = df['term(mths)'] * df['instalment']\n",
      "\n",
      "pct_rcvrd = df['total_repayments_inc_fees'].sum()*100/df['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "205/35:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "df['total_repayments_inc_fees'] = df['total_payment']+ df['total_rec_late_fee']+ df['recoveries'] + df['collection_recovery_fee']\n",
      "df['expected_payments'] = df['term(mths)'] * df['instalment']\n",
      "\n",
      "pct_rcvrd = df['total_repayments_inc_fees'].sum()*100/df['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "205/36:\n",
      "check_corr = df1.select_dtypes(['float64','int64').corr()\n",
      "sns.heatmap(check_corr)\n",
      "205/37:\n",
      "check_corr = df1.select_dtypes(['float64','int64']).corr()\n",
      "sns.heatmap(check_corr)\n",
      "205/38:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "df1['total_repayments_inc_fees'] = df1['total_payment']+ df1['total_rec_late_fee']+ df1['recoveries'] + df1['collection_recovery_fee']\n",
      "df1['expected_payments'] = df1['term(mths)'] * df1['instalment']\n",
      "\n",
      "pct_rcvrd = df1['total_repayments_inc_fees'].sum()*100/df1['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "205/39:\n",
      "\n",
      "rows_to_consider = df1['loan_status'] = \"Current\"\n",
      "print(rows_to_consider)\n",
      "205/40:\n",
      "\n",
      "rows_to_consider = df1[['loan_status'] = \"Current\"]\n",
      "print(rows_to_consider)\n",
      "205/41:\n",
      "\n",
      "rows_to_consider = df1[['loan_status'] == \"Current\"]\n",
      "print(rows_to_consider)\n",
      "205/42:\n",
      "\n",
      "rows_to_consider = df1.loc[df1['loan_status'] == \"Current\"]\n",
      "print(rows_to_consider)\n",
      "205/43: dataframeinfo.get_skew_info(new_df)\n",
      "205/44: dataframeinfo.get_skew_info(df1)\n",
      "206/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "206/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "206/3: print(df.dtypes)\n",
      "206/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "206/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "206/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "206/7: df.head()\n",
      "206/8: msno.bar(df)\n",
      "206/9: msno.heatmap(df)\n",
      "206/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "206/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "206/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "206/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "206/14: df.to_csv('dfnonulls.csv')\n",
      "206/15: dataframeinfo.get_skew_info(df)\n",
      "206/16: visuals.visualise_skewness(df)\n",
      "206/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "#in order to keep the intergrity of my data it is necessary to create a copy of the dataframe before applying skew transforms\n",
      "\n",
      "df_to_unskew = df.copy()\n",
      "\n",
      "for col in df_to_unskew:\n",
      "    if df_to_unskew[col].dtype == 'float64' or df_to_unskew[col].dtype == 'Int64' or df_to_unskew[col].dtype == 'int64' or df_to_unskew[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df_to_unskew, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "206/18: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "206/19: visuals.visualise_outliers(df)\n",
      "206/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df_IQR = outlier_trans.treat_outliers_IQR(df)\n",
      "206/21:\n",
      "visuals.visualise_outliers(outliers_removed_df_IQR)\n",
      "\n",
      "outliers_removed_df_IQR.to_csv('dfnooutliersIQR.csv')\n",
      "206/22:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "206/23:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "206/24:\n",
      "check_corr = df1.select_dtypes(['float64','int64']).corr()\n",
      "sns.heatmap(check_corr)\n",
      "206/25:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df)\n",
      "206/26: new_df.to_csv('dfdropcorrelation.csv')\n",
      "206/27:\n",
      "df2 = df['loan_status'].unique()\n",
      "for i in range(0,len(df2)):\n",
      "    print(df2[i])\n",
      "206/28:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "df1['total_repayments_inc_fees'] = df1['total_payment']+ df1['total_rec_late_fee']+ df1['recoveries'] + df1['collection_recovery_fee']\n",
      "df1['expected_payments'] = df1['term(mths)'] * df1['instalment']\n",
      "\n",
      "pct_rcvrd = df1['total_repayments_inc_fees'].sum()*100/df1['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "206/29:\n",
      "\n",
      "rows_to_consider = df1.loc[df1['loan_status'] == \"Current\"]\n",
      "print(rows_to_consider)\n",
      "206/30: dataframeinfo.get_skew_info(df1)\n",
      "206/31:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df.columns)\n",
      "207/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import missingno as msno\n",
      "import db_utils\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "import dataFrameTransform\n",
      "import plotter\n",
      "from data_Transform import DataTransform\n",
      "import statsmodels\n",
      "import scipy\n",
      "import sklearn\n",
      "207/2:\n",
      "df = pd.read_csv('loan_payments.csv')\n",
      "df.head()\n",
      "207/3: print(df.dtypes)\n",
      "207/4:\n",
      "df.rename(columns={\"term\": \"term(mths)\"},inplace=True)\n",
      "df['term(mths)'] = df['term(mths)'].str.replace(\"months\", \" \")\n",
      "df.head()\n",
      "207/5:\n",
      "cols_to_fill = ['mths_since_last_delinq', 'mths_since_last_record', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n",
      "\n",
      "transform = DataTransform()\n",
      "\n",
      "for col in cols_to_fill:\n",
      "    transform.fill_blanks(df, col)\n",
      "\n",
      "df.head(25)\n",
      "207/6:\n",
      "cat_data = ['id', 'member_id','grade','sub_grade','home_ownership','verification_status','loan_status', 'purpose','application_type','employment_length']\n",
      "int_data = []\n",
      "float_data = ['term(mths)', 'mths_since_last_delinq','mths_since_last_record', 'collections_12_mths_ex_med','mths_since_last_major_derog']\n",
      "bool_data = ['payment_plan']\n",
      "date_data = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
      "\n",
      "for col in cat_data:\n",
      "    transform.cat_type(df,col)\n",
      "\n",
      "for col in int_data:\n",
      "    transform.num_type(df,col)\n",
      "\n",
      "for col in float_data:\n",
      "    transform.float_type(df,col)\n",
      "\n",
      "for col in bool_data:\n",
      "    transform.bool_type(df,col)\n",
      "\n",
      "for col in date_data:\n",
      "    transform.convert_dates(df,col)\n",
      "\n",
      "print(df.dtypes)\n",
      "207/7: df.head()\n",
      "207/8: msno.bar(df)\n",
      "207/9: msno.heatmap(df)\n",
      "207/10:\n",
      "dataframetransform = Data_FrameTransform()\n",
      "\n",
      "df = dataframetransform.Nullremoval(df,50)\n",
      "df.head()\n",
      "print(df.shape)\n",
      "207/11:\n",
      "visuals = plotter.Plotter(df)\n",
      "\n",
      "msno.bar(df)\n",
      "207/12:\n",
      "dataframeinfo = Data_FrameInfo()\n",
      "\n",
      "dataframeinfo.get_mean(df, 'loan_amount')\n",
      "\n",
      " #finds the number of unique entries for columns with categorical data\n",
      "cols_to_count=['application_type','loan_status','verification_status','home_ownership','home_ownership','grade','sub_grade']\n",
      "\n",
      "for col in cols_to_count:\n",
      "    dataframeinfo.count_distinct(df,col)\n",
      "207/13:\n",
      "dataframetransform.impute_null_values(df)\n",
      "\n",
      "df.head()\n",
      "\n",
      "msno.bar(df)\n",
      "207/14: df.to_csv('dfnonulls.csv')\n",
      "207/15: dataframeinfo.get_skew_info(df)\n",
      "207/16: visuals.visualise_skewness(df)\n",
      "207/17:\n",
      "skewtransforms = Data_FrameTransform()\n",
      "\n",
      "#in order to keep the intergrity of my data it is necessary to create a copy of the dataframe before applying skew transforms\n",
      "\n",
      "df_to_unskew = df.copy()\n",
      "\n",
      "for col in df_to_unskew:\n",
      "    if df_to_unskew[col].dtype == 'float64' or df_to_unskew[col].dtype == 'Int64' or df_to_unskew[col].dtype == 'int64' or df_to_unskew[col].dtype==  'object':\n",
      "        nonskewed_df = skewtransforms.transform_column(df_to_unskew, col)\n",
      "\n",
      "#visuals.visualise_skewness(nonskewed_df)\n",
      "        df.head()\n",
      "207/18: nonskewed_df.to_csv('dfnoskew.csv')\n",
      "207/19: visuals.visualise_outliers(df)\n",
      "207/20:\n",
      "outlier_trans = Data_FrameTransform()\n",
      "\n",
      "outliers_removed_df_IQR = outlier_trans.treat_outliers_IQR(df)\n",
      "207/21:\n",
      "visuals.visualise_outliers(outliers_removed_df_IQR)\n",
      "\n",
      "outliers_removed_df_IQR.to_csv('dfnooutliersIQR.csv')\n",
      "207/22:\n",
      "#selects numerical columns to check z-scores for\n",
      "data_to_check = df.select_dtypes('float64')\n",
      "\n",
      "#creates a dataframe of z-scores using a method I created\n",
      "z_scoresdf = dataframeinfo.z_scores(data_to_check)\n",
      "\n",
      "#selects rows from this dataframe that have any z-score with an absolute value of greater than 3.5\n",
      "rows_to_drop = z_scoresdf[z_scoresdf.apply(lambda row: row.ge(abs(3.5)).any(), axis=1)]\n",
      "\n",
      "#drops these rows and returns what remains\n",
      "df1 = df.drop(rows_to_drop.index) # () looks for row number, [] looks for a key\n",
      "\n",
      "print(df1)\n",
      "207/23:\n",
      "visuals.visualise_outliers(df1)\n",
      "\n",
      "df1.to_csv('dfnooutlierszscore.csv')\n",
      "207/24:\n",
      "check_corr = df1.select_dtypes(['float64','int64']).corr()\n",
      "sns.heatmap(check_corr)\n",
      "207/25:\n",
      "new_df = outlier_trans.remove_overcorrelation(df1, 0.9)\n",
      "\n",
      "print(new_df.columns)\n",
      "207/26: new_df.to_csv('dfdropcorrelation.csv')\n",
      "207/27:\n",
      "df2 = df['loan_status'].unique()\n",
      "for i in range(0,len(df2)):\n",
      "    print(df2[i])\n",
      "207/28:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "df1['total_repayments_inc_fees'] = df1['total_payment']+ df1['total_rec_late_fee']+ df1['recoveries'] + df1['collection_recovery_fee']\n",
      "df1['expected_payments'] = df1['term(mths)'] * df1['instalment']\n",
      "\n",
      "pct_rcvrd = df1['total_repayments_inc_fees'].sum()*100/df1['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "207/29:\n",
      "\n",
      "rows_to_consider = df1.loc[df1['loan_status'] == \"Current\"]\n",
      "print(rows_to_consider)\n",
      "207/30: dataframeinfo.get_skew_info(df1)\n",
      "207/31:\n",
      "new_df.to_csv('dfdropcorrelation.csv')\n",
      "df1.to_csv('outliers_removed_zscore.csv')\n",
      "207/32:\n",
      "# mean = df.loc[:, column].mean()\n",
      "\n",
      "df1['total_repayments_inc_fees'] = df1['total_payment']+ df1['total_rec_late_fee']+ df1['recoveries'] + df1['collection_recovery_fee']\n",
      "df1['expected_payments'] = df1['term(mths)'] * df1['instalment']\n",
      "\n",
      "pct_rcvrd = df1['total_repayments_inc_fees'].sum()*100/df1['expected_payments'].sum()\n",
      "\n",
      "print(pct_rcvrd)\n",
      "209/1:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = plotter()\n",
      "209/2:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = plotter()\n",
      "209/3:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter()\n",
      "209/4:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter(df)\n",
      "209/5:\n",
      "rows_to_consider = df.loc[df['loan_status'] == \"Current\"]\n",
      "print(rows_to_consider)\n",
      "209/6:\n",
      "total_recovered = df['total_payment'].sum()\n",
      "\n",
      "df['total_expected'] = df['instalment'] * df['terms']\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "print(total_recovered/total_expected*100)\n",
      "209/7:\n",
      "total_recovered = df['total_payment'].sum()\n",
      "\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "print(total_recovered/total_expected*100)\n",
      "209/8:\n",
      "total_recovered = df['total_payment'].sum()\n",
      "\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "pct_recovered = round(total_recovered/total_expected*100, 2)\n",
      "print(pct_recovered)\n",
      "209/9:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by funded amount and by investor\n",
      "pct_recovered_fun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % funded amount recovered is: {pct_recovered_fun}%')\n",
      "209/10:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by funded amount and by investor\n",
      "pct_recovered_fun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % invested amount recovered is: {pct_recovered_inv}%')\n",
      "209/11: df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\")\n",
      "209/12: df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/1:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter(df)\n",
      "210/2:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by funded amount and by investor\n",
      "pct_recovered_fun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % invested amount recovered is: {pct_recovered_inv}%')\n",
      "210/3: df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/4:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_invetor_rec, pct_total_rec]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "display(df)\n",
      "\n",
      "df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/5:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "display(df)\n",
      "\n",
      "df.plot(x=\"Funding\", y=\"Percent\", kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/6:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate total amounts loaned out and invested\n",
      "total_funded_inv = df['funded_amount_inv'].sum()\n",
      "total_funded = df['funded_amount'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "210/7:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter(df)\n",
      "210/8:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate total amounts loaned out and invested\n",
      "total_funded_inv = df['funded_amount_inv'].sum()\n",
      "total_funded = df['funded_amount'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "210/9:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "display(df)\n",
      "\n",
      "df.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\", kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/10:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "display(df)\n",
      "\n",
      "df.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/11:\n",
      "df_current = df.loc[df_copy['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "210/12:\n",
      "df_current = df.loc[df['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "210/13:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/14:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter(df)\n",
      "210/15:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate total amounts loaned out and invested\n",
      "total_funded_inv = df['funded_amount_inv'].sum()\n",
      "total_funded = df['funded_amount'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "210/16:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/17:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/18:\n",
      "df_current = df.loc[df['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "210/19:\n",
      "df_current = df.loc[df['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "df_current.head()\n",
      "210/20:\n",
      "df_current = df.loc[df['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "print(df_current)\n",
      "210/21:\n",
      "df_current['total_payment_six'] = df_current['total_payment'] + 6 * df_current['instalment']\n",
      "df_current\n",
      "210/22:\n",
      "#creates new column with 6 months worth of payments added to the existing payments\n",
      "df_current['total_payment_six'] = df_current['total_payment'] + 6 * df_current['instalment']\n",
      "\n",
      "#calculate amount to be added to previously exitisng total\n",
      "payments_to_add = df_current['total_payment_six'].sum()\n",
      "\n",
      "#edit preexisting totals \n",
      "\n",
      "total_recovered_fun = total_recovered_fun + payments_to_add\n",
      "total_recovered_inv = total_recovered_inv + payments_to_add\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "210/23:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "210/24:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "210/25:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "\n",
      "#count rows for percentage required\n",
      "no_of_loans = df.columns.count()\n",
      "no_of_charged_off = df_charged_off.count()\n",
      "\n",
      "#calculating percentage\n",
      "pct_charged_off = no_of_charged_off/no_of_loans*100\n",
      "210/26:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "\n",
      "#count rows for percentage required\n",
      "no_of_loans = len(df)\n",
      "no_of_charged_off = len(df_charged_off)\n",
      "\n",
      "#calculating percentage\n",
      "pct_charged_off = no_of_charged_off/no_of_loans*100\n",
      "210/27:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "\n",
      "#count rows for percentage required\n",
      "no_of_loans = len(df)\n",
      "no_of_charged_off = len(df_charged_off)\n",
      "\n",
      "#calculating percentage\n",
      "pct_charged_off = no_of_charged_off/no_of_loans*100\n",
      "\n",
      "print(f'The % of loans charged off is: {pct_charged_off}%')\n",
      "210/28:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "\n",
      "#count rows for percentage required\n",
      "no_of_loans = len(df)\n",
      "no_of_charged_off = len(df_charged_off)\n",
      "\n",
      "#calculating percentage\n",
      "pct_charged_off = round(no_of_charged_off/no_of_loans*100, 2)\n",
      "\n",
      "print(f'The % of loans charged off is: {pct_charged_off}%')\n",
      "211/1:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter(df)\n",
      "211/2:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate total amounts loaned out and invested\n",
      "total_funded_inv = df['funded_amount_inv'].sum()\n",
      "total_funded = df['funded_amount'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "211/3:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "211/4:\n",
      "df_current = df.loc[df['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "print(df_current)\n",
      "211/5:\n",
      "#creates new column with 6 months worth of payments added to the existing payments\n",
      "df_current['total_payment_six'] = df_current['total_payment'] + 6 * df_current['instalment']\n",
      "\n",
      "#calculate amount to be added to previously exitisng total\n",
      "payments_to_add = df_current['total_payment_six'].sum()\n",
      "\n",
      "#edit preexisting totals \n",
      "\n",
      "total_recovered_fun = total_recovered_fun + payments_to_add\n",
      "total_recovered_inv = total_recovered_inv + payments_to_add\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "211/6:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "211/7:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "\n",
      "#count rows for percentage required\n",
      "no_of_loans = len(df)\n",
      "no_of_charged_off = len(df_charged_off)\n",
      "\n",
      "#calculating percentage\n",
      "pct_charged_off = round(no_of_charged_off/no_of_loans*100, 2)\n",
      "\n",
      "print(f'The % of loans charged off is: {pct_charged_off}%')\n",
      "211/8:\n",
      "loans_not_charged_off = len(df)-len(df_charged_off)\n",
      "\n",
      "visual.pie_chart([\"Loans Charged Off\", \"Loans not charged off\"], [no_of_charged_off,loans_not_charged_off], \"Pie Chart showing percentgae of loans charged off\")\n",
      "211/9:\n",
      "total_paid_charged_off = df_charged_off['total_payment'].sum()\n",
      "print(total_paid_charged_off)\n",
      "211/10:\n",
      "total_paid_charged_off = round(df_charged_off['total_payment'].sum(),2)\n",
      "print(total_paid_charged_off)\n",
      "211/11:\n",
      "total_paid_charged_off = round(df_charged_off['total_payment'].sum(),2)\n",
      "print(f'The total amount paid on loans before they were charged off is : ${total_paid_charged_off})\n",
      "211/12:\n",
      "total_paid_charged_off = round(df_charged_off['total_payment'].sum(),2)\n",
      "print(f'The total amount paid on loans before they were charged off is : ${total_paid_charged_off}')\n",
      "211/13:\n",
      "amount_not_paid = df_charged_off['total_expected'].sum() - df_charged_off['total_payment']\n",
      "\n",
      "visual.pie_chart([\"paid on charged off loans\",\"not paid on charged off loans\"],[total_paid_charged_off, amount_not_paid], \"Percentage of money recovered from charged off loans\")\n",
      "211/14:\n",
      "amount_not_paid = df_charged_off['total_expected'].sum() - df_charged_off['total_payment'].sum()\n",
      "\n",
      "visual.pie_chart([\"paid on charged off loans\",\"not paid on charged off loans\"],[total_paid_charged_off, amount_not_paid], \"Percentage of money recovered from charged off loans\")\n",
      "211/15:\n",
      "amount_lost = df_charged_off['total_expected'].sum() - df_charged_off['total_payment'].sum()\n",
      "print(f'The total amount lost on loans  charged off is : ${amount_lost}')\n",
      "\n",
      "visual.pie_chart([\"paid on charged off loans\",\"not paid on charged off loans\"],[total_paid_charged_off, amount_lost], \"Percentage of money recovered from charged off loans\")\n",
      "211/16:\n",
      "amount_lost = round(df_charged_off['total_expected'].sum() - df_charged_off['total_payment'].sum(),2)\n",
      "print(f'The total amount lost on loans  charged off is : ${amount_lost}')\n",
      "\n",
      "visual.pie_chart([\"paid on charged off loans\",\"not paid on charged off loans\"],[total_paid_charged_off, amount_lost], \"Percentage of money recovered from charged off loans\")\n",
      "211/17: df_in_default = df.loc[df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\"]\n",
      "211/18: df_in_default = df.loc[df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\"]\n",
      "211/19:\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\"]\n",
      "\n",
      "df_in_default = df.loc[df['loan_status']== \"Default\"] + df.loc[df['loan_status']== \"Late(16-30 days)\"] + df.loc[df['loan_status']== \"Late(31-120 days)\"]\n",
      "211/20:\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\"]\n",
      "\n",
      "df_in_default = df.loc[df['loan_status']== \"Default\"] + df.loc[df['loan_status']== \"Late(16-30 days)\"] + df.loc[df['loan_status']== \"Late(31-120 days)\"]\n",
      "df_in_default\n",
      "211/21:\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\"]\n",
      "\n",
      "df_in_default = df.loc[df['loan_status']== [\"Default\",\"Late(16-30 days)\",\"Late(31-120 days)\"]]\n",
      "df_in_default\n",
      "211/22:\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\"]\n",
      "\n",
      "df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/23:\n",
      "df_in_default = df.loc[df['loan_status']== \"Default\" | df['loan_status']== \"Late(16-30 days)\" | df['loan_status']== \"Late(31-120 days)\"]\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/24:\n",
      "df_in_default = df.loc[df['loan_status']== \"Default\" | df['loan_status']== \"Late(16-30 days)\" | df['loan_status']== \"Late(31-120 days)\"]\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/25:\n",
      "df_in_default = df.loc[df['loan_status']== \"Default\" | df['loan_status']== \"Late(16-30 days)\"]\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/26:\n",
      "df_in_default = df.query(df['loan_status']== \"Default\" or df['loan_status']== \"Late(16-30 days)\" or df['loan_status']== \"Late(31-120 days)\")\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/27:\n",
      "df_in_default = df.query(df['loan_status']== \"Default\" | df['loan_status']== \"Late(16-30 days)\" | df['loan_status']== \"Late(31-120 days)\")\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/28:\n",
      "df_in_default = df.loc[df['loan_status']== 'Default' | df['loan_status']== 'Late(16-30 days)'| df['loan_status']== 'Late(31-120 days)']\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/29:\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)'])\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/30:\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "\n",
      "#df_in_default = df.loc[df['loan_status']== \"Default\"]\n",
      "df_in_default\n",
      "211/31:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/32:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df_in_default.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/33:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/34:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "print(len(df_in_default))\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/35:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "print(df_in_default)\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/36:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Late(16-30 days)')| (df['loan_status']== 'Late(31-120 days)')]\n",
      "print(df_in_default)\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/37:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "print(df_in_default)\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/38:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/39:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}') \n",
      "\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "211/40:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/41:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/42:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/43:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/44:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount__co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/45:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/46:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/47: %history -g\n",
      "211/48:\n",
      "\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default') | (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "print(df_in_default)\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/49:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df_in_default.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/50:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Querying dataframe for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/51:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/52:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/53:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = amount_co_exp - total_payments\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/54:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from these customers\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = round(amount_co_exp - total_payments,2)\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/55:\n",
      "#Filter those records that are behind in payments\n",
      "df_in_default = df.loc[(df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "print(df_in_default)\n",
      "\n",
      "#Find total payments of these records\n",
      "current_payments = df_in_default['total_payment'].sum()\n",
      "\n",
      "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
      "expected_income = df_in_default['total_expected'].sum()\n",
      "\n",
      "#Find possible loss\n",
      "poss_loss = round(expected_income - current_payments, 2)\n",
      "\n",
      "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
      "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
      "\n",
      "# Calculating the total amount of customers in this bracket\n",
      "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
      "\n",
      "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
      "211/56:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from late paying customers\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = round(amount_co_exp - total_payments,2)\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/57:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from late paying customers\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default)')| (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_charged_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = round(amount_co_exp - total_payments,2)\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/58:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from late paying customers\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default)')| (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = round(amount_co_exp - total_payments,2)\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/59:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from late paying customers\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default)')| (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = round(amount_co_exp - total_payments,2)\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "211/60:\n",
      "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
      "df_to_charge_off = df.copy()\n",
      "# Applying the condition\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
      "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
      "\n",
      "# Filter for charged off loans\n",
      "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
      "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
      "\n",
      "#find expected revenue from late paying customers\n",
      "df_in_default = df.loc[(df['loan_status']== 'Default)')| (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
      "amount_exp = df_in_default['total_expected'].sum()\n",
      "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
      "\n",
      "#including those already charged off\n",
      "amount_co_exp = df_to_charge_off['total_expected'].sum()\n",
      "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
      "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
      "\n",
      "#find loss from everyone in default being charged off\n",
      "total_payments = df_to_charge_off['total_payment'].sum()\n",
      "loss_if_charged_off = round(amount_co_exp - total_payments,2)\n",
      "\n",
      "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')\n",
      "   1:\n",
      "import pandas as pd\n",
      "\n",
      "#importing my classes\n",
      "from dataFrameInfo import Data_FrameInfo\n",
      "from data_Transform import DataTransform\n",
      "from dataFrameTransform import Data_FrameTransform\n",
      "from plotter import Plotter\n",
      "\n",
      "#importing dataframes\n",
      "df = pd.read_csv('dfnonulls.csv')\n",
      "normalised = pd.read_csv('dfnoskew.csv')\n",
      "\n",
      "#instantiating classes\n",
      "transform = DataTransform()\n",
      "query = Data_FrameInfo()\n",
      "visual = Plotter(df)\n",
      "   2:\n",
      "#calculate totals recovered by funded amount and by investor\n",
      "total_recovered_fun = df['total_payment'].sum()\n",
      "total_recovered_inv = df['total_payment_inv'].sum()\n",
      "\n",
      "#calculate total amounts loaned out and invested\n",
      "total_funded_inv = df['funded_amount_inv'].sum()\n",
      "total_funded = df['funded_amount'].sum()\n",
      "\n",
      "#calculate totals expected by funded amount and by investor\n",
      "df['total_expected'] = df['instalment'] * df['term(mths)']\n",
      "\n",
      "total_expected = df['total_expected'].sum()\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "   3:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "   4:\n",
      "df_current = df.loc[df['loan_status'] == 'Current']\n",
      "df_current = df_current.reset_index(drop=True)\n",
      "print(df_current)\n",
      "   5:\n",
      "#creates new column with 6 months worth of payments added to the existing payments\n",
      "df_current['total_payment_six'] = df_current['total_payment'] + 6 * df_current['instalment']\n",
      "\n",
      "#calculate amount to be added to previously exitisng total\n",
      "payments_to_add = df_current['total_payment_six'].sum()\n",
      "\n",
      "#edit preexisting totals \n",
      "\n",
      "total_recovered_fun = total_recovered_fun + payments_to_add\n",
      "total_recovered_inv = total_recovered_inv + payments_to_add\n",
      "\n",
      "#print percentages by expected funded amount and by investor\n",
      "pct_recovered_expfun = round(total_recovered_fun/total_expected*100, 2)\n",
      "pct_recovered_expinv = round(total_recovered_inv/total_expected*100, 2)\n",
      "print(f'The % of expected funded amount recovered is: {pct_recovered_expfun}%')\n",
      "print(f'The % of expected invested amount recovered is: {pct_recovered_expinv}%')\n",
      "\n",
      "#print percentages of loaned amounts and invested amounts\n",
      "pct_recovered_fun = round(total_recovered_fun/total_funded*100, 2)\n",
      "pct_recovered_inv = round(total_recovered_inv/total_funded_inv*100, 2)\n",
      "print(f'The % of funded amount recovered is: {pct_recovered_fun}%')\n",
      "print(f'The % of invested amount recovered is: {pct_recovered_inv}%')\n",
      "   6:\n",
      "data = {'Funding': ['Investor', 'Total'],\n",
      "        'Percent': [pct_recovered_fun, pct_recovered_inv],\n",
      "        'Percent Expected':[pct_recovered_expfun, pct_recovered_expinv]}\n",
      "\n",
      "# Create a Pandas DataFrame\n",
      "df_bar = pd.DataFrame(data)\n",
      "display(df_bar)\n",
      "\n",
      "df_bar.plot(x=\"Funding\", y=[\"Percent\",\"Percent Expected\"], kind=\"bar\", ylabel=\"% Recovered\")\n",
      "   7:\n",
      "#extract those loans marked as Charged off\n",
      "df_charged_off = df.loc[df['loan_status']== \"Charged Off\"]\n",
      "df_charged_off.reset_index\n",
      "\n",
      "#count rows for percentage required\n",
      "no_of_loans = len(df)\n",
      "no_of_charged_off = len(df_charged_off)\n",
      "\n",
      "#calculating percentage\n",
      "pct_charged_off = round(no_of_charged_off/no_of_loans*100, 2)\n",
      "\n",
      "print(f'The % of loans charged off is: {pct_charged_off}%')\n",
      "   8:\n",
      "loans_not_charged_off = len(df)-len(df_charged_off)\n",
      "\n",
      "visual.pie_chart([\"Loans Charged Off\", \"Loans not charged off\"], [no_of_charged_off,loans_not_charged_off], \"Pie Chart showing percentgae of loans charged off\")\n",
      "   9:\n",
      "total_paid_charged_off = round(df_charged_off['total_payment'].sum(),2)\n",
      "print(f'The total amount paid on loans before they were charged off is : ${total_paid_charged_off}')\n",
      "  10:\n",
      "amount_lost = round(df_charged_off['total_expected'].sum() - df_charged_off['total_payment'].sum(),2)\n",
      "print(f'The total amount lost on loans  charged off is : ${amount_lost}')\n",
      "\n",
      "visual.pie_chart([\"paid on charged off loans\",\"not paid on charged off loans\"],[total_paid_charged_off, amount_lost], \"Percentage of money recovered from charged off loans\")\n",
      "  11: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating potential loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are customers who are currently behind with their loan payments. This subset of customers represent a risk to company revenue.\n",
    "\n",
    "\n",
    "What percentage do users in this bracket currently represent?\n",
    "\n",
    "Calculate the total amount of customers in this bracket and how much loss the company would incur if their status was changed to Charged Off.\n",
    "\n",
    "What is the projected loss of these loans if the customer were to finish the full loan term?\n",
    "\n",
    "\n",
    "If customers that are late on payments converted to Charged Off, what percentage of total expected revenue do these customers and the customers who have already defaulted on their loan represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of users in the risky loans bracket as a percentage of all loans: 1.26%\n",
      "\n",
      "The total amount of customers in this bracket: 686\n",
      "The potential loss  on loans if charged off is : $7316814.49\n"
     ]
    }
   ],
   "source": [
    "#Filter those records that are behind in payments\n",
    "df_in_default = df.loc[(df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
    "\n",
    "#Find total payments of these records\n",
    "current_payments = df_in_default['total_payment'].sum()\n",
    "\n",
    "#Project expected payment of these records. This was previously calculated in another cell so only needs to be summed here\n",
    "expected_income = df_in_default['total_expected'].sum()\n",
    "\n",
    "#Find possible loss\n",
    "poss_loss = round(expected_income - current_payments, 2)\n",
    "\n",
    "# Calculating the percentage of users in risky loans bracket as a percentage of all loans.\n",
    "print(f'Percentage of users in the risky loans bracket as a percentage of all loans: {round(100 * df_in_default.shape[0]/df.shape[0], 2)}%\\n')\n",
    "\n",
    "# Calculating the total amount of customers in this bracket\n",
    "print(f'The total amount of customers in this bracket: {round(df_in_default.shape[0], 2)}')\n",
    "\n",
    "print(f'The potential loss  on loans if charged off is : ${poss_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of total expected income represented by those late or in default is : 1.53%\n",
      "The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : 12.65%\n",
      "The projected loss of these loans if the customer were to finish the full loan term is : $7316814.49\n"
     ]
    }
   ],
   "source": [
    "# Changing the loan status customers who are currently behind with their loan payments to Charged Off.\n",
    "df_to_charge_off = df.copy()\n",
    "# Applying the condition\n",
    "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Default\", \"loan_status\"] = 'Charged Off'\n",
    "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (16-30 days)\", \"loan_status\"] = 'Charged Off'\n",
    "df_to_charge_off.loc[df_to_charge_off[\"loan_status\"] == \"Late (31-120 days)\", \"loan_status\"] = 'Charged Off'\n",
    "\n",
    "# Filter for charged off loans\n",
    "df_to_charge_off = df_to_charge_off.loc[df_to_charge_off['loan_status'] == 'Charged Off']\n",
    "df_to_charge_off = df_to_charge_off.reset_index(drop=True)\n",
    "\n",
    "#find expected revenue from late paying customers\n",
    "df_in_default = df.loc[(df['loan_status']== 'Default)')| (df['loan_status']== 'Late (16-30 days)')| (df['loan_status']== 'Late (31-120 days)')]\n",
    "amount_exp = df_in_default['total_expected'].sum()\n",
    "pct_defaulted = round(amount_exp/total_expected*100,2)\n",
    "print(f'The percentage of total expected income represented by those late or in default is : {pct_defaulted}%')\n",
    "\n",
    "#including those already charged off\n",
    "amount_co_exp = df_to_charge_off['total_expected'].sum()\n",
    "pct_defaulted = round(amount_co_exp/total_expected*100,2)\n",
    "print(f'The percentage of total expected income represented by those late or in default INCLUDING those already charged off is : {pct_defaulted}%')\n",
    "\n",
    "#find loss from everyone in default being charged off\n",
    "total_payments = df_in_default['total_payment'].sum()\n",
    "loss_if_charged_off = round(amount_exp - total_payments,2)\n",
    "\n",
    "print(f'The projected loss of these loans if the customer were to finish the full loan term is : ${loss_if_charged_off}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploratory_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
